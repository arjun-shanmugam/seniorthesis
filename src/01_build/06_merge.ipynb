{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06_merge.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from dask.distributed import Client\n",
    "import dask.dataframe as dd\n",
    "from dask_jobqueue import SLURMCluster\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as cx\n",
    "from build_utilities import generate_variable_names, aggregate_crime_to_case_month\n",
    "import dask_geopandas\n",
    "INPUT_DATA_EVICTIONS = \"../../data/02_intermediate/evictions.csv\"\n",
    "INPUT_DATA_TRACTS = \"../../data/02_intermediate/tracts.csv\"\n",
    "INPUT_DATA_TAX_PARCELS = \"../../data/02_intermediate/tax_parcels.gpkg\"\n",
    "INPUT_DATA_ZESTIMATES = \"../../data/02_intermediate/zestimates.csv\"\n",
    "INPUT_DATA_CRIME = \"../../data/01_raw/crime_incidents\"\n",
    "OUTPUT_DATA_UNRESTRICTED = \"../../data/03_cleaned/unrestricted.csv\"\n",
    "OUTPUT_DATA_ZILLOW = \"../../data/03_cleaned/zestimates_analysis.csv\"\n",
    "OUTPUT_DATA_CRIME = \"../../data/03_cleaned/crime_analysis.csv\"\n",
    "VERBOSE = True\n",
    "N_PARTITIONS = 1\n",
    "value_vars_to_concat = []  # A list of DataFrames, where each DataFrame contains the panel data for a single outcome variable and has case_number as its index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Evictions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning with 40759 observations.\n",
      "Dropping 0 observations where file_date is missing.\n",
      "Dropping 24 observations which have malformed addresses.\n",
      "Dropping 1 evictions missing latitude or longitude.\n"
     ]
    }
   ],
   "source": [
    "# Load evictions data.\n",
    "with open(INPUT_DATA_EVICTIONS, 'r') as file:\n",
    "    all_column_names = set(file.readline().replace(\"\\\"\", \"\").replace(\"\\n\", \"\").split(\",\"))\n",
    "to_drop = {'Accuracy Score', 'Accuracy Type', 'Number', 'Street', 'Unit Type', 'Unit Number',\n",
    "           'State', 'Zip', 'Country', 'Source', 'Census Year', 'State FIPS', 'County FIPS',\n",
    "           'Place Name', 'Place FIPS', 'Census Tract Code', 'Census Block Code', 'Census Block Group',\n",
    "           'Metro/Micro Statistical Area Code', 'Metro/Micro Statistical Area Type',\n",
    "           'Combined Statistical Area Code', 'Metropolitan Division Area Code', 'court_location',\n",
    "           'defendant', 'defendant_atty', 'defendant_atty_address_apt',\n",
    "           'defendant_atty_address_city', 'defendant_atty_address_name', 'defendant_atty_address_state',\n",
    "           'defendant_atty_address_street', 'defendant_atty_address_zip', 'docket_history', 'execution', 'judgment_for',\n",
    "           'judgment_total', 'latest_docket_date', 'plaintiff', 'plaintiff_atty', 'plaintiff_atty_address_apt',\n",
    "           'plaintiff_atty_address_city', 'plaintiff_atty_address_name', 'plaintiff_atty_address_state',\n",
    "           'plaintiff_atty_address_street', 'plaintiff_atty_address_zip', 'Metropolitan Division Area Name',\n",
    "           'property_address_city', 'property_address_state', 'property_address_street',\n",
    "           'property_address_zip'}\n",
    "evictions_df = pd.read_csv(INPUT_DATA_EVICTIONS, usecols=set(all_column_names) - set(to_drop))\n",
    "original_N = len(evictions_df)\n",
    "if VERBOSE:\n",
    "    print(f\"Beginning with {original_N} observations.\")\n",
    "\n",
    "# Drop cases missing file_date.\n",
    "mask = evictions_df['file_date'].notna()\n",
    "if VERBOSE:\n",
    "    print(\n",
    "        f\"Dropping {(~mask).sum()} observations where file_date is missing.\")\n",
    "evictions_df = evictions_df.loc[mask, :]\n",
    "\n",
    "# Add file month and year to dataset.\n",
    "evictions_df.loc[:, 'file_month'] = pd.to_datetime(evictions_df['file_date']).dt.strftime('%Y-%m')\n",
    "evictions_df.loc[:, 'file_year'] = pd.to_datetime(evictions_df['file_date']).dt.year\n",
    "\n",
    "# Clean the values in the judgment_for_pdu variable.\n",
    "judgment_for_pdu_replacement_dict = {\"unknown\": \"Unknown\",\n",
    "                                     \"plaintiff\": \"Plaintiff\",\n",
    "                                     \"defendant\": \"Defendant\"}\n",
    "evictions_df.loc[:, \"judgment_for_pdu\"] = (evictions_df.loc[:, \"judgment_for_pdu\"]\n",
    "                                           .replace(judgment_for_pdu_replacement_dict))\n",
    "\n",
    "# Replace missing values in money judgment column with zeroes.\n",
    "evictions_df.loc[:, 'judgment'] = evictions_df['judgment'].fillna(0)\n",
    "\n",
    "# Rename duration to case_duration.\n",
    "evictions_df = evictions_df.rename(columns={'duration': 'case_duration'})\n",
    "\n",
    "# Drop malformed addresses.\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {evictions_df['property_address_full'].str.contains('span, span span').sum()} observations which \"\n",
    "          f\"have malformed addresses.\")\n",
    "evictions_df = evictions_df.loc[~evictions_df['property_address_full'].str.contains(\"span, span span\"), :]\n",
    "\n",
    "# Drop addresses without latitude and longitude.\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {evictions_df[['longitude', 'latitude']].isna().any(axis=1).sum()} evictions missing latitude \"\n",
    "          f\"or longitude.\")\n",
    "evictions_df = evictions_df.dropna(subset=['longitude', 'latitude'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Merging Evictions With Census Tract Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully merged 40732 observations with census tracts.\n"
     ]
    }
   ],
   "source": [
    "# Merge with census tract characteristics.\n",
    "evictions_df = evictions_df.rename(columns={'Full FIPS (tract)': 'tract_geoid'})\n",
    "evictions_tracts_df = evictions_df.merge(pd.read_csv(INPUT_DATA_TRACTS, dtype={'tract_geoid': float}),\n",
    "                                  on='tract_geoid',\n",
    "                                  how='left',\n",
    "                                  validate='m:1').set_index('case_number')\n",
    "if VERBOSE:\n",
    "    print(f\"Successfully merged {evictions_tracts_df['med_hhinc2016'].notna().sum()} observations with census tracts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Merging Evictions With Zestimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully matched 11496 evictions (28.22 percent of observations) to Zestimates.\n"
     ]
    }
   ],
   "source": [
    "evictions_tracts_zestimates_df = pd.read_csv(INPUT_DATA_ZESTIMATES, index_col='case_number').merge(evictions_tracts_df,\n",
    "                                                     right_index=True,\n",
    "                                                     left_index=True,\n",
    "                                                     how='right',\n",
    "                                                     validate='1:1')\n",
    "if VERBOSE:\n",
    "    successfully_matched_observations = (~evictions_tracts_zestimates_df['2022-12'].isna()).sum()\n",
    "    print(\n",
    "        f\"Successfully matched {successfully_matched_observations} evictions \"\n",
    "        f\"({100 * (successfully_matched_observations / len(evictions_tracts_zestimates_df)) :.2f} percent of observations) to \"\n",
    "        f\"Zestimates.\")\n",
    "\n",
    "# Rename columns containing Zestimates.\n",
    "years = [str(year) for year in range(2013, 2023)]\n",
    "months = [\"0\" + str(month) for month in range(1, 10)] + [str(month) for month in range(10, 13)]\n",
    "value_vars = [\"2012-12\"] + [str(year) + \"-\" + str(month) for year in years for month in months]\n",
    "value_vars_zestimates, _, _ = generate_variable_names('zestimate')\n",
    "for value_var, value_var_zestimates in zip(value_vars, value_vars_zestimates):\n",
    "    evictions_tracts_zestimates_df = evictions_tracts_zestimates_df.rename(columns={value_var: value_var_zestimates})\n",
    "value_vars_to_concat.append(evictions_tracts_zestimates_df[value_vars_zestimates])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Merging Evictions with Tax Parcels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Request computing resources.\n",
    "cluster = SLURMCluster(queue='batch',\n",
    "                       cores=32,\n",
    "                       memory='230 GB',\n",
    "                       walltime='01:00:00',\n",
    "                      scheduler_options={'dashboard_address': '8787'} )\n",
    "cluster.scale(jobs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['LOC_ID', 'geometry', 'index_right', 'case_number'], dtype='object')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'notavariable' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m evictions_tax_parcels_dgdf \u001b[38;5;241m=\u001b[39m dask_geopandas\u001b[38;5;241m.\u001b[39msjoin(tax_parcels_dgdf, evictions_dgdf, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m'\u001b[39m, predicate\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontains\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(evictions_tax_parcels_dgdf\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mnotavariable\u001b[49m)\n\u001b[1;32m     15\u001b[0m ddf \u001b[38;5;241m=\u001b[39m ddf\u001b[38;5;241m.\u001b[39mloc[ddf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLOC_ID\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF_819960_2934955\u001b[39m\u001b[38;5;124m\"\u001b[39m, :]  \u001b[38;5;66;03m# Drop the eviction which erroneously merges to two parcels. \u001b[39;00m\n\u001b[1;32m     16\u001b[0m ddf \u001b[38;5;241m=\u001b[39m ddf\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex_right\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcompute()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'notavariable' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a GeoDataFrame containing eviction Points as geometry and case_number as a column.\n",
    "evictions_gdf = gpd.GeoDataFrame(evictions_df, geometry=gpd.points_from_xy(evictions_df['longitude'], evictions_df['latitude']))[['case_number', 'geometry']]\n",
    "evictions_gdf = evictions_gdf.set_crs(\"EPSG:4326\", allow_override=True).to_crs('EPSG:26986')\n",
    "# Convert this GeoDataFrame to a Dask GeoDataFrame\n",
    "evictions_dgdf = dask_geopandas.from_geopandas(evictions_gdf, npartitions=N_PARTITIONS).repartition(partition_size='25 MB')\n",
    "\n",
    "# Read in tax parcel data.\n",
    "tax_parcels_dgdf = dask_geopandas.read_file(INPUT_DATA_TAX_PARCELS, npartitions=N_PARTITIONS, layer='layer').repartition(partition_size='25 MB')\n",
    "\n",
    "# Join tax parcels with evictions, keeping only the geometry of the tax parcels.\n",
    "evictions_tax_parcels_dgdf = dask_geopandas.sjoin(tax_parcels_dgdf, evictions_dgdf, how='inner', predicate='contains').drop(columns='index_right')\n",
    "evictions_tax_parcels_dgdf = evictions_tax_parcels_dgdf.loc[ddf['LOC_ID'] != \"F_819960_2934955\", :]  # Drop the eviction which erroneously merges to two parcels. \n",
    "\n",
    "# Set index to case_number and drop all columns besides geometry.\n",
    "evictions_tax_parcels_dgdf = evictions_tax_parcels_dgdf.set_index('case_number')['geometry']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5a. Merge Evictions With Own-Parcel Crime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert eviction data to Dask-GeoDataFrame.\n",
    "ddf = dask_geopandas.from_geopandas(ddf, npartitions=N_PARTITIONS)\n",
    "ddf = ddf.repartition(partition_size='25 MB')  # Reduce partition size in preparation for spatial join.\n",
    "\n",
    "# Read crime data as Dask DataFrame; clean it.\n",
    "crime_dgdf = (dd.read_csv(INPUT_DATA_CRIME + \"/*.csv\", dtype={'REPORTING_AREA': 'object', 'SHOOTING': 'object'})\n",
    "                .dropna(subset=['Long', 'Lat', 'OCCURRED_ON_DATE'])\n",
    "                .rename(columns={'OCCURRED_ON_DATE': 'month_of_crime_incident'})\n",
    "                .drop(columns=['OFFENSE_CODE', 'OFFENSE_CODE_GROUP', 'OFFENSE_DESCRIPTION', 'DISTRICT', 'REPORTING_AREA', 'SHOOTING', 'YEAR', 'MONTH',\n",
    "                               'DAY_OF_WEEK', 'HOUR', 'UCR_PART', 'STREET', 'Location']))\n",
    "crime_dgdf['month_of_crime_incident'] = dd.to_datetime(crime_dgdf['month_of_crime_incident'].str[:10]).dt.to_period(\"M\").astype(str)\n",
    "crime_dgdf = crime_dgdf.compute()\n",
    "\n",
    "# Convert crime data to GeoDataFrame.\n",
    "crime_dgdf = (gpd.GeoDataFrame(crime_dgdf, geometry=gpd.points_from_xy(crime_dgdf['Long'], crime_dgdf['Lat']))\n",
    "                               .set_crs(\"EPSG:4326\", allow_override=True)\n",
    "                               .to_crs(\"EPSG:26986\"))\n",
    "\n",
    "# Convert crime data to Dask-GeoDataFrame.\n",
    "crime_dgdf = dask_geopandas.from_geopandas(crime_dgdf, npartitions=N_PARTITIONS).repartition(partition_size='25 MB')\n",
    "crime_dgdf = crime_dgdf.dissolve(by='INCIDENT_NUMBER').reset_index()\n",
    "\n",
    "# Join Dask-GeoDataFrames containing eviction data and crime data.\n",
    "ddf = dask_geopandas.sjoin(crime_dgdf,\n",
    "                            ddf,\n",
    "                            how='inner',\n",
    "                            predicate='within')\n",
    "\n",
    "# We no longer have any use for crime incident geometry.\n",
    "ddf = ddf.drop(columns='geometry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b. Aggregating Own-Parcel Crime Data to Case-Month Level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = aggregate_crime_to_case_month(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, ddf contains wide format crime incident data for the 792 properties which experienced crime.\n",
    "# Merge with the evictions that experienced no crime, stored in df.\n",
    "ddf = ddf.set_index('case_number')\n",
    "df = df.set_index('case_number')\n",
    "ddf = pd.concat([df, ddf], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns containing own-parcel crime incident counts.\n",
    "years = [str(year) for year in range(2015, 2023)]\n",
    "months = [\"0\" + str(month) for month in range(1, 10)] + [str(month) for month in range(10, 13)]\n",
    "value_vars = [str(year) + \"-\" + str(month) for year in years for month in months]\n",
    "value_vars = value_vars[5:]\n",
    "value_vars.append('2023-01')\n",
    "value_vars_crimes_own_parcel, _, _ = generate_variable_names('crimes_own_parcel')\n",
    "for value_var, value_var_crimes_own_parcel in zip(value_vars, value_vars_crimes_own_parcel):\n",
    "    ddf = ddf.rename(columns={value_var: value_var_crimes_own_parcel})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing crime data with zero for evictions that were not matched to crimes.\n",
    "ddf.loc[:, value_vars_crimes_own_parcel] = ddf[value_vars_crimes_own_parcel].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separately store own-parcel crime counts.\n",
    "to_concat = []\n",
    "crimes_own_parcel_data = ddf[value_vars_crimes_own_parcel]\n",
    "to_concat.append(crimes_own_parcel_data)\n",
    "ddf = ddf.drop(columns=value_vars_crimes_own_parcel)  # Drop from ddf so that we are not spatially joining unnecessary data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6a. Merge Evictions with Crimes Within Varying Distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    "ddf : GeoDataFrame with only a geometry column, containing Points corresponding to evictions.\n",
    "\n",
    "\n",
    "for each radius in [60, 90, 140, 200]:\n",
    "    60m_gdf = ddf.copy()\n",
    "    60m_gdf.geometry = 60m_gdf.geomtry.buffer(radius)\n",
    "    \n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask_geopandas.core.GeoDataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>month_of_crime_incident</th>\n",
       "      <th>case_number</th>\n",
       "      <th>2015-06</th>\n",
       "      <th>2015-07</th>\n",
       "      <th>2015-08</th>\n",
       "      <th>2015-09</th>\n",
       "      <th>2015-10</th>\n",
       "      <th>2015-11</th>\n",
       "      <th>2015-12</th>\n",
       "      <th>2016-01</th>\n",
       "      <th>2016-02</th>\n",
       "      <th>...</th>\n",
       "      <th>2022-04</th>\n",
       "      <th>2022-05</th>\n",
       "      <th>2022-06</th>\n",
       "      <th>2022-07</th>\n",
       "      <th>2022-08</th>\n",
       "      <th>2022-09</th>\n",
       "      <th>2022-10</th>\n",
       "      <th>2022-11</th>\n",
       "      <th>2022-12</th>\n",
       "      <th>2023-01</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19H79SP002248</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19H82SP01078</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19H84SP001836</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19H84SP001837</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19H84SP001838</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5455</th>\n",
       "      <td>21H84SP001196</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5456</th>\n",
       "      <td>21H84SP001198</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5457</th>\n",
       "      <td>21H84SP001199</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5458</th>\n",
       "      <td>21H84SP001202</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5459</th>\n",
       "      <td>21H84SP001223</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5460 rows Ã— 93 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "month_of_crime_incident    case_number  2015-06  2015-07  2015-08  2015-09  \\\n",
       "0                        19H79SP002248      3.0      6.0      4.0      3.0   \n",
       "1                         19H82SP01078      NaN      NaN      NaN      NaN   \n",
       "2                        19H84SP001836      NaN      2.0      2.0      4.0   \n",
       "3                        19H84SP001837      4.0      9.0      5.0      6.0   \n",
       "4                        19H84SP001838      1.0      3.0      4.0      4.0   \n",
       "...                                ...      ...      ...      ...      ...   \n",
       "5455                     21H84SP001196      1.0      4.0      5.0      3.0   \n",
       "5456                     21H84SP001198      2.0      4.0      3.0      1.0   \n",
       "5457                     21H84SP001199      4.0      3.0      9.0      5.0   \n",
       "5458                     21H84SP001202      5.0      NaN      1.0      1.0   \n",
       "5459                     21H84SP001223      NaN      1.0      1.0      NaN   \n",
       "\n",
       "month_of_crime_incident  2015-10  2015-11  2015-12  2016-01  2016-02  ...  \\\n",
       "0                           13.0      6.0      6.0      2.0      2.0  ...   \n",
       "1                            NaN      NaN      NaN      NaN      NaN  ...   \n",
       "2                            1.0      1.0      3.0      3.0      3.0  ...   \n",
       "3                            6.0      5.0      7.0     10.0      6.0  ...   \n",
       "4                            2.0      3.0      7.0      6.0      4.0  ...   \n",
       "...                          ...      ...      ...      ...      ...  ...   \n",
       "5455                         1.0      3.0      7.0      1.0      1.0  ...   \n",
       "5456                         2.0      NaN      6.0      3.0      2.0  ...   \n",
       "5457                         3.0      1.0      4.0      1.0      4.0  ...   \n",
       "5458                         3.0      2.0      5.0      2.0      2.0  ...   \n",
       "5459                         NaN      NaN      1.0      1.0      1.0  ...   \n",
       "\n",
       "month_of_crime_incident  2022-04  2022-05  2022-06  2022-07  2022-08  2022-09  \\\n",
       "0                            4.0      6.0     10.0      5.0      9.0      2.0   \n",
       "1                            NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "2                            2.0      3.0      1.0      1.0      3.0      1.0   \n",
       "3                            9.0      7.0      4.0      3.0      8.0      8.0   \n",
       "4                            2.0      2.0      2.0      4.0      1.0      4.0   \n",
       "...                          ...      ...      ...      ...      ...      ...   \n",
       "5455                         NaN      NaN      1.0      1.0      2.0      1.0   \n",
       "5456                         1.0      NaN      NaN      1.0      4.0      1.0   \n",
       "5457                         1.0      2.0      2.0      2.0      5.0      2.0   \n",
       "5458                         2.0      NaN      NaN      2.0      NaN      NaN   \n",
       "5459                         NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "\n",
       "month_of_crime_incident  2022-10  2022-11  2022-12  2023-01  \n",
       "0                            5.0      3.0      2.0      2.0  \n",
       "1                            NaN      NaN      NaN      NaN  \n",
       "2                            1.0      NaN      3.0      3.0  \n",
       "3                            2.0      7.0      7.0      5.0  \n",
       "4                            2.0      3.0      1.0      2.0  \n",
       "...                          ...      ...      ...      ...  \n",
       "5455                         1.0      2.0      NaN      NaN  \n",
       "5456                         1.0      1.0      2.0      NaN  \n",
       "5457                         2.0      1.0      1.0      NaN  \n",
       "5458                         NaN      1.0      NaN      NaN  \n",
       "5459                         NaN      NaN      NaN      NaN  \n",
       "\n",
       "[5460 rows x 93 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop existing geometry column and create a new one using the latitude and longitude of the eviction coordinates.\n",
    "ddf = gpd.GeoDataFrame(ddf,\n",
    "                          geometry=gpd.points_from_xy(ddf['longitude'], ddf['latitude']))\n",
    "ddf = ddf.set_crs(\"EPSG:4326\", allow_override=True).to_crs('EPSG:26986')\n",
    "ddf = ddf.reset_index()  # We need case_number as a column for gropuby operation.\n",
    "\n",
    "# TODO: perform merge at each value of radius\n",
    "for radius in [60, 90, 140, 200]:\n",
    "    current_gdf = ddf.copy()  # Copy dataframe containing geometry for each eviction.\n",
    "    current_gdf.geometry = current_gdf.geometry.buffer(radius)  # Add buffer.\n",
    "    \n",
    "    # Convert to Dask-GeoDataFrame and repartition\n",
    "    current_dgdf = dask_geopandas.from_geopandas(current_gdf, npartitions=N_PARTITIONS)\n",
    "    current_dgdf = current_dgdf.repartition(partition_size='25 MB')\n",
    "    \n",
    "    current_dgdf = dask_geopandas.sjoin(crime_dgdf,\n",
    "                            current_dgdf,\n",
    "                            how='inner',\n",
    "                            predicate='within')\n",
    "    print(type(current_dgdf))\n",
    "    # We no longer have any use for crime incident geometry.\n",
    "    current_dgdf = current_dgdf.drop(columns='geometry')\n",
    "    \n",
    "    current_dgdf = aggregate_crime_to_case_month(current_dgdf)\n",
    "    break\n",
    "    # TODO: finish aggregating crimes to case month, rename value vars, add to to_concat,\n",
    "    # then continue the for loop to the next iteration!\n",
    "current_dgdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Producing the Unrestricted Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrestricted_df = pd.DataFrame(unrestricted_df.drop(columns='geometry'))\n",
    "unrestricted_df.to_csv(OUTPUT_DATA_UNRESTRICTED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Producing the Samples Used in Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop cases resolved via mediation.\n",
    "mediated_mask = unrestricted_df['disposition_found'] == \"Mediated\"\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {mediated_mask.sum()} cases resolved through mediation.\")\n",
    "unrestricted_df = unrestricted_df.loc[~mediated_mask, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop cases resolved via volntary dismissal (dropped by plaintiff). \n",
    "voluntary_dismissal_mask = unrestricted_df['disposition'].str.contains(\"R 41(a)(1) Voluntary Dismissal on\", na=False, regex=False)\n",
    "if VERBOSE:\n",
    "    print(f\"Droppping {voluntary_dismissal_mask.sum()} cases resolved through voluntary dismissal.\")\n",
    "unrestricted_df = unrestricted_df.loc[~voluntary_dismissal_mask, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop cases where disposition found is other.\n",
    "disposition_found_other_mask = unrestricted_df['disposition_found'] == \"Other\"\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {disposition_found_other_mask.sum()} cases where disposition_found is \\\"Other\\\"\")\n",
    "unrestricted_df = unrestricted_df.loc[~disposition_found_other_mask, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows which contain consistent values of disposition_found and judgment_for_pdu.\n",
    "\n",
    "# First, we drop cases where disposition_found is \"Defaulted\" but judgment_for_pdu is \"Defendant\"\n",
    "inconsistent_mask_1 = ((unrestricted_df['disposition_found'] == \"Defaulted\") & (unrestricted_df['judgment_for_pdu'] == \"Defendant\"))\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {inconsistent_mask_1.sum()} observations where disposition_found is \\\"Defaulted\\\" but judgment_for_pdu is \\\"Defendant\\\".\")\n",
    "unrestricted_df = unrestricted_df.loc[~inconsistent_mask_1, :]\n",
    "          \n",
    "# Next, we drop cases where disposition_found is \"Dismissed\" yet judgment_for_pdu is \"Plaintiff\"\n",
    "inconsistent_mask_2 = ((unrestricted_df['disposition_found'] == \"Dismissed\") & (unrestricted_df['judgment_for_pdu'] == \"Plaintiff\"))\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {inconsistent_mask_2.sum()} observations where disposition_found is \\\"Dismissed\\\" but judgment_for_pdu is \\\"Plaintiff\\\".\")\n",
    "unrestricted_df = unrestricted_df.loc[~inconsistent_mask_2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a variable indicating judgment in favor of defendant.\n",
    "unrestricted_df.loc[:, 'judgment_for_defendant'] = 0\n",
    "defendant_won_mask = ((unrestricted_df['disposition_found'] == \"Dismissed\") |\n",
    "                      (unrestricted_df['judgment_for_pdu'] == \"Defendant\"))\n",
    "unrestricted_df.loc[defendant_won_mask, 'judgment_for_defendant'] = 1\n",
    "\n",
    "# Generate a variable indicating judgement in favor of plaintiff.\n",
    "unrestricted_df.loc[:, 'judgment_for_plaintiff'] = 1 - unrestricted_df['judgment_for_defendant']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8a. Producing the Zillow Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where we are missing any Zestimates.\n",
    "zestimates_df = unrestricted_df.copy()\n",
    "has_all_zestimates_mask = zestimates_df[value_vars_zestimates].notna().all(axis=1)\n",
    "if VERBOSE:\n",
    "    print(f\"Limiting sample to {has_all_zestimates_mask.sum()} evictions for which we observe Zestimates at every month from 2012-12 to 2022-12.\")\n",
    "zestimates_df = zestimates_df.loc[has_all_zestimates_mask, :]\n",
    "zestimates_df.to_csv(OUTPUT_DATA_ZILLOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8b. Producing the Crime Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_df = unrestricted_df.copy()\n",
    "# Restrict to evictions which took place in Boston.\n",
    "boston_mask = ((crime_df['County'] == \"Suffolk County\") & (~crime_df['City'].isin([\"Chelsea\", \"Revere\", \"Winthrop\"])))\n",
    "if VERBOSE:\n",
    "    print(f\"Limiting sample to {boston_mask.sum()} observations which are in Boston.\")\n",
    "crime_df = crime_df.loc[boston_mask, :]\n",
    "crime_df.to_csv(OUTPUT_DATA_CRIME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
