{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06_merge.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/geopandas/_compat.py:123: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_173015/3932295706.py:2: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from dask.distributed import Client\n",
    "import dask.dataframe as dd\n",
    "from dask_jobqueue import SLURMCluster\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as cx\n",
    "\n",
    "import dask_geopandas\n",
    "INPUT_DATA_EVICTIONS = \"../../data/02_intermediate/evictions.csv\"\n",
    "INPUT_DATA_TRACTS = \"../../data/02_intermediate/tracts.csv\"\n",
    "INPUT_DATA_TAX_PARCELS = \"../../data/02_intermediate/tax_parcels.gpkg\"\n",
    "INPUT_DATA_ZESTIMATES = \"../../data/02_intermediate/zestimates.csv\"\n",
    "INPUT_DATA_CRIME = \"../../data/01_raw/crime_incidents\"\n",
    "OUTPUT_DATA_UNRESTRICTED = \"../../data/03_cleaned/unrestricted.csv\"\n",
    "OUTPUT_DATA_ZILLOW = \"../../data/03_cleaned/zestimates_analysis.csv\"\n",
    "OUTPUT_DATA_CRIME = \"../../data/03_cleaned/crime_analysis.csv\"\n",
    "VERBOSE = True\n",
    "N_JOBS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Evictions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning with 40759 observations.\n",
      "Dropping 0 observations where file_date is missing (0.0 percent of original dataset).\n",
      "Dropping 24 observations which have malformed addresses (0.00 percent of observations).\n",
      "Dropping 1 evictions missing latitude or longitude (0.00) percent of observations.\n"
     ]
    }
   ],
   "source": [
    "# Load evictions data.\n",
    "with open(INPUT_DATA_EVICTIONS, 'r') as file:\n",
    "    all_column_names = set(file.readline().replace(\"\\\"\", \"\").replace(\"\\n\", \"\").split(\",\"))\n",
    "to_drop = {'Accuracy Score', 'Accuracy Type', 'Number', 'Street', 'Unit Type', 'Unit Number',\n",
    "           'State', 'Zip', 'Country', 'Source', 'Census Year', 'State FIPS', 'County FIPS',\n",
    "           'Place Name', 'Place FIPS', 'Census Tract Code', 'Census Block Code', 'Census Block Group',\n",
    "           'Metro/Micro Statistical Area Code', 'Metro/Micro Statistical Area Type',\n",
    "           'Combined Statistical Area Code', 'Metropolitan Division Area Code', 'court_location',\n",
    "           'defendant', 'defendant_atty', 'defendant_atty_address_apt',\n",
    "           'defendant_atty_address_city', 'defendant_atty_address_name', 'defendant_atty_address_state',\n",
    "           'defendant_atty_address_street', 'defendant_atty_address_zip', 'docket_history', 'execution', 'judgment_for',\n",
    "           'judgment_total', 'latest_docket_date', 'plaintiff', 'plaintiff_atty', 'plaintiff_atty_address_apt',\n",
    "           'plaintiff_atty_address_city', 'plaintiff_atty_address_name', 'plaintiff_atty_address_state',\n",
    "           'plaintiff_atty_address_street', 'plaintiff_atty_address_zip', 'Metropolitan Division Area Name',\n",
    "           'property_address_city', 'property_address_state', 'property_address_street',\n",
    "           'property_address_zip'}\n",
    "df = pd.read_csv(INPUT_DATA_EVICTIONS, usecols=set(all_column_names) - set(to_drop))\n",
    "original_N = len(df)\n",
    "if VERBOSE:\n",
    "    print(f\"Beginning with {original_N} observations.\")\n",
    "\n",
    "# Drop cases missing file_date.\n",
    "mask = df['file_date'].notna()\n",
    "if VERBOSE:\n",
    "    print(\n",
    "        f\"Dropping {(~mask).sum()} observations where file_date is missing ({100 * (((~mask).sum()) / original_N):.3} percent \"\n",
    "        f\"of original dataset).\")\n",
    "df = df.loc[mask, :]\n",
    "\n",
    "# Add file month and year to dataset.\n",
    "df.loc[:, 'file_month'] = pd.to_datetime(df['file_date']).dt.strftime('%Y-%m')\n",
    "df.loc[:, 'file_year'] = pd.to_datetime(df['file_date']).dt.year\n",
    "\n",
    "# Clean the values in the judgment_for_pdu variable.\n",
    "judgment_for_pdu_replacement_dict = {\"unknown\": \"Unknown\",\n",
    "                                     \"plaintiff\": \"Plaintiff\",\n",
    "                                     \"defendant\": \"Defendant\"}\n",
    "df.loc[:, \"judgment_for_pdu\"] = (df.loc[:, \"judgment_for_pdu\"]\n",
    "                                           .replace(judgment_for_pdu_replacement_dict))\n",
    "\n",
    "# Replace missing values in money judgment column with zeroes.\n",
    "df.loc[:, 'judgment'] = df['judgment'].fillna(0)\n",
    "\n",
    "# Rename duration to case_duration.\n",
    "df = df.rename(columns={'duration': 'case_duration'})\n",
    "\n",
    "# Drop malformed addresses.\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {df['property_address_full'].str.contains('span, span span').sum()} observations which \"\n",
    "          f\"have malformed addresses \"\n",
    "          f\"({df['property_address_full'].str.contains('span, span span').sum() / original_N:.2f} \"\n",
    "          f\"percent of observations).\")\n",
    "df = df.loc[~df['property_address_full'].str.contains(\"span, span span\"), :]\n",
    "\n",
    "# Drop addresses without latitude and longitude.\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {df[['longitude', 'latitude']].isna().any(axis=1).sum()} evictions missing latitude \"\n",
    "          f\"or longitude ({df[['longitude', 'latitude']].isna().any(axis=1).sum() / original_N:.2f}) \"\n",
    "          f\"percent of observations.\")\n",
    "df = df.dropna(subset=['longitude', 'latitude'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Merging Evictions With Census Tract Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully merged 40732 observations (1.00 percent of observations) with census tracts.\n"
     ]
    }
   ],
   "source": [
    "# Merge with census tract characteristics.\n",
    "df = df.rename(columns={'Full FIPS (tract)': 'tract_geoid'})\n",
    "df = df.merge(pd.read_csv(INPUT_DATA_TRACTS, dtype={'tract_geoid': float}),\n",
    "                                  on='tract_geoid',\n",
    "                                  how='left',\n",
    "                                  validate='m:1')\n",
    "if VERBOSE:\n",
    "    print(f\"Successfully merged {df['med_hhinc2016'].notna().sum()} observations \"\n",
    "          f\"({df['med_hhinc2016'].notna().sum() / original_N:.2f} percent of observations) with census \"\n",
    "          f\"tracts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Merging Evictions With Zestimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully matched 11496 evictions (28.22 percent of observations) to Zestimates.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(INPUT_DATA_ZESTIMATES).merge(df,\n",
    "                                                     on='case_number',\n",
    "                                                     how='right',\n",
    "                                                     validate='1:1')\n",
    "if VERBOSE:\n",
    "    successfully_matched_observations = (~df['2022-12'].isna()).sum()\n",
    "    print(\n",
    "        f\"Successfully matched {successfully_matched_observations} evictions \"\n",
    "        f\"({100 * (successfully_matched_observations / len(df)) :.2f} percent of observations) to \"\n",
    "        f\"Zestimates.\")\n",
    "\n",
    "# Rename columns containing Zestimates.\n",
    "years = [str(year) for year in range(2013, 2023)]\n",
    "months = [\"0\" + str(month) for month in range(1, 10)] + [str(month) for month in range(10, 13)]\n",
    "value_vars = [\"2012-12\"] + [str(year) + \"-\" + str(month) for year in years for month in months]\n",
    "for value_var in value_vars:\n",
    "    df = df.rename(columns={value_var: value_var + \"_zestimate\"})\n",
    "value_vars_zestimates = [value_var + \"_zestimate\" for value_var in value_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Merging Evictions with Tax Parcels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Request computing resources.\n",
    "cluster = SLURMCluster(queue='batch',\n",
    "                       cores=32,\n",
    "                       memory='230 GB',\n",
    "                       walltime='00:30:00',\n",
    "                      scheduler_options={'dashboard_address': '8787'} )\n",
    "cluster.scale(jobs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = gpd.GeoDataFrame(df,\n",
    "                          geometry=gpd.points_from_xy(df['longitude'], df['latitude']))\n",
    "\n",
    "df = df.set_crs(\"EPSG:4326\", allow_override=True).to_crs('EPSG:26986')\n",
    "ddf = dask_geopandas.from_geopandas(df, npartitions=N_JOBS).repartition(partition_size='50 MB')\n",
    "\n",
    "tax_parcels_dgdf = dask_geopandas.read_file(INPUT_DATA_TAX_PARCELS, npartitions=N_JOBS, layer='layer').repartition(partition_size='50 MB')\n",
    "\n",
    "ddf = dask_geopandas.sjoin(tax_parcels_dgdf, ddf, how='inner', predicate='contains')\n",
    "ddf = ddf.loc[ddf['LOC_ID'] != \"F_819960_2934955\", :]  # Drop the eviction which erroneously merges to two parcels. \n",
    "ddf = ddf.drop(columns=['index_right']).compute()\n",
    "\n",
    "# Append the evictions which could not be merged to tax parcels back into the Dask DataFrame.\n",
    "evictions_without_parcels = df.loc[~(df['case_number'].isin(ddf['case_number'])), :]\n",
    "ddf = pd.concat([ddf, evictions_without_parcels], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Merge Evictions With Crime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dask_geopandas.from_geopandas(ddf, npartitions=N_JOBS)\n",
    "ddf = ddf.repartition(partition_size='25 MB')  # Reduce partition size in preparation for spatial join.\n",
    "\n",
    "crime_dgdf = (dd.read_csv(INPUT_DATA_CRIME + \"/*.csv\", dtype={'REPORTING_AREA': 'object', 'SHOOTING': 'object'})\n",
    "                .dropna(subset=['Long', 'Lat', 'OCCURRED_ON_DATE'])\n",
    "                .rename(columns={'OCCURRED_ON_DATE': 'month_of_crime_incident'})\n",
    "                .drop(columns=['OFFENSE_CODE', 'OFFENSE_CODE_GROUP', 'OFFENSE_DESCRIPTION', 'DISTRICT', 'REPORTING_AREA', 'SHOOTING', 'YEAR', 'MONTH',\n",
    "                               'DAY_OF_WEEK', 'HOUR', 'UCR_PART', 'STREET', 'Location']))\n",
    "crime_dgdf['month_of_crime_incident'] = dd.to_datetime(crime_dgdf['month_of_crime_incident'].str[:10]).dt.to_period(\"M\").astype(str)\n",
    "crime_dgdf = crime_dgdf.compute()\n",
    "\n",
    "crime_dgdf = (gpd.GeoDataFrame(crime_dgdf, geometry=gpd.points_from_xy(crime_dgdf['Long'], crime_dgdf['Lat']))\n",
    "                               .set_crs(\"EPSG:4326\", allow_override=True)\n",
    "                               .to_crs(\"EPSG:26986\"))\n",
    "\n",
    "crime_dgdf = dask_geopandas.from_geopandas(crime_dgdf, npartitions=N_JOBS).repartition(partition_size='25 MB')\n",
    "ddf = dask_geopandas.sjoin(crime_dgdf,\n",
    "                            ddf,\n",
    "                            how='inner',\n",
    "                            predicate='within').compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Aggregating Crime Data to Case-Month Level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set aggregation function for each column.\n",
    "# We will aggregate by the 'first' value for all columns except incident number, which we will count.\n",
    "aggregate_by_first = ddf.columns.tolist()\n",
    "aggregate_by_first.remove('INCIDENT_NUMBER')\n",
    "aggregate_by_first.remove('case_number')\n",
    "aggregate_by_first.remove('month_of_crime_incident')\n",
    "agg_dict = {'INCIDENT_NUMBER': 'count'}\n",
    "for column in aggregate_by_first:\n",
    "    agg_dict[column] = 'first'\n",
    "ddf = ddf.groupby(['case_number', 'month_of_crime_incident'])\n",
    "ddf = ddf.aggregate(agg_dict)\n",
    "ddf = ddf.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n",
      "/tmp/ipykernel_173015/2974544706.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()\n"
     ]
    }
   ],
   "source": [
    "# Pivot from wide to long.\n",
    "ddf = ddf.rename(columns={'INCIDENT_NUMBER': 'crime_incidents'})\n",
    "ddf = pd.pivot(ddf, index=['case_number'] + aggregate_by_first, columns=['month_of_crime_incident'], values='crime_incidents').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['case_number', 'Lat', 'Long', 'geometry', 'index_right', 'LOC_ID', '2012-12_zestimate', '2013-01_zestimate', '2013-02_zestimate', '2013-03_zestimate', '2013-04_zestimate', '2013-05_zestimate', '2013-06_zestimate', '2013-07_zestimate', '2013-08_zestimate', '2013-09_zestimate', '2013-10_zestimate', '2013-11_zestimate', '2013-12_zestimate', '2014-01_zestimate', '2014-02_zestimate', '2014-03_zestimate', '2014-04_zestimate', '2014-05_zestimate', '2014-06_zestimate', '2014-07_zestimate', '2014-08_zestimate', '2014-09_zestimate', '2014-10_zestimate', '2014-11_zestimate', '2014-12_zestimate', '2015-01_zestimate', '2015-02_zestimate', '2015-03_zestimate', '2015-04_zestimate', '2015-05_zestimate', '2015-06_zestimate', '2015-07_zestimate', '2015-08_zestimate', '2015-09_zestimate', '2015-10_zestimate', '2015-11_zestimate', '2015-12_zestimate', '2016-01_zestimate', '2016-02_zestimate', '2016-03_zestimate', '2016-04_zestimate', '2016-05_zestimate', '2016-06_zestimate', '2016-07_zestimate', '2016-08_zestimate', '2016-09_zestimate', '2016-10_zestimate', '2016-11_zestimate', '2016-12_zestimate', '2017-01_zestimate', '2017-02_zestimate', '2017-03_zestimate', '2017-04_zestimate', '2017-05_zestimate', '2017-06_zestimate', '2017-07_zestimate', '2017-08_zestimate', '2017-09_zestimate', '2017-10_zestimate', '2017-11_zestimate', '2017-12_zestimate', '2018-01_zestimate', '2018-02_zestimate', '2018-03_zestimate', '2018-04_zestimate', '2018-05_zestimate', '2018-06_zestimate', '2018-07_zestimate', '2018-08_zestimate', '2018-09_zestimate', '2018-10_zestimate', '2018-11_zestimate', '2018-12_zestimate', '2019-01_zestimate', '2019-02_zestimate', '2019-03_zestimate', '2019-04_zestimate', '2019-05_zestimate', '2019-06_zestimate', '2019-07_zestimate', '2019-08_zestimate', '2019-09_zestimate', '2019-10_zestimate', '2019-11_zestimate', '2019-12_zestimate', '2020-01_zestimate', '2020-02_zestimate', '2020-03_zestimate', '2020-04_zestimate', '2020-05_zestimate', '2020-06_zestimate', '2020-07_zestimate', '2020-08_zestimate', '2020-09_zestimate', '2020-10_zestimate', '2020-11_zestimate', '2020-12_zestimate', '2021-01_zestimate', '2021-02_zestimate', '2021-03_zestimate', '2021-04_zestimate', '2021-05_zestimate', '2021-06_zestimate', '2021-07_zestimate', '2021-08_zestimate', '2021-09_zestimate', '2021-10_zestimate', '2021-11_zestimate', '2021-12_zestimate', '2022-01_zestimate', '2022-02_zestimate', '2022-03_zestimate', '2022-04_zestimate', '2022-05_zestimate', '2022-06_zestimate', '2022-07_zestimate', '2022-08_zestimate', '2022-09_zestimate', '2022-10_zestimate', '2022-11_zestimate', '2022-12_zestimate', 'court_division', 'court_person', 'court_person_type', 'disposition', 'disposition_date', 'disposition_found', 'case_duration', 'file_date', 'hasAttyD', 'hasAttyP', 'iehap', 'inhad', 'initiating_action', 'isEntityD', 'isEntityP', 'judgment', 'judgment_for_pdu', 'property_address_full', 'reason', 'latitude', 'longitude', 'Latitude', 'Longitude', 'City', 'County', 'Full FIPS (block)', 'tract_geoid', 'Metro/Micro Statistical Area Name', 'Combined Statistical Area Name', 'file_month', 'file_year', 'Unnamed: 0', 'med_hhinc2016', 'popdensity2010', 'share_white2010', 'frac_coll_plus2010', 'job_density_2013', 'poor_share2010', 'traveltime15_2010', 'rent_twobed2015', 'czname', '2015-06', '2015-07', '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01', '2016-02', '2016-03', '2016-04', '2016-05', '2016-06', '2016-07', '2016-08', '2016-09', '2016-10', '2016-11', '2016-12', '2017-01', '2017-02', '2017-03', '2017-04', '2017-05', '2017-06', '2017-07', '2017-08', '2017-09', '2017-10', '2017-11', '2017-12', '2018-01', '2018-02', '2018-03', '2018-04', '2018-05', '2018-06', '2018-07', '2018-08', '2018-09', '2018-10', '2018-11', '2018-12', '2019-01', '2019-02', '2019-03', '2019-04', '2019-05', '2019-06', '2019-07', '2019-08', '2019-09', '2019-10', '2019-11', '2019-12', '2020-01', '2020-02', '2020-03', '2020-04', '2020-05', '2020-06', '2020-07', '2020-08', '2020-09', '2020-10', '2020-11', '2020-12', '2021-01', '2021-02', '2021-03', '2021-04', '2021-05', '2021-06', '2021-07', '2021-08', '2021-09', '2021-10', '2021-11', '2021-12', '2022-01', '2022-02', '2022-03', '2022-04', '2022-05', '2022-06', '2022-07', '2022-08', '2022-09', '2022-10', '2022-11', '2022-12', '2023-01']\n"
     ]
    }
   ],
   "source": [
    "print(ddf.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/geopandas/array.py:1406: UserWarning: CRS not set for some of the concatenation inputs. Setting output's CRS as NAD83 / Massachusetts Mainland (the single non-null crs provided).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Append the evictions which could not be merged to tax parcels back into the DataFrame.\n",
    "evictions_without_parcels = df.loc[~(df['case_number'].isin(ddf['case_number'])), :]\n",
    "ddf = pd.concat([ddf, evictions_without_parcels], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns containing crime incident counts.\n",
    "years = [str(year) for year in range(2015, 2023)]\n",
    "months = [\"0\" + str(month) for month in range(1, 10)] + [str(month) for month in range(10, 13)]\n",
    "value_vars = [str(year) + \"-\" + str(month) for year in years for month in months]\n",
    "for value_var in value_vars:\n",
    "    ddf = ddf.rename(columns={value_var: value_var + \"_crimes\"})\n",
    "value_vars_crime = [value_var + \"_crimes\" for value_var in value_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['case_number', 'Lat', 'Long', 'geometry', 'index_right', 'LOC_ID', '2012-12_zestimate', '2013-01_zestimate', '2013-02_zestimate', '2013-03_zestimate', '2013-04_zestimate', '2013-05_zestimate', '2013-06_zestimate', '2013-07_zestimate', '2013-08_zestimate', '2013-09_zestimate', '2013-10_zestimate', '2013-11_zestimate', '2013-12_zestimate', '2014-01_zestimate', '2014-02_zestimate', '2014-03_zestimate', '2014-04_zestimate', '2014-05_zestimate', '2014-06_zestimate', '2014-07_zestimate', '2014-08_zestimate', '2014-09_zestimate', '2014-10_zestimate', '2014-11_zestimate', '2014-12_zestimate', '2015-01_zestimate', '2015-02_zestimate', '2015-03_zestimate', '2015-04_zestimate', '2015-05_zestimate', '2015-06_zestimate', '2015-07_zestimate', '2015-08_zestimate', '2015-09_zestimate', '2015-10_zestimate', '2015-11_zestimate', '2015-12_zestimate', '2016-01_zestimate', '2016-02_zestimate', '2016-03_zestimate', '2016-04_zestimate', '2016-05_zestimate', '2016-06_zestimate', '2016-07_zestimate', '2016-08_zestimate', '2016-09_zestimate', '2016-10_zestimate', '2016-11_zestimate', '2016-12_zestimate', '2017-01_zestimate', '2017-02_zestimate', '2017-03_zestimate', '2017-04_zestimate', '2017-05_zestimate', '2017-06_zestimate', '2017-07_zestimate', '2017-08_zestimate', '2017-09_zestimate', '2017-10_zestimate', '2017-11_zestimate', '2017-12_zestimate', '2018-01_zestimate', '2018-02_zestimate', '2018-03_zestimate', '2018-04_zestimate', '2018-05_zestimate', '2018-06_zestimate', '2018-07_zestimate', '2018-08_zestimate', '2018-09_zestimate', '2018-10_zestimate', '2018-11_zestimate', '2018-12_zestimate', '2019-01_zestimate', '2019-02_zestimate', '2019-03_zestimate', '2019-04_zestimate', '2019-05_zestimate', '2019-06_zestimate', '2019-07_zestimate', '2019-08_zestimate', '2019-09_zestimate', '2019-10_zestimate', '2019-11_zestimate', '2019-12_zestimate', '2020-01_zestimate', '2020-02_zestimate', '2020-03_zestimate', '2020-04_zestimate', '2020-05_zestimate', '2020-06_zestimate', '2020-07_zestimate', '2020-08_zestimate', '2020-09_zestimate', '2020-10_zestimate', '2020-11_zestimate', '2020-12_zestimate', '2021-01_zestimate', '2021-02_zestimate', '2021-03_zestimate', '2021-04_zestimate', '2021-05_zestimate', '2021-06_zestimate', '2021-07_zestimate', '2021-08_zestimate', '2021-09_zestimate', '2021-10_zestimate', '2021-11_zestimate', '2021-12_zestimate', '2022-01_zestimate', '2022-02_zestimate', '2022-03_zestimate', '2022-04_zestimate', '2022-05_zestimate', '2022-06_zestimate', '2022-07_zestimate', '2022-08_zestimate', '2022-09_zestimate', '2022-10_zestimate', '2022-11_zestimate', '2022-12_zestimate', 'court_division', 'court_person', 'court_person_type', 'disposition', 'disposition_date', 'disposition_found', 'case_duration', 'file_date', 'hasAttyD', 'hasAttyP', 'iehap', 'inhad', 'initiating_action', 'isEntityD', 'isEntityP', 'judgment', 'judgment_for_pdu', 'property_address_full', 'reason', 'latitude', 'longitude', 'Latitude', 'Longitude', 'City', 'County', 'Full FIPS (block)', 'tract_geoid', 'Metro/Micro Statistical Area Name', 'Combined Statistical Area Name', 'file_month', 'file_year', 'Unnamed: 0', 'med_hhinc2016', 'popdensity2010', 'share_white2010', 'frac_coll_plus2010', 'job_density_2013', 'poor_share2010', 'traveltime15_2010', 'rent_twobed2015', 'czname', '2015-06_crimes', '2015-07_crimes', '2015-08_crimes', '2015-09_crimes', '2015-10_crimes', '2015-11_crimes', '2015-12_crimes', '2016-01_crimes', '2016-02_crimes', '2016-03_crimes', '2016-04_crimes', '2016-05_crimes', '2016-06_crimes', '2016-07_crimes', '2016-08_crimes', '2016-09_crimes', '2016-10_crimes', '2016-11_crimes', '2016-12_crimes', '2017-01_crimes', '2017-02_crimes', '2017-03_crimes', '2017-04_crimes', '2017-05_crimes', '2017-06_crimes', '2017-07_crimes', '2017-08_crimes', '2017-09_crimes', '2017-10_crimes', '2017-11_crimes', '2017-12_crimes', '2018-01_crimes', '2018-02_crimes', '2018-03_crimes', '2018-04_crimes', '2018-05_crimes', '2018-06_crimes', '2018-07_crimes', '2018-08_crimes', '2018-09_crimes', '2018-10_crimes', '2018-11_crimes', '2018-12_crimes', '2019-01_crimes', '2019-02_crimes', '2019-03_crimes', '2019-04_crimes', '2019-05_crimes', '2019-06_crimes', '2019-07_crimes', '2019-08_crimes', '2019-09_crimes', '2019-10_crimes', '2019-11_crimes', '2019-12_crimes', '2020-01_crimes', '2020-02_crimes', '2020-03_crimes', '2020-04_crimes', '2020-05_crimes', '2020-06_crimes', '2020-07_crimes', '2020-08_crimes', '2020-09_crimes', '2020-10_crimes', '2020-11_crimes', '2020-12_crimes', '2021-01_crimes', '2021-02_crimes', '2021-03_crimes', '2021-04_crimes', '2021-05_crimes', '2021-06_crimes', '2021-07_crimes', '2021-08_crimes', '2021-09_crimes', '2021-10_crimes', '2021-11_crimes', '2021-12_crimes', '2022-01_crimes', '2022-02_crimes', '2022-03_crimes', '2022-04_crimes', '2022-05_crimes', '2022-06_crimes', '2022-07_crimes', '2022-08_crimes', '2022-09_crimes', '2022-10_crimes', '2022-11_crimes', '2022-12_crimes', '2023-01']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['2015-01_crimes', '2015-02_crimes', '2015-03_crimes', '2015-04_crimes', '2015-05_crimes'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(ddf\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Replace missing crime data with zero for evictions that were not matched to crimes.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m ddf\u001b[38;5;241m.\u001b[39mloc[:, value_vars_crime] \u001b[38;5;241m=\u001b[39m \u001b[43mddf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalue_vars_crime\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/pandas/core/frame.py:3511\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3510\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3511\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3513\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:5782\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5780\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5782\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5784\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   5785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5786\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:5845\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5842\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5844\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 5845\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['2015-01_crimes', '2015-02_crimes', '2015-03_crimes', '2015-04_crimes', '2015-05_crimes'] not in index\""
     ]
    }
   ],
   "source": [
    "# Replace missing crime data with zero for evictions that were not matched to crimes.\n",
    "ddf.loc[:, value_vars_crime] = ddf[value_vars_crime].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Save unrestricted dataset.\n",
    "ddf = pd.DataFrame(ddf.drop(columns='geometry'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Save Zillow sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOOD: Save crime sample.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
