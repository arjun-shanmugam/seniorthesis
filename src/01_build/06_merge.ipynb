{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06_merge.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from dask.distributed import Client\n",
    "import dask.dataframe as dd\n",
    "from dask_jobqueue import SLURMCluster\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as cx\n",
    "from build_utilities import generate_variable_names\n",
    "import dask_geopandas\n",
    "INPUT_DATA_EVICTIONS = \"../../data/02_intermediate/evictions.csv\"\n",
    "INPUT_DATA_TRACTS = \"../../data/02_intermediate/tracts.csv\"\n",
    "INPUT_DATA_TAX_PARCELS = \"../../data/02_intermediate/tax_parcels.gpkg\"\n",
    "INPUT_DATA_ZESTIMATES = \"../../data/02_intermediate/zestimates.csv\"\n",
    "INPUT_DATA_CRIME = \"../../data/01_raw/crime_incidents\"\n",
    "OUTPUT_DATA_UNRESTRICTED = \"../../data/03_cleaned/unrestricted.csv\"\n",
    "OUTPUT_DATA_ZILLOW = \"../../data/03_cleaned/zestimates_analysis.csv\"\n",
    "OUTPUT_DATA_CRIME = \"../../data/03_cleaned/crime_analysis.csv\"\n",
    "VERBOSE = True\n",
    "N_PARTITIONS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Evictions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning with 40759 observations.\n",
      "Dropping 0 observations where file_date is missing.\n",
      "Dropping 24 observations which have malformed addresses.\n",
      "Dropping 1 evictions missing latitude or longitude.\n"
     ]
    }
   ],
   "source": [
    "# Load evictions data.\n",
    "with open(INPUT_DATA_EVICTIONS, 'r') as file:\n",
    "    all_column_names = set(file.readline().replace(\"\\\"\", \"\").replace(\"\\n\", \"\").split(\",\"))\n",
    "to_drop = {'Accuracy Score', 'Accuracy Type', 'Number', 'Street', 'Unit Type', 'Unit Number',\n",
    "           'State', 'Zip', 'Country', 'Source', 'Census Year', 'State FIPS', 'County FIPS',\n",
    "           'Place Name', 'Place FIPS', 'Census Tract Code', 'Census Block Code', 'Census Block Group',\n",
    "           'Metro/Micro Statistical Area Code', 'Metro/Micro Statistical Area Type',\n",
    "           'Combined Statistical Area Code', 'Metropolitan Division Area Code', 'court_location',\n",
    "           'defendant', 'defendant_atty', 'defendant_atty_address_apt',\n",
    "           'defendant_atty_address_city', 'defendant_atty_address_name', 'defendant_atty_address_state',\n",
    "           'defendant_atty_address_street', 'defendant_atty_address_zip', 'docket_history', 'execution', 'judgment_for',\n",
    "           'judgment_total', 'latest_docket_date', 'plaintiff', 'plaintiff_atty', 'plaintiff_atty_address_apt',\n",
    "           'plaintiff_atty_address_city', 'plaintiff_atty_address_name', 'plaintiff_atty_address_state',\n",
    "           'plaintiff_atty_address_street', 'plaintiff_atty_address_zip', 'Metropolitan Division Area Name',\n",
    "           'property_address_city', 'property_address_state', 'property_address_street',\n",
    "           'property_address_zip'}\n",
    "df = pd.read_csv(INPUT_DATA_EVICTIONS, usecols=set(all_column_names) - set(to_drop))\n",
    "original_N = len(df)\n",
    "if VERBOSE:\n",
    "    print(f\"Beginning with {original_N} observations.\")\n",
    "\n",
    "# Drop cases missing file_date.\n",
    "mask = df['file_date'].notna()\n",
    "if VERBOSE:\n",
    "    print(\n",
    "        f\"Dropping {(~mask).sum()} observations where file_date is missing.\")\n",
    "df = df.loc[mask, :]\n",
    "\n",
    "# Add file month and year to dataset.\n",
    "df.loc[:, 'file_month'] = pd.to_datetime(df['file_date']).dt.strftime('%Y-%m')\n",
    "df.loc[:, 'file_year'] = pd.to_datetime(df['file_date']).dt.year\n",
    "\n",
    "# Clean the values in the judgment_for_pdu variable.\n",
    "judgment_for_pdu_replacement_dict = {\"unknown\": \"Unknown\",\n",
    "                                     \"plaintiff\": \"Plaintiff\",\n",
    "                                     \"defendant\": \"Defendant\"}\n",
    "df.loc[:, \"judgment_for_pdu\"] = (df.loc[:, \"judgment_for_pdu\"]\n",
    "                                           .replace(judgment_for_pdu_replacement_dict))\n",
    "\n",
    "# Replace missing values in money judgment column with zeroes.\n",
    "df.loc[:, 'judgment'] = df['judgment'].fillna(0)\n",
    "\n",
    "# Rename duration to case_duration.\n",
    "df = df.rename(columns={'duration': 'case_duration'})\n",
    "\n",
    "# Drop malformed addresses.\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {df['property_address_full'].str.contains('span, span span').sum()} observations which \"\n",
    "          f\"have malformed addresses.\")\n",
    "df = df.loc[~df['property_address_full'].str.contains(\"span, span span\"), :]\n",
    "\n",
    "# Drop addresses without latitude and longitude.\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {df[['longitude', 'latitude']].isna().any(axis=1).sum()} evictions missing latitude \"\n",
    "          f\"or longitude.\")\n",
    "df = df.dropna(subset=['longitude', 'latitude'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Merging Evictions With Census Tract Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully merged 40732 observations (1.00 percent of observations) with census tracts.\n"
     ]
    }
   ],
   "source": [
    "# Merge with census tract characteristics.\n",
    "df = df.rename(columns={'Full FIPS (tract)': 'tract_geoid'})\n",
    "df = df.merge(pd.read_csv(INPUT_DATA_TRACTS, dtype={'tract_geoid': float}),\n",
    "                                  on='tract_geoid',\n",
    "                                  how='left',\n",
    "                                  validate='m:1')\n",
    "if VERBOSE:\n",
    "    print(f\"Successfully merged {df['med_hhinc2016'].notna().sum()} observations \"\n",
    "          f\"({df['med_hhinc2016'].notna().sum() / original_N:.2f} percent of observations) with census \"\n",
    "          f\"tracts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Merging Evictions With Zestimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully matched 11496 evictions (28.22 percent of observations) to Zestimates.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(INPUT_DATA_ZESTIMATES).merge(df,\n",
    "                                                     on='case_number',\n",
    "                                                     how='right',\n",
    "                                                     validate='1:1')\n",
    "if VERBOSE:\n",
    "    successfully_matched_observations = (~df['2022-12'].isna()).sum()\n",
    "    print(\n",
    "        f\"Successfully matched {successfully_matched_observations} evictions \"\n",
    "        f\"({100 * (successfully_matched_observations / len(df)) :.2f} percent of observations) to \"\n",
    "        f\"Zestimates.\")\n",
    "\n",
    "# Rename columns containing Zestimates.\n",
    "years = [str(year) for year in range(2013, 2023)]\n",
    "months = [\"0\" + str(month) for month in range(1, 10)] + [str(month) for month in range(10, 13)]\n",
    "value_vars = [\"2012-12\"] + [str(year) + \"-\" + str(month) for year in years for month in months]\n",
    "value_vars_zestimates, _, _ = generate_variable_names('zestimate')\n",
    "for value_var, value_var_zestimates in zip(value_vars, value_vars_zestimates):\n",
    "    df = df.rename(columns={value_var: value_var_zestimates})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Merging Evictions with Tax Parcels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Request computing resources.\n",
    "cluster = SLURMCluster(queue='batch',\n",
    "                       cores=32,\n",
    "                       memory='230 GB',\n",
    "                       walltime='00:30:00',\n",
    "                      scheduler_options={'dashboard_address': '8787'} )\n",
    "cluster.scale(jobs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = gpd.GeoDataFrame(df,\n",
    "                          geometry=gpd.points_from_xy(df['longitude'], df['latitude']))\n",
    "\n",
    "df = df.set_crs(\"EPSG:4326\", allow_override=True).to_crs('EPSG:26986')\n",
    "ddf = dask_geopandas.from_geopandas(df, npartitions=N_PARTITIONS).repartition(partition_size='50 MB')\n",
    "\n",
    "tax_parcels_dgdf = dask_geopandas.read_file(INPUT_DATA_TAX_PARCELS, npartitions=N_PARTITIONS, layer='layer').repartition(partition_size='50 MB')\n",
    "\n",
    "ddf = dask_geopandas.sjoin(tax_parcels_dgdf, ddf, how='inner', predicate='contains')\n",
    "ddf = ddf.loc[ddf['LOC_ID'] != \"F_819960_2934955\", :]  # Drop the eviction which erroneously merges to two parcels. \n",
    "ddf = ddf.drop(columns=['index_right']).compute()\n",
    "\n",
    "# Append the evictions which could not be merged to tax parcels back into the Dask DataFrame.\n",
    "evictions_without_parcels = df.loc[~(df['case_number'].isin(ddf['case_number'])), :]\n",
    "ddf = pd.concat([ddf, evictions_without_parcels], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5a. Merge Evictions With Own-Parcel Crime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert eviction data to Dask-GeoDataFrame.\n",
    "ddf = dask_geopandas.from_geopandas(ddf, npartitions=N_PARTITIONS)\n",
    "ddf = ddf.repartition(partition_size='25 MB')  # Reduce partition size in preparation for spatial join.\n",
    "\n",
    "# Read crime data as Dask DataFrame; clean it.\n",
    "crime_dgdf = (dd.read_csv(INPUT_DATA_CRIME + \"/*.csv\", dtype={'REPORTING_AREA': 'object', 'SHOOTING': 'object'})\n",
    "                .dropna(subset=['Long', 'Lat', 'OCCURRED_ON_DATE'])\n",
    "                .rename(columns={'OCCURRED_ON_DATE': 'month_of_crime_incident'})\n",
    "                .drop(columns=['OFFENSE_CODE', 'OFFENSE_CODE_GROUP', 'OFFENSE_DESCRIPTION', 'DISTRICT', 'REPORTING_AREA', 'SHOOTING', 'YEAR', 'MONTH',\n",
    "                               'DAY_OF_WEEK', 'HOUR', 'UCR_PART', 'STREET', 'Location']))\n",
    "crime_dgdf['month_of_crime_incident'] = dd.to_datetime(crime_dgdf['month_of_crime_incident'].str[:10]).dt.to_period(\"M\").astype(str)\n",
    "crime_dgdf = crime_dgdf.compute()\n",
    "\n",
    "# Convert crime data to GeoDataFrame.\n",
    "crime_dgdf = (gpd.GeoDataFrame(crime_dgdf, geometry=gpd.points_from_xy(crime_dgdf['Long'], crime_dgdf['Lat']))\n",
    "                               .set_crs(\"EPSG:4326\", allow_override=True)\n",
    "                               .to_crs(\"EPSG:26986\"))\n",
    "\n",
    "# Convert crime data to Dask-GeoDataFrame.\n",
    "crime_dgdf = dask_geopandas.from_geopandas(crime_dgdf, npartitions=N_PARTITIONS).repartition(partition_size='25 MB')\n",
    "crime_dgdf = crime_dgdf.dissolve(by='INCIDENT_NUMBER').reset_index()\n",
    "\n",
    "# Join Dask-GeoDataFrames containing eviction data and crime data.\n",
    "ddf = dask_geopandas.sjoin(crime_dgdf,\n",
    "                            ddf,\n",
    "                            how='inner',\n",
    "                            predicate='within').compute()\n",
    "\n",
    "# We no longer have any use for crime incident geometry.\n",
    "ddf = ddf.drop(columns='geometry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b. Aggregating Own-Parcel Crime Data to Case-Month Level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = aggregate_crime_to_case_month(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, ddf contains wide format crime incident data for the 792 properties which experienced crime.\n",
    "# Merge with the evictions that experienced no crime, stored in df.\n",
    "ddf = ddf.set_index('case_number')\n",
    "df = df.set_index('case_number')\n",
    "ddf = pd.concat([df, ddf], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns containing own-parcel crime incident counts.\n",
    "years = [str(year) for year in range(2015, 2023)]\n",
    "months = [\"0\" + str(month) for month in range(1, 10)] + [str(month) for month in range(10, 13)]\n",
    "value_vars = [str(year) + \"-\" + str(month) for year in years for month in months]\n",
    "value_vars = value_vars[5:]\n",
    "value_vars.append('2023-01')\n",
    "value_vars_crimes_own_parcel, _, _ = generate_variable_names('crimes_own_parcel')\n",
    "for value_var, value_var_crimes_own_parcel in zip(value_vars, value_vars_crimes_own_parcel):\n",
    "    ddf = ddf.rename(columns={value_var: value_var_crimes_own_parcel})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing crime data with zero for evictions that were not matched to crimes.\n",
    "ddf.loc[:, value_vars_crimes_own_parcel] = ddf[value_vars_crimes_own_parcel].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separately store own-parcel crime counts.\n",
    "to_concat = []\n",
    "crimes_own_parcel_data = ddf[value_vars_crimes_own_parcel]\n",
    "to_concat.append(crimes_own_parcel_data)\n",
    "ddf = ddf.drop(columns=value_vars_crimes_own_parcel)  # Drop from ddf so that we are not spatially joining unnecessary data.\n",
    "ddf = ddf[['longitude', 'latitude']]  # Can merge eviction data later; only need geometry column for spatial join."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6a. Merge Evictions with Crimes Within Varying Distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    "ddf : GeoDataFrame with only a geometry column, containing Points corresponding to evictions.\n",
    "\n",
    "\n",
    "for each radius in [60, 90, 140, 200]:\n",
    "    60m_gdf = ddf.copy()\n",
    "    60m_gdf.geometry = 60m_gdf.geomtry.buffer(radius)\n",
    "    \n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: perform merge at each value of radius\n",
    "\n",
    "# Drop existing geometry column and create a new one using the latitude and longitude of the eviction coordinates.\n",
    "ddf = ddf.drop(columns='geometry')\n",
    "ddf = gpd.GeoDataFrame(ddf,\n",
    "                          geometry=gpd.points_from_xy(ddf['longitude'], ddf['latitude']))\n",
    "\n",
    "ddf = ddf.set_crs(\"EPSG:4326\", allow_override=True).to_crs('EPSG:26986')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ddf.geometry = ddf.geometry.buffer(500)  # Add 500m buffer around every eviction. \n",
    "ddf = dask_geopandas.from_geopandas(ddf, npartitions=N_PARTITIONS).repartition(partition_size='50 MB')\n",
    "# ddf is now a dask-geodataframe where each eviction's geometry is a 500\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Producing the Unrestricted Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrestricted_df = pd.DataFrame(unrestricted_df.drop(columns='geometry'))\n",
    "unrestricted_df.to_csv(OUTPUT_DATA_UNRESTRICTED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Producing the Samples Used in Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop cases resolved via mediation.\n",
    "mediated_mask = unrestricted_df['disposition_found'] == \"Mediated\"\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {mediated_mask.sum()} cases resolved through mediation.\")\n",
    "unrestricted_df = unrestricted_df.loc[~mediated_mask, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop cases resolved via volntary dismissal (dropped by plaintiff). \n",
    "voluntary_dismissal_mask = unrestricted_df['disposition'].str.contains(\"R 41(a)(1) Voluntary Dismissal on\", na=False, regex=False)\n",
    "if VERBOSE:\n",
    "    print(f\"Droppping {voluntary_dismissal_mask.sum()} cases resolved through voluntary dismissal.\")\n",
    "unrestricted_df = unrestricted_df.loc[~voluntary_dismissal_mask, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop cases where disposition found is other.\n",
    "disposition_found_other_mask = unrestricted_df['disposition_found'] == \"Other\"\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {disposition_found_other_mask.sum()} cases where disposition_found is \\\"Other\\\"\")\n",
    "unrestricted_df = unrestricted_df.loc[~disposition_found_other_mask, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows which contain consistent values of disposition_found and judgment_for_pdu.\n",
    "\n",
    "# First, we drop cases where disposition_found is \"Defaulted\" but judgment_for_pdu is \"Defendant\"\n",
    "inconsistent_mask_1 = ((unrestricted_df['disposition_found'] == \"Defaulted\") & (unrestricted_df['judgment_for_pdu'] == \"Defendant\"))\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {inconsistent_mask_1.sum()} observations where disposition_found is \\\"Defaulted\\\" but judgment_for_pdu is \\\"Defendant\\\".\")\n",
    "unrestricted_df = unrestricted_df.loc[~inconsistent_mask_1, :]\n",
    "          \n",
    "# Next, we drop cases where disposition_found is \"Dismissed\" yet judgment_for_pdu is \"Plaintiff\"\n",
    "inconsistent_mask_2 = ((unrestricted_df['disposition_found'] == \"Dismissed\") & (unrestricted_df['judgment_for_pdu'] == \"Plaintiff\"))\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {inconsistent_mask_2.sum()} observations where disposition_found is \\\"Dismissed\\\" but judgment_for_pdu is \\\"Plaintiff\\\".\")\n",
    "unrestricted_df = unrestricted_df.loc[~inconsistent_mask_2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a variable indicating judgment in favor of defendant.\n",
    "unrestricted_df.loc[:, 'judgment_for_defendant'] = 0\n",
    "defendant_won_mask = ((unrestricted_df['disposition_found'] == \"Dismissed\") |\n",
    "                      (unrestricted_df['judgment_for_pdu'] == \"Defendant\"))\n",
    "unrestricted_df.loc[defendant_won_mask, 'judgment_for_defendant'] = 1\n",
    "\n",
    "# Generate a variable indicating judgement in favor of plaintiff.\n",
    "unrestricted_df.loc[:, 'judgment_for_plaintiff'] = 1 - unrestricted_df['judgment_for_defendant']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8a. Producing the Zillow Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where we are missing any Zestimates.\n",
    "zestimates_df = unrestricted_df.copy()\n",
    "has_all_zestimates_mask = zestimates_df[value_vars_zestimates].notna().all(axis=1)\n",
    "if VERBOSE:\n",
    "    print(f\"Limiting sample to {has_all_zestimates_mask.sum()} evictions for which we observe Zestimates at every month from 2012-12 to 2022-12.\")\n",
    "zestimates_df = zestimates_df.loc[has_all_zestimates_mask, :]\n",
    "zestimates_df.to_csv(OUTPUT_DATA_ZILLOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8b. Producing the Crime Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_df = unrestricted_df.copy()\n",
    "# Restrict to evictions which took place in Boston.\n",
    "boston_mask = ((crime_df['County'] == \"Suffolk County\") & (~crime_df['City'].isin([\"Chelsea\", \"Revere\", \"Winthrop\"])))\n",
    "if VERBOSE:\n",
    "    print(f\"Limiting sample to {boston_mask.sum()} observations which are in Boston.\")\n",
    "crime_df = crime_df.loc[boston_mask, :]\n",
    "crime_df.to_csv(OUTPUT_DATA_CRIME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
