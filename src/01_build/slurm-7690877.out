## SLURM PROLOG ###############################################################
##    Job ID : 7690877
##  Job Name : dask-worker
##  Nodelist : node1328
##      CPUs : 1
##  Mem/Node : 8192 MB
## Directory : /gpfs/home/ashanmu1/seniorthesis/src/01_build
##   Job Started : Sat Jan 28 18:35:52 EST 2023
###############################################################################
2023-01-28 18:35:53,131 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.209.28:33734'
2023-01-28 18:35:53,135 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.209.28:42700'
2023-01-28 18:35:53,136 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.209.28:34216'
2023-01-28 18:35:53,144 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.209.28:32978'
2023-01-28 18:35:53,975 - distributed.worker - INFO -       Start worker at:  tcp://172.20.209.28:38420
2023-01-28 18:35:53,976 - distributed.worker - INFO -          Listening to:  tcp://172.20.209.28:38420
2023-01-28 18:35:53,976 - distributed.worker - INFO -           Worker name:           SLURMCluster-2-2
2023-01-28 18:35:53,976 - distributed.worker - INFO -          dashboard at:        172.20.209.28:35593
2023-01-28 18:35:53,976 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.1:33063
2023-01-28 18:35:53,976 - distributed.worker - INFO - -------------------------------------------------
2023-01-28 18:35:53,976 - distributed.worker - INFO -               Threads:                          1
2023-01-28 18:35:53,976 - distributed.worker - INFO -                Memory:                   1.86 GiB
2023-01-28 18:35:53,976 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pdql85_d
2023-01-28 18:35:53,976 - distributed.worker - INFO - -------------------------------------------------
2023-01-28 18:35:53,982 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.1:33063
2023-01-28 18:35:53,982 - distributed.worker - INFO - -------------------------------------------------
2023-01-28 18:35:53,982 - distributed.core - INFO - Starting established connection to tcp://172.20.207.1:33063
2023-01-28 18:35:54,002 - distributed.worker - INFO -       Start worker at:  tcp://172.20.209.28:41184
2023-01-28 18:35:54,002 - distributed.worker - INFO -          Listening to:  tcp://172.20.209.28:41184
2023-01-28 18:35:54,002 - distributed.worker - INFO -           Worker name:           SLURMCluster-2-1
2023-01-28 18:35:54,002 - distributed.worker - INFO -          dashboard at:        172.20.209.28:39596
2023-01-28 18:35:54,002 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.1:33063
2023-01-28 18:35:54,002 - distributed.worker - INFO - -------------------------------------------------
2023-01-28 18:35:54,002 - distributed.worker - INFO -               Threads:                          1
2023-01-28 18:35:54,002 - distributed.worker - INFO -                Memory:                   1.86 GiB
2023-01-28 18:35:54,002 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-i3xelpy6
2023-01-28 18:35:54,002 - distributed.worker - INFO - -------------------------------------------------
2023-01-28 18:35:54,007 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.1:33063
2023-01-28 18:35:54,007 - distributed.worker - INFO - -------------------------------------------------
2023-01-28 18:35:54,008 - distributed.core - INFO - Starting established connection to tcp://172.20.207.1:33063
2023-01-28 18:35:54,052 - distributed.worker - INFO -       Start worker at:  tcp://172.20.209.28:42019
2023-01-28 18:35:54,052 - distributed.worker - INFO -          Listening to:  tcp://172.20.209.28:42019
2023-01-28 18:35:54,052 - distributed.worker - INFO -           Worker name:           SLURMCluster-2-0
2023-01-28 18:35:54,052 - distributed.worker - INFO -          dashboard at:        172.20.209.28:44432
2023-01-28 18:35:54,052 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.1:33063
2023-01-28 18:35:54,054 - distributed.worker - INFO - -------------------------------------------------
2023-01-28 18:35:54,054 - distributed.worker - INFO -               Threads:                          1
2023-01-28 18:35:54,054 - distributed.worker - INFO -                Memory:                   1.86 GiB
2023-01-28 18:35:54,054 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9_njapan
2023-01-28 18:35:54,054 - distributed.worker - INFO - -------------------------------------------------
2023-01-28 18:35:54,059 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.1:33063
2023-01-28 18:35:54,059 - distributed.worker - INFO - -------------------------------------------------
2023-01-28 18:35:54,059 - distributed.core - INFO - Starting established connection to tcp://172.20.207.1:33063
2023-01-28 18:35:54,109 - distributed.worker - INFO -       Start worker at:  tcp://172.20.209.28:34566
2023-01-28 18:35:54,109 - distributed.worker - INFO -          Listening to:  tcp://172.20.209.28:34566
2023-01-28 18:35:54,109 - distributed.worker - INFO -           Worker name:           SLURMCluster-2-3
2023-01-28 18:35:54,109 - distributed.worker - INFO -          dashboard at:        172.20.209.28:45106
2023-01-28 18:35:54,109 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.1:33063
2023-01-28 18:35:54,109 - distributed.worker - INFO - -------------------------------------------------
2023-01-28 18:35:54,109 - distributed.worker - INFO -               Threads:                          1
2023-01-28 18:35:54,109 - distributed.worker - INFO -                Memory:                   1.86 GiB
2023-01-28 18:35:54,109 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3redw0rl
2023-01-28 18:35:54,109 - distributed.worker - INFO - -------------------------------------------------
2023-01-28 18:35:54,114 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.1:33063
2023-01-28 18:35:54,114 - distributed.worker - INFO - -------------------------------------------------
2023-01-28 18:35:54,114 - distributed.core - INFO - Starting established connection to tcp://172.20.207.1:33063
slurmstepd: error: *** JOB 7690877 ON node1328 CANCELLED AT 2023-01-28T18:37:43 ***
