{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06_merge_monthly.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/geopandas/_compat.py:123: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_70520/3460744447.py:2: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from dask.distributed import Client\n",
    "import dask.dataframe as dd\n",
    "from dask_jobqueue import SLURMCluster\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as cx\n",
    "from build_utilities import generate_variable_names, aggregate_crime_to_case_month\n",
    "import dask_geopandas\n",
    "INPUT_DATA_EVICTIONS = \"../../data/02_intermediate/evictions.csv\"\n",
    "INPUT_DATA_TRACTS = \"../../data/02_intermediate/tracts.csv\"\n",
    "INPUT_DATA_TAX_PARCELS = \"../../data/02_intermediate/tax_parcels.gpkg\"\n",
    "INPUT_DATA_ZESTIMATES = \"../../data/02_intermediate/zestimates.csv\"\n",
    "INPUT_DATA_CRIME = \"../../data/01_raw/crime_incidents\"\n",
    "OUTPUT_DATA_UNRESTRICTED = \"../../data/03_cleaned/unrestricted_monthly.parquet\"\n",
    "OUTPUT_DATA_ZILLOW = \"../../data/03_cleaned/zestimates_analysis.csv\"\n",
    "OUTPUT_DATA_CRIME = \"../../data/03_cleaned/crime_analysis_monthly.parquet\"\n",
    "VERBOSE = True\n",
    "N_PARTITIONS = 1\n",
    "value_vars_to_concat = []  # A list of DataFrames, where each DataFrame contains the panel data for a single outcome variable and has case_number as its index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Evictions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning with 43351 observations.\n",
      "Dropping 0 observations where file_date is missing.\n",
      "Dropping 24 observations which have malformed addresses.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['longitude', 'latitude'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 58\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Drop addresses without latitude and longitude.\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m VERBOSE:\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDropping \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mevictions_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlongitude\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlatitude\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39many(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m evictions missing latitude \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor longitude.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m evictions_df \u001b[38;5;241m=\u001b[39m evictions_df\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/pandas/core/frame.py:3511\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3510\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3511\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3513\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:5782\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5780\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5782\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5784\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   5785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5786\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:5842\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5840\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   5841\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 5842\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5844\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   5845\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['longitude', 'latitude'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# Load evictions data.\n",
    "with open(INPUT_DATA_EVICTIONS, 'r') as file:\n",
    "    all_column_names = set(file.readline().replace(\"\\\"\", \"\").replace(\"\\n\", \"\").split(\",\"))\n",
    "to_drop = {'Accuracy Score', 'Accuracy Type', 'Number', 'Street', 'Unit Type', 'Unit Number',\n",
    "           'State', 'Zip', 'Country', 'Source', 'Census Year', 'State FIPS', 'County FIPS',\n",
    "           'Place Name', 'Place FIPS', 'Census Tract Code', 'Census Block Code', 'Census Block Group',\n",
    "           'Metro/Micro Statistical Area Code', 'Metro/Micro Statistical Area Type',\n",
    "           'Combined Statistical Area Code', 'Metropolitan Division Area Code', 'court_location',\n",
    "           'defendant', 'defendant_atty', 'defendant_atty_address_apt',\n",
    "           'defendant_atty_address_city', 'defendant_atty_address_name', 'defendant_atty_address_state',\n",
    "           'defendant_atty_address_street', 'defendant_atty_address_zip', 'docket_history', 'execution', 'judgment_for',\n",
    "           'judgment_total', 'plaintiff', 'plaintiff_atty', 'plaintiff_atty_address_apt',\n",
    "           'plaintiff_atty_address_city', 'plaintiff_atty_address_name', 'plaintiff_atty_address_state',\n",
    "           'plaintiff_atty_address_street', 'plaintiff_atty_address_zip', 'Metropolitan Division Area Name',\n",
    "           'property_address_city', 'property_address_state', 'property_address_street',\n",
    "           'property_address_zip'}\n",
    "evictions_df = pd.read_csv(INPUT_DATA_EVICTIONS, usecols=set(all_column_names) - set(to_drop))\n",
    "original_N = len(evictions_df)\n",
    "if VERBOSE:\n",
    "    print(f\"Beginning with {original_N} observations.\")\n",
    "\n",
    "# Drop cases missing file_date.\n",
    "mask = evictions_df['file_date'].notna()\n",
    "if VERBOSE:\n",
    "    print(\n",
    "        f\"Dropping {(~mask).sum()} observations where file_date is missing.\")\n",
    "evictions_df = evictions_df.loc[mask, :]\n",
    "\n",
    "# Add file month and year to dataset.\n",
    "evictions_df.loc[:, 'file_month'] = pd.to_datetime(evictions_df['file_date']).dt.strftime('%Y-%m')\n",
    "evictions_df.loc[:, 'file_year'] = pd.to_datetime(evictions_df['file_date']).dt.year\n",
    "\n",
    "# Add latest docket month and year to dataset.\n",
    "evictions_df.loc[:, 'latest_docket_month'] = pd.to_datetime(evictions_df['latest_docket_date']).dt.strftime('%Y-%m')\n",
    "evictions_df.loc[:, 'latest_docket_year'] = pd.to_datetime(evictions_df['latest_docket_date']).dt.year\n",
    "\n",
    "# Clean the values in the judgment_for_pdu variable.\n",
    "judgment_for_pdu_replacement_dict = {\"unknown\": \"Unknown\",\n",
    "                                     \"plaintiff\": \"Plaintiff\",\n",
    "                                     \"defendant\": \"Defendant\"}\n",
    "evictions_df.loc[:, \"judgment_for_pdu\"] = (evictions_df.loc[:, \"judgment_for_pdu\"]\n",
    "                                           .replace(judgment_for_pdu_replacement_dict))\n",
    "\n",
    "# Replace missing values in money judgment column with zeroes.\n",
    "evictions_df.loc[:, 'judgment'] = evictions_df['judgment'].fillna(0)\n",
    "\n",
    "# Rename duration to case_duration.\n",
    "evictions_df = evictions_df.rename(columns={'duration': 'case_duration'})\n",
    "\n",
    "# Drop malformed addresses.\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {evictions_df['property_address_full'].str.contains('span, span span').sum()} observations which \"\n",
    "          f\"have malformed addresses.\")\n",
    "evictions_df = evictions_df.loc[~evictions_df['property_address_full'].str.contains(\"span, span span\"), :]\n",
    "\n",
    "# Drop addresses without latitude and longitude.\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {evictions_df[['longitude', 'latitude']].isna().any(axis=1).sum()} evictions missing latitude \"\n",
    "          f\"or longitude.\")\n",
    "evictions_df = evictions_df.dropna(subset=['longitude', 'latitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2019-05       2\n",
       "2019-06     441\n",
       "2019-07    1208\n",
       "2019-08    1865\n",
       "2019-09    1962\n",
       "2019-10    2284\n",
       "2019-11    1939\n",
       "2019-12    2477\n",
       "2020-01    2609\n",
       "2020-02    2415\n",
       "2020-03    2004\n",
       "2020-04     417\n",
       "2020-05     270\n",
       "2020-06     213\n",
       "2020-07     171\n",
       "2020-08     164\n",
       "2020-09     113\n",
       "2020-10     450\n",
       "2020-11    1081\n",
       "2020-12     950\n",
       "2021-01    1024\n",
       "2021-02    1291\n",
       "2021-03    1806\n",
       "2021-04    1651\n",
       "2021-05    1671\n",
       "2021-06    1779\n",
       "2021-07    1617\n",
       "2021-08    1518\n",
       "2021-09    1157\n",
       "2021-10     767\n",
       "2021-11     608\n",
       "2021-12     501\n",
       "2022-01     325\n",
       "2022-02     281\n",
       "2022-03     353\n",
       "2022-04     249\n",
       "2022-05     203\n",
       "2022-06     195\n",
       "2022-07     123\n",
       "2022-08     128\n",
       "2022-09      74\n",
       "2022-10      38\n",
       "2022-11      40\n",
       "2022-12      26\n",
       "2023-01       3\n",
       "Name: latest_docket_month, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evictions_df['latest_docket_month'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Merging Evictions With Census Tract Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully merged 40732 observations with census tracts.\n"
     ]
    }
   ],
   "source": [
    "# Merge with census tract characteristics.\n",
    "evictions_df = evictions_df.rename(columns={'Full FIPS (tract)': 'tract_geoid'})\n",
    "evictions_tracts_df = evictions_df.merge(pd.read_csv(INPUT_DATA_TRACTS, dtype={'tract_geoid': float}),\n",
    "                                  on='tract_geoid',\n",
    "                                  how='left',\n",
    "                                  validate='m:1').set_index('case_number')\n",
    "if VERBOSE:\n",
    "    print(f\"Successfully merged {evictions_tracts_df['med_hhinc2016'].notna().sum()} observations with census tracts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Merging Evictions With Zestimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully matched 11496 evictions (28.22 percent of observations) to Zestimates.\n"
     ]
    }
   ],
   "source": [
    "evictions_tracts_zestimates_df = pd.read_csv(INPUT_DATA_ZESTIMATES, index_col='case_number').merge(evictions_tracts_df,\n",
    "                                                     right_index=True,\n",
    "                                                     left_index=True,\n",
    "                                                     how='right',\n",
    "                                                     validate='1:1')\n",
    "if VERBOSE:\n",
    "    successfully_matched_observations = (~evictions_tracts_zestimates_df['2022-12'].isna()).sum()\n",
    "    print(\n",
    "        f\"Successfully matched {successfully_matched_observations} evictions \"\n",
    "        f\"({100 * (successfully_matched_observations / len(evictions_tracts_zestimates_df)) :.2f} percent of observations) to \"\n",
    "        f\"Zestimates.\")\n",
    "\n",
    "# Rename columns containing Zestimates.\n",
    "years = [str(year) for year in range(2013, 2023)]\n",
    "months = [\"0\" + str(month) for month in range(1, 10)] + [str(month) for month in range(10, 13)]\n",
    "value_vars = [\"2012-12\"] + [str(year) + \"-\" + str(month) for year in years for month in months]\n",
    "value_vars_zestimates, _, _ = generate_variable_names('zestimate')\n",
    "for value_var, value_var_zestimates in zip(value_vars, value_vars_zestimates):\n",
    "    evictions_tracts_zestimates_df = evictions_tracts_zestimates_df.rename(columns={value_var: value_var_zestimates})\n",
    "value_vars_to_concat.append(evictions_tracts_zestimates_df[value_vars_zestimates])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Merge Evictions with Crimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Request computing resources.\n",
    "cluster = SLURMCluster(queue='batch',\n",
    "                       cores=32,\n",
    "                       memory='230 GB',\n",
    "                       walltime='00:20:00',\n",
    "                      scheduler_options={'dashboard_address': '8787'} )\n",
    "cluster.scale(jobs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GeoSeries containing eviction Points as its geometry, case_number as its index, and no other columns.\n",
    "evictions_gdf = gpd.GeoDataFrame(evictions_df, geometry=gpd.points_from_xy(evictions_df['longitude'], evictions_df['latitude']))[['geometry', 'case_number']]\n",
    "evictions_gdf = evictions_gdf.set_crs(\"EPSG:4326\", allow_override=True).to_crs('EPSG:26986')\n",
    "\n",
    "# Read crime data as Dask DataFrame, then compute back to DataFrame.\n",
    "crime_df = (dd.read_csv(INPUT_DATA_CRIME + \"/*.csv\", dtype={'REPORTING_AREA': 'object', 'SHOOTING': 'object'})\n",
    "                .dropna(subset=['Long', 'Lat', 'OCCURRED_ON_DATE'])  # Drop crimes missing latitude, longitude, or date, as they cannot be merged with panel.\n",
    "                .rename(columns={'OCCURRED_ON_DATE': 'month_of_crime_incident'})\n",
    "                .drop(columns=['OFFENSE_CODE_GROUP', 'OFFENSE_DESCRIPTION', 'DISTRICT', 'REPORTING_AREA', 'SHOOTING', 'YEAR', 'MONTH',\n",
    "                               'DAY_OF_WEEK', 'HOUR', 'UCR_PART', 'STREET', 'Location'])  # Drop unneeded columns\n",
    "                .compute())\n",
    "                # Must call compute here and then briefly convert back to in-memory dataset because dask_geopandas.points_from_xy not working.\n",
    "crime_df.loc[:, 'INCIDENT_NUMBER'] = 1  # Replace column with 1s so we can count crimes using sum function.\n",
    "# Keep track of the month of crime incident in YYYY-MM format.\n",
    "crime_df.loc[:, 'month_of_crime_incident'] = pd.to_datetime(crime_df['month_of_crime_incident'].str[:10]).dt.to_period(\"M\").astype(str)\n",
    "# Convert DataFrame to GeoDataFrame.\n",
    "crime_gdf = gpd.GeoDataFrame(crime_df, geometry=gpd.points_from_xy(crime_df['Long'], crime_df['Lat']))\n",
    "crime_gdf = crime_gdf.set_crs(\"EPSG:4326\", allow_override=True).to_crs(\"EPSG:26986\")  # Convert to the correct CRS.\n",
    "# Convert GeoDataFrame to Dask GeoDataFrame.\n",
    "crime_dgdf = dask_geopandas.from_geopandas(crime_gdf, npartitions=N_PARTITIONS).repartition(partition_size='5 MB')\n",
    "\n",
    "\n",
    "# Create lists containing each group of offense codes.\n",
    "# Group 1\n",
    "vandalism = [1402, 1415]\n",
    "# Group 2\n",
    "violence_offense_codes = [111, 801, 802,803, 423, 413, 401, 402, 403, 404, 411, 412, 413, 421, 422, 423, 424, 431, 432 ,433]\n",
    "# Group 3\n",
    "substance_abuse_response_codes = [3023, 3021, 1875, 3022, 1832, 1831, 1830]\n",
    "# Group 4\n",
    "firearm_codes = [3119, 1501, 1503, 1502, 1504, 1510, 3017, 3016, 1501, 3119, 3203, 3204, 1503,\n",
    "                 1502]\n",
    "# Group 5\n",
    "rape_codes = [242, 254, 271, 244, 251, 241, 243,261, 252,253, 222, 230, 236, 211, 232, 233, 224,\n",
    "              213, 223, 231, 237, 212, 234, 235, 242]\n",
    "\n",
    "# Group 6\n",
    "burglary_codes = [2616, 541, 540, 562, 561, 542, 521, 520, 522, 560]\n",
    "larceny_codes = [616, 618, 617, 614, 619, 611, 616, 626, 636, 618, 628, 638, 617, 627,\n",
    "                 637, 614, 624, 634, 619, 629,639,621,611,631,612,622,632,613,623,633,\n",
    "                 615,625,635,649, 612, 613, 615]\n",
    "\n",
    "\n",
    "columns_for_each_outcome = []\n",
    "for offense_group_num, offense_group in enumerate(['all',\n",
    "                                                   vandalism,\n",
    "                                                   violence_offense_codes,\n",
    "                                                   substance_abuse_response_codes,\n",
    "                                                   firearm_codes,\n",
    "                                                   rape_codes,\n",
    "                                                   burglary_codes+larceny_codes]):\n",
    "    for radius in [500]:\n",
    "        # Create a new GeoDataFrame with geometry equal to circles around each eviction with the current radius.\n",
    "        current_evictions_gdf = evictions_gdf.copy()\n",
    "        current_evictions_gdf.geometry = current_evictions_gdf.geometry.buffer(radius)\n",
    "        current_evictions_dgdf = dask_geopandas.from_geopandas(current_evictions_gdf, npartitions=N_PARTITIONS).repartition(partition_size='5 MB')\n",
    "\n",
    "        # Merge evictions with crimes that fall into radius.\n",
    "        current_evictions_crime_dgdf = dask_geopandas.sjoin(crime_dgdf, current_evictions_dgdf, how='inner', predicate='within')\n",
    "        current_evictions_crime_dgdf = current_evictions_crime_dgdf.drop(columns=['geometry','index_right'])\n",
    "        \n",
    "        \n",
    "        if offense_group != 'all':  # If we are summing a specific subcategory of crimes\n",
    "            mask = current_evictions_crime_dgdf['OFFENSE_CODE'].isin(offense_group)\n",
    "            # Multiply mask by 'INCIDENT_NUMBER' to zero out crimes in different offense groups.\n",
    "            current_evictions_crime_dgdf['INCIDENT_NUMBER'] = current_evictions_crime_dgdf['INCIDENT_NUMBER'] * mask\n",
    "\n",
    "            \n",
    "        # Aggregate crimes to case-month level.\n",
    "        current_panel_long = (current_evictions_crime_dgdf\n",
    "                              .groupby(['case_number', 'month_of_crime_incident'])\n",
    "                              .aggregate({'INCIDENT_NUMBER': 'sum'})\n",
    "                              .reset_index()\n",
    "                              .rename(columns={'INCIDENT_NUMBER': 'crime_incidents'})\n",
    "                              .compute())\n",
    "\n",
    "        current_panel_wide = pd.pivot(current_panel_long, index=['case_number'], columns=['month_of_crime_incident'],\n",
    "                                      values='crime_incidents').reset_index().set_index('case_number')\n",
    "        current_panel_wide.columns = [f'{column}_group_{offense_group_num}_crimes_{radius}m' for column in current_panel_wide.columns]\n",
    "        columns_for_each_outcome.append(current_panel_wide.columns)\n",
    "        value_vars_to_concat.append(dd.from_pandas(current_panel_wide, npartitions=N_PARTITIONS))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Producing the Unrestricted Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrestricted_df = dd.multi.concat([dd.from_pandas(evictions_tracts_df, npartitions=N_PARTITIONS)] + value_vars_to_concat, axis=1).compute()\n",
    "# For evictions not matched to any crimes, fill NA values with 0.\n",
    "for group_of_columns in columns_for_each_outcome:\n",
    "    unrestricted_df[group_of_columns] = unrestricted_df[group_of_columns].fillna(0)\n",
    "unrestricted_df.to_parquet(OUTPUT_DATA_UNRESTRICTED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Producing the Samples Used in Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop cases resolved via mediation.\n",
    "mediated_mask = unrestricted_df['disposition_found'] == \"Mediated\"\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {mediated_mask.sum()} cases resolved through mediation.\")\n",
    "unrestricted_df = unrestricted_df.loc[~mediated_mask, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop cases resolved via volntary dismissal (dropped by plaintiff). \n",
    "# voluntary_dismissal_mask = unrestricted_df['disposition'].str.contains(\"R 41(a)(1) Voluntary Dismissal on\", na=False, regex=False)\n",
    "# if VERBOSE:\n",
    "#     print(f\"Droppping {voluntary_dismissal_mask.sum()} cases resolved through voluntary dismissal.\")\n",
    "# unrestricted_df = unrestricted_df.loc[~voluntary_dismissal_mask, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop cases where disposition found is other.\n",
    "disposition_found_other_mask = unrestricted_df['disposition_found'] == \"Other\"\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {disposition_found_other_mask.sum()} cases where disposition_found is \\\"Other\\\"\")\n",
    "unrestricted_df = unrestricted_df.loc[~disposition_found_other_mask, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows which contain inconsistent values of disposition_found and judgment_for_pdu.\n",
    "\n",
    "# First, we drop cases where disposition_found is \"Defaulted\" but judgment_for_pdu is \"Defendant\"\n",
    "inconsistent_mask_1 = ((unrestricted_df['disposition_found'] == \"Defaulted\") & (unrestricted_df['judgment_for_pdu'] == \"Defendant\"))\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {inconsistent_mask_1.sum()} observations where disposition_found is \\\"Defaulted\\\" but judgment_for_pdu is \\\"Defendant\\\".\")\n",
    "unrestricted_df = unrestricted_df.loc[~inconsistent_mask_1, :]\n",
    "          \n",
    "# Next, we drop cases where disposition_found is \"Dismissed\" yet judgment_for_pdu is \"Plaintiff\"\n",
    "inconsistent_mask_2 = ((unrestricted_df['disposition_found'] == \"Dismissed\") & (unrestricted_df['judgment_for_pdu'] == \"Plaintiff\"))\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {inconsistent_mask_2.sum()} observations where disposition_found is \\\"Dismissed\\\" but judgment_for_pdu is \\\"Plaintiff\\\".\")\n",
    "unrestricted_df = unrestricted_df.loc[~inconsistent_mask_2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a variable indicating judgment in favor of defendant.\n",
    "unrestricted_df.loc[:, 'judgment_for_defendant'] = 0\n",
    "defendant_won_mask = ((unrestricted_df['disposition_found'] == \"Dismissed\") |\n",
    "                      (unrestricted_df['judgment_for_pdu'] == \"Defendant\") |\n",
    "                      (unrestricted_df['disposition'].str.contains('R 41(a)(1) Voluntary Dismissal', regex=False)))\n",
    "unrestricted_df.loc[defendant_won_mask, 'judgment_for_defendant'] = 1\n",
    "\n",
    "# Generate a variable indicating judgement in favor of plaintiff.\n",
    "unrestricted_df.loc[:, 'judgment_for_plaintiff'] = 1 - unrestricted_df['judgment_for_defendant']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6a. Producing the Zillow Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where we are missing any Zestimates.\n",
    "zestimates_df = unrestricted_df.copy()\n",
    "has_all_zestimates_mask = zestimates_df[value_vars_zestimates].notna().all(axis=1)\n",
    "if VERBOSE:\n",
    "    print(f\"Limiting sample to {has_all_zestimates_mask.sum()} evictions for which we observe Zestimates at every month from 2012-12 to 2022-12.\")\n",
    "zestimates_df = zestimates_df.loc[has_all_zestimates_mask, :]\n",
    "zestimates_df.to_csv(OUTPUT_DATA_ZILLOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6b. Producing the Crime Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_df = unrestricted_df.copy()\n",
    "# Restrict to evictions which took place in Boston.\n",
    "boston_mask = ((crime_df['County'] == \"Suffolk County\") & (~crime_df['City'].isin([\"Chelsea\", \"Revere\", \"Winthrop\"])))\n",
    "if VERBOSE:\n",
    "    print(f\"Limiting sample to {boston_mask.sum()} observations which are in Boston.\")\n",
    "crime_df = crime_df.loc[boston_mask, :]\n",
    "crime_df.to_parquet(OUTPUT_DATA_CRIME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
