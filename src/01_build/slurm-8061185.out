## SLURM PROLOG ###############################################################
##    Job ID : 8061185
##  Job Name : dask-worker
##  Nodelist : node1161
##      CPUs : 1
##  Mem/Node : 10240 MB
## Directory : /gpfs/home/ashanmu1/seniorthesis/src/01_build
##   Job Started : Fri Feb  3 12:08:09 EST 2023
###############################################################################
2023-02-03 12:08:09,715 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.207.61:40074'
2023-02-03 12:08:09,718 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.207.61:46448'
2023-02-03 12:08:09,718 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.207.61:44501'
2023-02-03 12:08:09,724 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.207.61:35652'
2023-02-03 12:08:10,729 - distributed.worker - INFO -       Start worker at:  tcp://172.20.207.61:42974
2023-02-03 12:08:10,729 - distributed.worker - INFO -          Listening to:  tcp://172.20.207.61:42974
2023-02-03 12:08:10,729 - distributed.worker - INFO -           Worker name:          SLURMCluster-19-2
2023-02-03 12:08:10,729 - distributed.worker - INFO -          dashboard at:        172.20.207.61:37567
2023-02-03 12:08:10,729 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.2:33176
2023-02-03 12:08:10,729 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 12:08:10,729 - distributed.worker - INFO -               Threads:                          1
2023-02-03 12:08:10,729 - distributed.worker - INFO -                Memory:                   2.33 GiB
2023-02-03 12:08:10,729 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ihk7qdik
2023-02-03 12:08:10,729 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 12:08:10,747 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.2:33176
2023-02-03 12:08:10,747 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 12:08:10,748 - distributed.core - INFO - Starting established connection to tcp://172.20.207.2:33176
2023-02-03 12:08:10,772 - distributed.worker - INFO -       Start worker at:  tcp://172.20.207.61:40821
2023-02-03 12:08:10,772 - distributed.worker - INFO -          Listening to:  tcp://172.20.207.61:40821
2023-02-03 12:08:10,772 - distributed.worker - INFO -           Worker name:          SLURMCluster-19-0
2023-02-03 12:08:10,772 - distributed.worker - INFO -          dashboard at:        172.20.207.61:43261
2023-02-03 12:08:10,772 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.2:33176
2023-02-03 12:08:10,772 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 12:08:10,772 - distributed.worker - INFO -               Threads:                          1
2023-02-03 12:08:10,773 - distributed.worker - INFO -                Memory:                   2.33 GiB
2023-02-03 12:08:10,773 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4ces4k98
2023-02-03 12:08:10,773 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 12:08:10,778 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.2:33176
2023-02-03 12:08:10,778 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 12:08:10,778 - distributed.core - INFO - Starting established connection to tcp://172.20.207.2:33176
2023-02-03 12:08:10,813 - distributed.worker - INFO -       Start worker at:  tcp://172.20.207.61:34056
2023-02-03 12:08:10,814 - distributed.worker - INFO -          Listening to:  tcp://172.20.207.61:34056
2023-02-03 12:08:10,814 - distributed.worker - INFO -           Worker name:          SLURMCluster-19-3
2023-02-03 12:08:10,814 - distributed.worker - INFO -          dashboard at:        172.20.207.61:41537
2023-02-03 12:08:10,814 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.2:33176
2023-02-03 12:08:10,814 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 12:08:10,814 - distributed.worker - INFO -               Threads:                          1
2023-02-03 12:08:10,814 - distributed.worker - INFO -                Memory:                   2.33 GiB
2023-02-03 12:08:10,814 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-t2sr9452
2023-02-03 12:08:10,815 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 12:08:10,819 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.2:33176
2023-02-03 12:08:10,819 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 12:08:10,819 - distributed.core - INFO - Starting established connection to tcp://172.20.207.2:33176
2023-02-03 12:08:10,853 - distributed.worker - INFO -       Start worker at:  tcp://172.20.207.61:42345
2023-02-03 12:08:10,853 - distributed.worker - INFO -          Listening to:  tcp://172.20.207.61:42345
2023-02-03 12:08:10,853 - distributed.worker - INFO -           Worker name:          SLURMCluster-19-1
2023-02-03 12:08:10,853 - distributed.worker - INFO -          dashboard at:        172.20.207.61:34978
2023-02-03 12:08:10,853 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.2:33176
2023-02-03 12:08:10,853 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 12:08:10,853 - distributed.worker - INFO -               Threads:                          1
2023-02-03 12:08:10,853 - distributed.worker - INFO -                Memory:                   2.33 GiB
2023-02-03 12:08:10,853 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-cikfy7nk
2023-02-03 12:08:10,853 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 12:08:10,858 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.2:33176
2023-02-03 12:08:10,858 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 12:08:10,858 - distributed.core - INFO - Starting established connection to tcp://172.20.207.2:33176
slurmstepd: error: *** JOB 8061185 ON node1161 CANCELLED AT 2023-02-03T12:08:18 ***
