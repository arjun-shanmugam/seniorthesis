## SLURM PROLOG ###############################################################
##    Job ID : 8061183
##  Job Name : dask-worker
##  Nodelist : node1155
##      CPUs : 1
##  Mem/Node : 10240 MB
## Directory : /gpfs/home/ashanmu1/seniorthesis/src/01_build
##   Job Started : Fri Feb  3 12:08:09 EST 2023
###############################################################################
2023-02-03 12:08:09,727 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.207.55:36189'
2023-02-03 12:08:09,731 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.207.55:40569'
2023-02-03 12:08:09,738 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.207.55:39401'
2023-02-03 12:08:09,738 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.207.55:33054'
2023-02-03 12:08:10,642 - distributed.worker - INFO -       Start worker at:  tcp://172.20.207.55:35027
2023-02-03 12:08:10,643 - distributed.worker - INFO -          Listening to:  tcp://172.20.207.55:35027
2023-02-03 12:08:10,643 - distributed.worker - INFO -           Worker name:           SLURMCluster-5-2
2023-02-03 12:08:10,643 - distributed.worker - INFO -          dashboard at:        172.20.207.55:37441
2023-02-03 12:08:10,643 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.2:33176
2023-02-03 12:08:10,643 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 12:08:10,643 - distributed.worker - INFO -               Threads:                          1
2023-02-03 12:08:10,643 - distributed.worker - INFO -                Memory:                   2.33 GiB
2023-02-03 12:08:10,643 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pfnr83qd
2023-02-03 12:08:10,643 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 12:08:10,652 - distributed.worker - INFO -       Start worker at:  tcp://172.20.207.55:45763
2023-02-03 12:08:10,652 - distributed.worker - INFO -          Listening to:  tcp://172.20.207.55:45763
2023-02-03 12:08:10,652 - distributed.worker - INFO -           Worker name:           SLURMCluster-5-0
2023-02-03 12:08:10,653 - distributed.worker - INFO -          dashboard at:        172.20.207.55:34585
2023-02-03 12:08:10,653 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.2:33176
2023-02-03 12:08:10,653 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 12:08:10,653 - distributed.worker - INFO -               Threads:                          1
2023-02-03 12:08:10,653 - distributed.worker - INFO -                Memory:                   2.33 GiB
2023-02-03 12:08:10,653 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ip19f78m
2023-02-03 12:08:10,653 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 12:08:10,654 - distributed.worker - INFO -       Start worker at:  tcp://172.20.207.55:42586
2023-02-03 12:08:10,654 - distributed.worker - INFO -          Listening to:  tcp://172.20.207.55:42586
2023-02-03 12:08:10,654 - distributed.worker - INFO -           Worker name:           SLURMCluster-5-3
2023-02-03 12:08:10,654 - distributed.worker - INFO -          dashboard at:        172.20.207.55:38322
2023-02-03 12:08:10,654 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.2:33176
2023-02-03 12:08:10,654 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 12:08:10,654 - distributed.worker - INFO -               Threads:                          1
2023-02-03 12:08:10,654 - distributed.worker - INFO -                Memory:                   2.33 GiB
2023-02-03 12:08:10,654 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-31yid28o
2023-02-03 12:08:10,654 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 12:08:10,658 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.2:33176
2023-02-03 12:08:10,659 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 12:08:10,659 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.2:33176
2023-02-03 12:08:10,659 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 12:08:10,659 - distributed.core - INFO - Starting established connection to tcp://172.20.207.2:33176
2023-02-03 12:08:10,659 - distributed.core - INFO - Starting established connection to tcp://172.20.207.2:33176
2023-02-03 12:08:10,660 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.2:33176
2023-02-03 12:08:10,660 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 12:08:10,661 - distributed.core - INFO - Starting established connection to tcp://172.20.207.2:33176
2023-02-03 12:08:10,680 - distributed.worker - INFO -       Start worker at:  tcp://172.20.207.55:45766
2023-02-03 12:08:10,681 - distributed.worker - INFO -          Listening to:  tcp://172.20.207.55:45766
2023-02-03 12:08:10,681 - distributed.worker - INFO -           Worker name:           SLURMCluster-5-1
2023-02-03 12:08:10,681 - distributed.worker - INFO -          dashboard at:        172.20.207.55:35923
2023-02-03 12:08:10,681 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.2:33176
2023-02-03 12:08:10,681 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 12:08:10,681 - distributed.worker - INFO -               Threads:                          1
2023-02-03 12:08:10,681 - distributed.worker - INFO -                Memory:                   2.33 GiB
2023-02-03 12:08:10,681 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ut8vmp_q
2023-02-03 12:08:10,681 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 12:08:10,685 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.2:33176
2023-02-03 12:08:10,686 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 12:08:10,686 - distributed.core - INFO - Starting established connection to tcp://172.20.207.2:33176
slurmstepd: error: *** JOB 8061183 ON node1155 CANCELLED AT 2023-02-03T12:08:18 ***
