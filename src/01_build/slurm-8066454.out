## SLURM PROLOG ###############################################################
##    Job ID : 8066454
##  Job Name : dask-worker
##  Nodelist : node1624
##      CPUs : 1
##  Mem/Node : 96256 MB
## Directory : /gpfs/home/ashanmu1/seniorthesis/src/01_build
##   Job Started : Fri Feb  3 18:54:42 EST 2023
###############################################################################
2023-02-03 18:54:42,944 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.212.24:34468'
2023-02-03 18:54:42,947 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.212.24:40371'
2023-02-03 18:54:42,948 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.212.24:37999'
2023-02-03 18:54:42,949 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.212.24:39849'
2023-02-03 18:54:42,949 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.212.24:41485'
2023-02-03 18:54:42,957 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.212.24:39427'
2023-02-03 18:54:42,959 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.212.24:41626'
2023-02-03 18:54:42,959 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.212.24:37059'
2023-02-03 18:54:43,693 - distributed.worker - INFO -       Start worker at:  tcp://172.20.212.24:44641
2023-02-03 18:54:43,694 - distributed.worker - INFO -          Listening to:  tcp://172.20.212.24:44641
2023-02-03 18:54:43,694 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-0
2023-02-03 18:54:43,694 - distributed.worker - INFO -          dashboard at:        172.20.212.24:37848
2023-02-03 18:54:43,694 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.1:45232
2023-02-03 18:54:43,694 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 18:54:43,694 - distributed.worker - INFO -               Threads:                          4
2023-02-03 18:54:43,694 - distributed.worker - INFO -                Memory:                  11.64 GiB
2023-02-03 18:54:43,694 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8wbnnfy_
2023-02-03 18:54:43,694 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 18:54:43,701 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.1:45232
2023-02-03 18:54:43,701 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 18:54:43,701 - distributed.core - INFO - Starting established connection to tcp://172.20.207.1:45232
2023-02-03 18:54:43,729 - distributed.worker - INFO -       Start worker at:  tcp://172.20.212.24:45753
2023-02-03 18:54:43,729 - distributed.worker - INFO -          Listening to:  tcp://172.20.212.24:45753
2023-02-03 18:54:43,729 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-5
2023-02-03 18:54:43,729 - distributed.worker - INFO -          dashboard at:        172.20.212.24:38857
2023-02-03 18:54:43,729 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.1:45232
2023-02-03 18:54:43,729 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 18:54:43,729 - distributed.worker - INFO -               Threads:                          4
2023-02-03 18:54:43,729 - distributed.worker - INFO -                Memory:                  11.64 GiB
2023-02-03 18:54:43,729 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-snj9q86k
2023-02-03 18:54:43,729 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 18:54:43,732 - distributed.worker - INFO -       Start worker at:  tcp://172.20.212.24:37587
2023-02-03 18:54:43,733 - distributed.worker - INFO -          Listening to:  tcp://172.20.212.24:37587
2023-02-03 18:54:43,733 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-2
2023-02-03 18:54:43,733 - distributed.worker - INFO -          dashboard at:        172.20.212.24:39053
2023-02-03 18:54:43,733 - distributed.worker - INFO -       Start worker at:  tcp://172.20.212.24:34460
2023-02-03 18:54:43,734 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.1:45232
2023-02-03 18:54:43,734 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 18:54:43,734 - distributed.worker - INFO -          Listening to:  tcp://172.20.212.24:34460
2023-02-03 18:54:43,734 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-1
2023-02-03 18:54:43,734 - distributed.worker - INFO -          dashboard at:        172.20.212.24:35513
2023-02-03 18:54:43,734 - distributed.worker - INFO -               Threads:                          4
2023-02-03 18:54:43,734 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.1:45232
2023-02-03 18:54:43,734 - distributed.worker - INFO -                Memory:                  11.64 GiB
2023-02-03 18:54:43,734 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 18:54:43,734 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5nhsz9l1
2023-02-03 18:54:43,734 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 18:54:43,734 - distributed.worker - INFO -               Threads:                          4
2023-02-03 18:54:43,734 - distributed.worker - INFO -                Memory:                  11.64 GiB
2023-02-03 18:54:43,734 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rv45skse
2023-02-03 18:54:43,734 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 18:54:43,736 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.1:45232
2023-02-03 18:54:43,736 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 18:54:43,736 - distributed.core - INFO - Starting established connection to tcp://172.20.207.1:45232
2023-02-03 18:54:43,739 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.1:45232
2023-02-03 18:54:43,739 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 18:54:43,739 - distributed.core - INFO - Starting established connection to tcp://172.20.207.1:45232
2023-02-03 18:54:43,739 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.1:45232
2023-02-03 18:54:43,739 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 18:54:43,740 - distributed.core - INFO - Starting established connection to tcp://172.20.207.1:45232
2023-02-03 18:54:43,740 - distributed.worker - INFO -       Start worker at:  tcp://172.20.212.24:37504
2023-02-03 18:54:43,740 - distributed.worker - INFO -          Listening to:  tcp://172.20.212.24:37504
2023-02-03 18:54:43,740 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-7
2023-02-03 18:54:43,740 - distributed.worker - INFO -          dashboard at:        172.20.212.24:43195
2023-02-03 18:54:43,740 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.1:45232
2023-02-03 18:54:43,740 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 18:54:43,740 - distributed.worker - INFO -               Threads:                          4
2023-02-03 18:54:43,740 - distributed.worker - INFO -                Memory:                  11.64 GiB
2023-02-03 18:54:43,740 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fl1qm_rx
2023-02-03 18:54:43,740 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 18:54:43,741 - distributed.worker - INFO -       Start worker at:  tcp://172.20.212.24:40044
2023-02-03 18:54:43,741 - distributed.worker - INFO -          Listening to:  tcp://172.20.212.24:40044
2023-02-03 18:54:43,741 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-3
2023-02-03 18:54:43,741 - distributed.worker - INFO -          dashboard at:        172.20.212.24:39135
2023-02-03 18:54:43,741 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.1:45232
2023-02-03 18:54:43,741 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 18:54:43,741 - distributed.worker - INFO -               Threads:                          4
2023-02-03 18:54:43,741 - distributed.worker - INFO -                Memory:                  11.64 GiB
2023-02-03 18:54:43,741 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-znv_5b4x
2023-02-03 18:54:43,741 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 18:54:43,743 - distributed.worker - INFO -       Start worker at:  tcp://172.20.212.24:33481
2023-02-03 18:54:43,743 - distributed.worker - INFO -          Listening to:  tcp://172.20.212.24:33481
2023-02-03 18:54:43,743 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-6
2023-02-03 18:54:43,743 - distributed.worker - INFO -          dashboard at:        172.20.212.24:43156
2023-02-03 18:54:43,743 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.1:45232
2023-02-03 18:54:43,743 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 18:54:43,743 - distributed.worker - INFO -               Threads:                          4
2023-02-03 18:54:43,744 - distributed.worker - INFO -                Memory:                  11.64 GiB
2023-02-03 18:54:43,744 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dgx__weq
2023-02-03 18:54:43,744 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 18:54:43,745 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.1:45232
2023-02-03 18:54:43,745 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 18:54:43,746 - distributed.core - INFO - Starting established connection to tcp://172.20.207.1:45232
2023-02-03 18:54:43,746 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.1:45232
2023-02-03 18:54:43,746 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 18:54:43,746 - distributed.core - INFO - Starting established connection to tcp://172.20.207.1:45232
2023-02-03 18:54:43,749 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.1:45232
2023-02-03 18:54:43,749 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 18:54:43,749 - distributed.core - INFO - Starting established connection to tcp://172.20.207.1:45232
2023-02-03 18:54:43,753 - distributed.worker - INFO -       Start worker at:  tcp://172.20.212.24:34794
2023-02-03 18:54:43,753 - distributed.worker - INFO -          Listening to:  tcp://172.20.212.24:34794
2023-02-03 18:54:43,753 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-4
2023-02-03 18:54:43,753 - distributed.worker - INFO -          dashboard at:        172.20.212.24:34302
2023-02-03 18:54:43,753 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.1:45232
2023-02-03 18:54:43,753 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 18:54:43,753 - distributed.worker - INFO -               Threads:                          4
2023-02-03 18:54:43,754 - distributed.worker - INFO -                Memory:                  11.64 GiB
2023-02-03 18:54:43,754 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6ywswz4w
2023-02-03 18:54:43,754 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 18:54:43,758 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.1:45232
2023-02-03 18:54:43,758 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 18:54:43,758 - distributed.core - INFO - Starting established connection to tcp://172.20.207.1:45232
2023-02-03 18:57:11,016 - distributed.core - INFO - Connection to tcp://172.20.207.1:45232 has been closed.
2023-02-03 18:57:11,016 - distributed.core - INFO - Connection to tcp://172.20.207.1:45232 has been closed.
2023-02-03 18:57:11,016 - distributed.core - INFO - Connection to tcp://172.20.207.1:45232 has been closed.
2023-02-03 18:57:11,016 - distributed.core - INFO - Connection to tcp://172.20.207.1:45232 has been closed.
2023-02-03 18:57:11,016 - distributed.worker - INFO - Stopping worker at tcp://172.20.212.24:37587. Reason: worker-handle-scheduler-connection-broken
2023-02-03 18:57:11,016 - distributed.worker - INFO - Stopping worker at tcp://172.20.212.24:33481. Reason: worker-handle-scheduler-connection-broken
2023-02-03 18:57:11,016 - distributed.worker - INFO - Stopping worker at tcp://172.20.212.24:34460. Reason: worker-handle-scheduler-connection-broken
2023-02-03 18:57:11,016 - distributed.worker - INFO - Stopping worker at tcp://172.20.212.24:37504. Reason: worker-handle-scheduler-connection-broken
2023-02-03 18:57:11,016 - distributed.core - INFO - Connection to tcp://172.20.207.1:45232 has been closed.
2023-02-03 18:57:11,017 - distributed.worker - INFO - Stopping worker at tcp://172.20.212.24:45753. Reason: worker-handle-scheduler-connection-broken
2023-02-03 18:57:11,016 - distributed.core - INFO - Connection to tcp://172.20.207.1:45232 has been closed.
2023-02-03 18:57:11,016 - distributed.core - INFO - Connection to tcp://172.20.207.1:45232 has been closed.
2023-02-03 18:57:11,017 - distributed.worker - INFO - Stopping worker at tcp://172.20.212.24:44641. Reason: worker-handle-scheduler-connection-broken
2023-02-03 18:57:11,016 - distributed.core - INFO - Connection to tcp://172.20.207.1:45232 has been closed.
2023-02-03 18:57:11,017 - distributed.worker - INFO - Stopping worker at tcp://172.20.212.24:40044. Reason: worker-handle-scheduler-connection-broken
2023-02-03 18:57:11,017 - distributed.worker - INFO - Stopping worker at tcp://172.20.212.24:34794. Reason: worker-handle-scheduler-connection-broken
2023-02-03 18:57:11,019 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://172.20.212.24:39849'. Reason: worker-handle-scheduler-connection-broken
2023-02-03 18:57:11,020 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://172.20.212.24:41626'. Reason: worker-handle-scheduler-connection-broken
2023-02-03 18:57:11,020 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://172.20.212.24:37999'. Reason: worker-handle-scheduler-connection-broken
2023-02-03 18:57:11,021 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://172.20.212.24:37059'. Reason: worker-handle-scheduler-connection-broken
2023-02-03 18:57:11,021 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://172.20.212.24:40371'. Reason: worker-handle-scheduler-connection-broken
2023-02-03 18:57:11,021 - distributed.nanny - INFO - Worker closed
2023-02-03 18:57:11,022 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://172.20.212.24:34468'. Reason: worker-handle-scheduler-connection-broken
2023-02-03 18:57:11,022 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://172.20.212.24:41485'. Reason: worker-handle-scheduler-connection-broken
2023-02-03 18:57:11,022 - distributed.nanny - INFO - Worker closed
2023-02-03 18:57:11,022 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://172.20.212.24:39427'. Reason: worker-handle-scheduler-connection-broken
2023-02-03 18:57:11,022 - distributed.nanny - INFO - Worker closed
2023-02-03 18:57:11,023 - distributed.nanny - INFO - Worker closed
2023-02-03 18:57:11,024 - distributed.nanny - INFO - Worker closed
2023-02-03 18:57:11,025 - distributed.nanny - INFO - Worker closed
2023-02-03 18:57:11,026 - distributed.nanny - INFO - Worker closed
2023-02-03 18:57:11,026 - distributed.nanny - INFO - Worker closed
2023-02-03 18:57:13,025 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-02-03 18:57:13,026 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-02-03 18:57:13,027 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-02-03 18:57:13,282 - distributed.nanny - INFO - Closing Nanny at 'tcp://172.20.212.24:37059'. Reason: nanny-close-gracefully
2023-02-03 18:57:13,285 - distributed.nanny - INFO - Closing Nanny at 'tcp://172.20.212.24:37999'. Reason: nanny-close-gracefully
2023-02-03 18:57:13,286 - distributed.nanny - INFO - Closing Nanny at 'tcp://172.20.212.24:41626'. Reason: nanny-close-gracefully
2023-02-03 18:57:13,286 - distributed.nanny - INFO - Closing Nanny at 'tcp://172.20.212.24:39849'. Reason: nanny-close-gracefully
2023-02-03 18:57:13,286 - distributed.nanny - INFO - Closing Nanny at 'tcp://172.20.212.24:40371'. Reason: nanny-close-gracefully
2023-02-03 18:57:13,292 - distributed.nanny - INFO - Closing Nanny at 'tcp://172.20.212.24:34468'. Reason: nanny-close-gracefully
2023-02-03 18:57:13,294 - distributed.nanny - INFO - Closing Nanny at 'tcp://172.20.212.24:39427'. Reason: nanny-close-gracefully
2023-02-03 18:57:13,297 - distributed.nanny - INFO - Closing Nanny at 'tcp://172.20.212.24:41485'. Reason: nanny-close-gracefully
2023-02-03 18:57:13,298 - distributed.dask_worker - INFO - End worker
