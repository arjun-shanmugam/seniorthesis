## SLURM PROLOG ###############################################################
##    Job ID : 8063403
##  Job Name : dask-worker
##  Nodelist : node1826
##      CPUs : 1
##  Mem/Node : 191488 MB
## Directory : /gpfs/home/ashanmu1/seniorthesis/src/01_build
##   Job Started : Fri Feb  3 14:40:19 EST 2023
###############################################################################
2023-02-03 14:40:21,069 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.214.26:46855'
2023-02-03 14:40:21,076 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.214.26:35274'
2023-02-03 14:40:21,077 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.214.26:45286'
2023-02-03 14:40:21,079 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.214.26:33411'
2023-02-03 14:40:21,079 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.214.26:38852'
2023-02-03 14:40:21,085 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.214.26:41527'
2023-02-03 14:40:21,087 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.214.26:34499'
2023-02-03 14:40:21,087 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.214.26:39689'
2023-02-03 14:40:22,718 - distributed.worker - INFO -       Start worker at:  tcp://172.20.214.26:35438
2023-02-03 14:40:22,718 - distributed.worker - INFO -       Start worker at:  tcp://172.20.214.26:38414
2023-02-03 14:40:22,718 - distributed.worker - INFO -          Listening to:  tcp://172.20.214.26:35438
2023-02-03 14:40:22,718 - distributed.worker - INFO -       Start worker at:  tcp://172.20.214.26:42240
2023-02-03 14:40:22,718 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-7
2023-02-03 14:40:22,718 - distributed.worker - INFO -       Start worker at:  tcp://172.20.214.26:44265
2023-02-03 14:40:22,718 - distributed.worker - INFO -          Listening to:  tcp://172.20.214.26:38414
2023-02-03 14:40:22,718 - distributed.worker - INFO -          dashboard at:        172.20.214.26:35801
2023-02-03 14:40:22,718 - distributed.worker - INFO -          Listening to:  tcp://172.20.214.26:42240
2023-02-03 14:40:22,718 - distributed.worker - INFO -       Start worker at:  tcp://172.20.214.26:37438
2023-02-03 14:40:22,718 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-2
2023-02-03 14:40:22,718 - distributed.worker - INFO -          Listening to:  tcp://172.20.214.26:44265
2023-02-03 14:40:22,718 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.2:41954
2023-02-03 14:40:22,718 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-1
2023-02-03 14:40:22,718 - distributed.worker - INFO -       Start worker at:  tcp://172.20.214.26:44176
2023-02-03 14:40:22,718 - distributed.worker - INFO -       Start worker at:  tcp://172.20.214.26:41766
2023-02-03 14:40:22,718 - distributed.worker - INFO -          dashboard at:        172.20.214.26:39647
2023-02-03 14:40:22,718 - distributed.worker - INFO -       Start worker at:  tcp://172.20.214.26:36012
2023-02-03 14:40:22,718 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 14:40:22,718 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-6
2023-02-03 14:40:22,718 - distributed.worker - INFO -          Listening to:  tcp://172.20.214.26:37438
2023-02-03 14:40:22,718 - distributed.worker - INFO -          dashboard at:        172.20.214.26:35915
2023-02-03 14:40:22,718 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.2:41954
2023-02-03 14:40:22,718 - distributed.worker - INFO -          Listening to:  tcp://172.20.214.26:44176
2023-02-03 14:40:22,718 - distributed.worker - INFO -          Listening to:  tcp://172.20.214.26:41766
2023-02-03 14:40:22,718 - distributed.worker - INFO -          dashboard at:        172.20.214.26:38866
2023-02-03 14:40:22,718 - distributed.worker - INFO -          Listening to:  tcp://172.20.214.26:36012
2023-02-03 14:40:22,718 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-3
2023-02-03 14:40:22,718 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 14:40:22,718 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.2:41954
2023-02-03 14:40:22,720 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-4
2023-02-03 14:40:22,718 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-5
2023-02-03 14:40:22,720 - distributed.worker - INFO -          dashboard at:        172.20.214.26:38542
2023-02-03 14:40:22,719 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.2:41954
2023-02-03 14:40:22,720 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 14:40:22,720 - distributed.worker - INFO -          dashboard at:        172.20.214.26:41991
2023-02-03 14:40:22,719 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-0
2023-02-03 14:40:22,720 - distributed.worker - INFO -          dashboard at:        172.20.214.26:37584
2023-02-03 14:40:22,720 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.2:41954
2023-02-03 14:40:22,721 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 14:40:22,721 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.2:41954
2023-02-03 14:40:22,718 - distributed.worker - INFO -               Threads:                          4
2023-02-03 14:40:22,720 - distributed.worker - INFO -               Threads:                          4
2023-02-03 14:40:22,721 - distributed.worker - INFO -          dashboard at:        172.20.214.26:44413
2023-02-03 14:40:22,721 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 14:40:22,721 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.2:41954
2023-02-03 14:40:22,721 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 14:40:22,721 - distributed.worker - INFO -                Memory:                  23.28 GiB
2023-02-03 14:40:22,721 - distributed.worker - INFO -                Memory:                  23.28 GiB
2023-02-03 14:40:22,721 - distributed.worker - INFO -               Threads:                          4
2023-02-03 14:40:22,721 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.2:41954
2023-02-03 14:40:22,721 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 14:40:22,721 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-yyahvk_q
2023-02-03 14:40:22,721 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-89g4d_c_
2023-02-03 14:40:22,721 - distributed.worker - INFO -               Threads:                          4
2023-02-03 14:40:22,721 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 14:40:22,721 - distributed.worker - INFO -                Memory:                  23.28 GiB
2023-02-03 14:40:22,721 - distributed.worker - INFO -                Memory:                  23.28 GiB
2023-02-03 14:40:22,721 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 14:40:22,721 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-r2xdu5f3
2023-02-03 14:40:22,721 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 14:40:22,721 - distributed.worker - INFO -               Threads:                          4
2023-02-03 14:40:22,721 - distributed.worker - INFO -               Threads:                          4
2023-02-03 14:40:22,721 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-u5a9twzo
2023-02-03 14:40:22,721 - distributed.worker - INFO -               Threads:                          4
2023-02-03 14:40:22,721 - distributed.worker - INFO -                Memory:                  23.28 GiB
2023-02-03 14:40:22,721 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 14:40:22,721 - distributed.worker - INFO -                Memory:                  23.28 GiB
2023-02-03 14:40:22,721 - distributed.worker - INFO -               Threads:                          4
2023-02-03 14:40:22,721 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 14:40:22,721 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-owogm0jm
2023-02-03 14:40:22,721 - distributed.worker - INFO -                Memory:                  23.28 GiB
2023-02-03 14:40:22,721 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3v3h89_g
2023-02-03 14:40:22,721 - distributed.worker - INFO -                Memory:                  23.28 GiB
2023-02-03 14:40:22,721 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-n2u3w4cx
2023-02-03 14:40:22,721 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 14:40:22,721 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-iaegucmb
2023-02-03 14:40:22,721 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 14:40:22,721 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 14:40:22,721 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 14:40:22,737 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.2:41954
2023-02-03 14:40:22,737 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 14:40:22,737 - distributed.core - INFO - Starting established connection to tcp://172.20.207.2:41954
2023-02-03 14:40:22,738 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.2:41954
2023-02-03 14:40:22,738 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 14:40:22,738 - distributed.core - INFO - Starting established connection to tcp://172.20.207.2:41954
2023-02-03 14:40:22,739 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.2:41954
2023-02-03 14:40:22,739 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 14:40:22,739 - distributed.core - INFO - Starting established connection to tcp://172.20.207.2:41954
2023-02-03 14:40:22,739 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.2:41954
2023-02-03 14:40:22,739 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 14:40:22,740 - distributed.core - INFO - Starting established connection to tcp://172.20.207.2:41954
2023-02-03 14:40:22,740 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.2:41954
2023-02-03 14:40:22,740 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 14:40:22,741 - distributed.core - INFO - Starting established connection to tcp://172.20.207.2:41954
2023-02-03 14:40:22,741 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.2:41954
2023-02-03 14:40:22,741 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 14:40:22,742 - distributed.core - INFO - Starting established connection to tcp://172.20.207.2:41954
2023-02-03 14:40:22,742 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.2:41954
2023-02-03 14:40:22,742 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 14:40:22,742 - distributed.core - INFO - Starting established connection to tcp://172.20.207.2:41954
2023-02-03 14:40:22,743 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.2:41954
2023-02-03 14:40:22,743 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 14:40:22,743 - distributed.core - INFO - Starting established connection to tcp://172.20.207.2:41954
slurmstepd: error: *** JOB 8063403 ON node1826 CANCELLED AT 2023-02-03T14:50:20 DUE TO TIME LIMIT ***
