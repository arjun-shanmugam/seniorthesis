## SLURM PROLOG ###############################################################
##    Job ID : 8117443
##  Job Name : dask-worker
##  Nodelist : node1363
##      CPUs : 1
##  Mem/Node : 220160 MB
## Directory : /gpfs/home/ashanmu1/seniorthesis/src/01_build
##   Job Started : Tue Feb  7 19:58:38 EST 2023
###############################################################################
2023-02-07 19:58:39,985 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.209.63:45578'
2023-02-07 19:58:39,992 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.209.63:41535'
2023-02-07 19:58:39,995 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.209.63:35183'
2023-02-07 19:58:39,995 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.209.63:33429'
2023-02-07 19:58:39,995 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.209.63:37689'
2023-02-07 19:58:39,995 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.209.63:45661'
2023-02-07 19:58:39,997 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.209.63:45750'
2023-02-07 19:58:39,997 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.209.63:38423'
2023-02-07 19:58:41,601 - distributed.worker - INFO -       Start worker at:  tcp://172.20.209.63:34364
2023-02-07 19:58:41,601 - distributed.worker - INFO -       Start worker at:  tcp://172.20.209.63:45196
2023-02-07 19:58:41,601 - distributed.worker - INFO -       Start worker at:  tcp://172.20.209.63:38121
2023-02-07 19:58:41,601 - distributed.worker - INFO -       Start worker at:  tcp://172.20.209.63:38080
2023-02-07 19:58:41,601 - distributed.worker - INFO -          Listening to:  tcp://172.20.209.63:34364
2023-02-07 19:58:41,601 - distributed.worker - INFO -          Listening to:  tcp://172.20.209.63:45196
2023-02-07 19:58:41,601 - distributed.worker - INFO -          Listening to:  tcp://172.20.209.63:38121
2023-02-07 19:58:41,601 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-3
2023-02-07 19:58:41,601 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-0
2023-02-07 19:58:41,601 - distributed.worker - INFO -          Listening to:  tcp://172.20.209.63:38080
2023-02-07 19:58:41,601 - distributed.worker - INFO -       Start worker at:  tcp://172.20.209.63:42105
2023-02-07 19:58:41,601 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-5
2023-02-07 19:58:41,601 - distributed.worker - INFO -       Start worker at:  tcp://172.20.209.63:43743
2023-02-07 19:58:41,601 - distributed.worker - INFO -       Start worker at:  tcp://172.20.209.63:44977
2023-02-07 19:58:41,601 - distributed.worker - INFO -          dashboard at:        172.20.209.63:36918
2023-02-07 19:58:41,601 - distributed.worker - INFO -          dashboard at:        172.20.209.63:34165
2023-02-07 19:58:41,601 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-1
2023-02-07 19:58:41,601 - distributed.worker - INFO -       Start worker at:  tcp://172.20.209.63:34018
2023-02-07 19:58:41,601 - distributed.worker - INFO -          dashboard at:        172.20.209.63:35564
2023-02-07 19:58:41,601 - distributed.worker - INFO -          Listening to:  tcp://172.20.209.63:42105
2023-02-07 19:58:41,601 - distributed.worker - INFO -          Listening to:  tcp://172.20.209.63:43743
2023-02-07 19:58:41,601 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.2:46353
2023-02-07 19:58:41,601 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.2:46353
2023-02-07 19:58:41,601 - distributed.worker - INFO -          dashboard at:        172.20.209.63:45135
2023-02-07 19:58:41,601 - distributed.worker - INFO -          Listening to:  tcp://172.20.209.63:44977
2023-02-07 19:58:41,601 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.2:46353
2023-02-07 19:58:41,601 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-7
2023-02-07 19:58:41,601 - distributed.worker - INFO -          Listening to:  tcp://172.20.209.63:34018
2023-02-07 19:58:41,601 - distributed.worker - INFO - -------------------------------------------------
2023-02-07 19:58:41,602 - distributed.worker - INFO -          dashboard at:        172.20.209.63:43569
2023-02-07 19:58:41,601 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-2
2023-02-07 19:58:41,602 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-6
2023-02-07 19:58:41,601 - distributed.worker - INFO - -------------------------------------------------
2023-02-07 19:58:41,602 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.2:46353
2023-02-07 19:58:41,602 - distributed.worker - INFO -          dashboard at:        172.20.209.63:34452
2023-02-07 19:58:41,602 - distributed.worker - INFO -          dashboard at:        172.20.209.63:33126
2023-02-07 19:58:41,602 - distributed.worker - INFO - -------------------------------------------------
2023-02-07 19:58:41,601 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.2:46353
2023-02-07 19:58:41,602 - distributed.worker - INFO -               Threads:                          4
2023-02-07 19:58:41,602 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.2:46353
2023-02-07 19:58:41,602 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.2:46353
2023-02-07 19:58:41,602 - distributed.worker - INFO - -------------------------------------------------
2023-02-07 19:58:41,601 - distributed.worker - INFO - -------------------------------------------------
2023-02-07 19:58:41,602 - distributed.worker - INFO - -------------------------------------------------
2023-02-07 19:58:41,602 - distributed.worker - INFO -                Memory:                  26.78 GiB
2023-02-07 19:58:41,602 - distributed.worker - INFO - -------------------------------------------------
2023-02-07 19:58:41,602 - distributed.worker - INFO -               Threads:                          4
2023-02-07 19:58:41,602 - distributed.worker - INFO -               Threads:                          4
2023-02-07 19:58:41,602 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ry9x7w46
2023-02-07 19:58:41,601 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-4
2023-02-07 19:58:41,602 - distributed.worker - INFO -                Memory:                  26.78 GiB
2023-02-07 19:58:41,602 - distributed.worker - INFO -                Memory:                  26.78 GiB
2023-02-07 19:58:41,602 - distributed.worker - INFO -               Threads:                          4
2023-02-07 19:58:41,602 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-g32r818y
2023-02-07 19:58:41,602 - distributed.worker - INFO -          dashboard at:        172.20.209.63:33609
2023-02-07 19:58:41,602 - distributed.worker - INFO - -------------------------------------------------
2023-02-07 19:58:41,602 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pf12epti
2023-02-07 19:58:41,602 - distributed.worker - INFO -                Memory:                  26.78 GiB
2023-02-07 19:58:41,602 - distributed.worker - INFO -               Threads:                          4
2023-02-07 19:58:41,602 - distributed.worker - INFO -               Threads:                          4
2023-02-07 19:58:41,602 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.2:46353
2023-02-07 19:58:41,602 - distributed.worker - INFO -               Threads:                          4
2023-02-07 19:58:41,602 - distributed.worker - INFO - -------------------------------------------------
2023-02-07 19:58:41,602 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-j_sdhia7
2023-02-07 19:58:41,602 - distributed.worker - INFO - -------------------------------------------------
2023-02-07 19:58:41,602 - distributed.worker - INFO -                Memory:                  26.78 GiB
2023-02-07 19:58:41,602 - distributed.worker - INFO -                Memory:                  26.78 GiB
2023-02-07 19:58:41,602 - distributed.worker - INFO - -------------------------------------------------
2023-02-07 19:58:41,602 - distributed.worker - INFO -                Memory:                  26.78 GiB
2023-02-07 19:58:41,603 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-49fg18n5
2023-02-07 19:58:41,603 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-luaeps52
2023-02-07 19:58:41,603 - distributed.worker - INFO - -------------------------------------------------
2023-02-07 19:58:41,603 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ocd_yc46
2023-02-07 19:58:41,603 - distributed.worker - INFO - -------------------------------------------------
2023-02-07 19:58:41,603 - distributed.worker - INFO - -------------------------------------------------
2023-02-07 19:58:41,602 - distributed.worker - INFO -               Threads:                          4
2023-02-07 19:58:41,603 - distributed.worker - INFO - -------------------------------------------------
2023-02-07 19:58:41,603 - distributed.worker - INFO -                Memory:                  26.78 GiB
2023-02-07 19:58:41,603 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-kztjhy81
2023-02-07 19:58:41,603 - distributed.worker - INFO - -------------------------------------------------
2023-02-07 19:58:41,616 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.2:46353
2023-02-07 19:58:41,616 - distributed.worker - INFO - -------------------------------------------------
2023-02-07 19:58:41,616 - distributed.core - INFO - Starting established connection to tcp://172.20.207.2:46353
2023-02-07 19:58:41,617 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.2:46353
2023-02-07 19:58:41,617 - distributed.worker - INFO - -------------------------------------------------
2023-02-07 19:58:41,617 - distributed.core - INFO - Starting established connection to tcp://172.20.207.2:46353
2023-02-07 19:58:41,618 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.2:46353
2023-02-07 19:58:41,618 - distributed.worker - INFO - -------------------------------------------------
2023-02-07 19:58:41,618 - distributed.core - INFO - Starting established connection to tcp://172.20.207.2:46353
2023-02-07 19:58:41,619 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.2:46353
2023-02-07 19:58:41,619 - distributed.worker - INFO - -------------------------------------------------
2023-02-07 19:58:41,619 - distributed.core - INFO - Starting established connection to tcp://172.20.207.2:46353
2023-02-07 19:58:41,619 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.2:46353
2023-02-07 19:58:41,619 - distributed.worker - INFO - -------------------------------------------------
2023-02-07 19:58:41,620 - distributed.core - INFO - Starting established connection to tcp://172.20.207.2:46353
2023-02-07 19:58:41,620 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.2:46353
2023-02-07 19:58:41,620 - distributed.worker - INFO - -------------------------------------------------
2023-02-07 19:58:41,621 - distributed.core - INFO - Starting established connection to tcp://172.20.207.2:46353
2023-02-07 19:58:41,621 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.2:46353
2023-02-07 19:58:41,621 - distributed.worker - INFO - -------------------------------------------------
2023-02-07 19:58:41,621 - distributed.core - INFO - Starting established connection to tcp://172.20.207.2:46353
2023-02-07 19:58:41,622 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.2:46353
2023-02-07 19:58:41,622 - distributed.worker - INFO - -------------------------------------------------
2023-02-07 19:58:41,622 - distributed.core - INFO - Starting established connection to tcp://172.20.207.2:46353
/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/geopandas/_compat.py:123: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.
  warnings.warn(
/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/distributed/protocol/pickle.py:71: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:

import os
os.environ['USE_PYGEOS'] = '0'
import geopandas

In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).
  return pickle.loads(x, buffers=buffers)
/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/geopandas/_compat.py:123: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.
  warnings.warn(
/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/dask_geopandas/backends.py:13: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:

import os
os.environ['USE_PYGEOS'] = '0'
import geopandas

In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).
  import geopandas
2023-02-07 19:58:52,255 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/geopandas/_compat.py:123: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.
  warnings.warn(
/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/dask_geopandas/backends.py:13: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:

import os
os.environ['USE_PYGEOS'] = '0'
import geopandas

In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).
  import geopandas
2023-02-07 19:59:02,069 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/dask/dataframe/io/csv.py:194: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.
  df = reader(bio, **kwargs)
/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/dask/dataframe/io/csv.py:194: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.
  df = reader(bio, **kwargs)
/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/dask/dataframe/io/csv.py:194: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.
  df = reader(bio, **kwargs)
/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/dask/dataframe/io/csv.py:194: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.
  df = reader(bio, **kwargs)
/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/geopandas/_compat.py:123: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.
  warnings.warn(
/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/distributed/protocol/pickle.py:71: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:

import os
os.environ['USE_PYGEOS'] = '0'
import geopandas

In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).
  return pickle.loads(x, buffers=buffers)
2023-02-07 20:03:15,569 - distributed.core - INFO - Connection to tcp://172.20.207.2:46353 has been closed.
2023-02-07 20:03:15,569 - distributed.core - INFO - Connection to tcp://172.20.207.2:46353 has been closed.
2023-02-07 20:03:15,569 - distributed.core - INFO - Connection to tcp://172.20.207.2:46353 has been closed.
2023-02-07 20:03:15,569 - distributed.core - INFO - Connection to tcp://172.20.207.2:46353 has been closed.
2023-02-07 20:03:15,569 - distributed.core - INFO - Connection to tcp://172.20.207.2:46353 has been closed.
2023-02-07 20:03:15,569 - distributed.core - INFO - Connection to tcp://172.20.207.2:46353 has been closed.
2023-02-07 20:03:15,570 - distributed.core - INFO - Connection to tcp://172.20.207.2:46353 has been closed.
2023-02-07 20:03:15,570 - distributed.worker - INFO - Stopping worker at tcp://172.20.209.63:38080. Reason: worker-handle-scheduler-connection-broken
2023-02-07 20:03:15,570 - distributed.core - INFO - Connection to tcp://172.20.207.2:46353 has been closed.
2023-02-07 20:03:15,570 - distributed.worker - INFO - Stopping worker at tcp://172.20.209.63:42105. Reason: worker-handle-scheduler-connection-broken
2023-02-07 20:03:15,570 - distributed.worker - INFO - Stopping worker at tcp://172.20.209.63:44977. Reason: worker-handle-scheduler-connection-broken
2023-02-07 20:03:15,570 - distributed.worker - INFO - Stopping worker at tcp://172.20.209.63:34364. Reason: worker-handle-scheduler-connection-broken
2023-02-07 20:03:15,570 - distributed.worker - INFO - Stopping worker at tcp://172.20.209.63:38121. Reason: worker-handle-scheduler-connection-broken
2023-02-07 20:03:15,570 - distributed.worker - INFO - Stopping worker at tcp://172.20.209.63:45196. Reason: worker-handle-scheduler-connection-broken
2023-02-07 20:03:15,570 - distributed.worker - INFO - Stopping worker at tcp://172.20.209.63:43743. Reason: worker-handle-scheduler-connection-broken
2023-02-07 20:03:15,570 - distributed.worker - INFO - Stopping worker at tcp://172.20.209.63:34018. Reason: worker-handle-scheduler-connection-broken
2023-02-07 20:03:15,574 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://172.20.209.63:41535'. Reason: worker-handle-scheduler-connection-broken
2023-02-07 20:03:15,574 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://172.20.209.63:38423'. Reason: worker-handle-scheduler-connection-broken
2023-02-07 20:03:15,575 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://172.20.209.63:45750'. Reason: worker-handle-scheduler-connection-broken
2023-02-07 20:03:15,575 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://172.20.209.63:37689'. Reason: worker-handle-scheduler-connection-broken
2023-02-07 20:03:15,575 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://172.20.209.63:35183'. Reason: worker-handle-scheduler-connection-broken
2023-02-07 20:03:15,575 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://172.20.209.63:45578'. Reason: worker-handle-scheduler-connection-broken
2023-02-07 20:03:15,576 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://172.20.209.63:33429'. Reason: worker-handle-scheduler-connection-broken
2023-02-07 20:03:15,576 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://172.20.209.63:45661'. Reason: worker-handle-scheduler-connection-broken
2023-02-07 20:03:15,576 - distributed.nanny - INFO - Worker closed
2023-02-07 20:03:15,576 - distributed.nanny - INFO - Worker closed
2023-02-07 20:03:15,577 - distributed.nanny - INFO - Worker closed
2023-02-07 20:03:15,577 - distributed.nanny - INFO - Worker closed
2023-02-07 20:03:15,578 - distributed.nanny - INFO - Worker closed
2023-02-07 20:03:15,578 - distributed.nanny - INFO - Worker closed
2023-02-07 20:03:15,579 - distributed.nanny - INFO - Worker closed
2023-02-07 20:03:15,579 - distributed.nanny - INFO - Worker closed
2023-02-07 20:03:17,578 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-02-07 20:03:17,578 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-02-07 20:03:17,768 - distributed.nanny - INFO - Closing Nanny at 'tcp://172.20.209.63:33429'. Reason: nanny-close-gracefully
2023-02-07 20:03:17,819 - distributed.nanny - INFO - Closing Nanny at 'tcp://172.20.209.63:45661'. Reason: nanny-close-gracefully
2023-02-07 20:03:17,851 - distributed.nanny - INFO - Closing Nanny at 'tcp://172.20.209.63:45750'. Reason: nanny-close-gracefully
2023-02-07 20:03:17,857 - distributed.nanny - INFO - Closing Nanny at 'tcp://172.20.209.63:35183'. Reason: nanny-close-gracefully
2023-02-07 20:03:17,880 - distributed.nanny - INFO - Closing Nanny at 'tcp://172.20.209.63:45578'. Reason: nanny-close-gracefully
2023-02-07 20:03:17,970 - distributed.nanny - INFO - Closing Nanny at 'tcp://172.20.209.63:37689'. Reason: nanny-close-gracefully
2023-02-07 20:03:18,012 - distributed.nanny - INFO - Closing Nanny at 'tcp://172.20.209.63:41535'. Reason: nanny-close-gracefully
2023-02-07 20:03:18,048 - distributed.nanny - INFO - Closing Nanny at 'tcp://172.20.209.63:38423'. Reason: nanny-close-gracefully
2023-02-07 20:03:18,049 - distributed.dask_worker - INFO - End worker
