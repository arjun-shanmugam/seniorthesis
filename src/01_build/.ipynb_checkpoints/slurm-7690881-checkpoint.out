## SLURM PROLOG ###############################################################
##    Job ID : 7690881
##  Job Name : dask-worker
##  Nodelist : node1748
##      CPUs : 1
##  Mem/Node : 8192 MB
## Directory : /gpfs/home/ashanmu1/seniorthesis/src/01_build
##   Job Started : Sat Jan 28 18:35:52 EST 2023
###############################################################################
2023-01-28 18:35:53,811 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.213.48:36660'
2023-01-28 18:35:53,817 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.213.48:35638'
2023-01-28 18:35:53,818 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.213.48:36022'
2023-01-28 18:35:53,825 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.213.48:33290'
2023-01-28 18:35:55,554 - distributed.worker - INFO -       Start worker at:  tcp://172.20.213.48:38890
2023-01-28 18:35:55,555 - distributed.worker - INFO -          Listening to:  tcp://172.20.213.48:38890
2023-01-28 18:35:55,555 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-3
2023-01-28 18:35:55,555 - distributed.worker - INFO -          dashboard at:        172.20.213.48:42588
2023-01-28 18:35:55,555 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.1:33063
2023-01-28 18:35:55,555 - distributed.worker - INFO - -------------------------------------------------
2023-01-28 18:35:55,555 - distributed.worker - INFO -               Threads:                          1
2023-01-28 18:35:55,555 - distributed.worker - INFO -                Memory:                   1.86 GiB
2023-01-28 18:35:55,555 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-w4pzyu3_
2023-01-28 18:35:55,555 - distributed.worker - INFO - -------------------------------------------------
2023-01-28 18:35:55,559 - distributed.worker - INFO -       Start worker at:  tcp://172.20.213.48:44645
2023-01-28 18:35:55,559 - distributed.worker - INFO -       Start worker at:  tcp://172.20.213.48:37354
2023-01-28 18:35:55,559 - distributed.worker - INFO -          Listening to:  tcp://172.20.213.48:44645
2023-01-28 18:35:55,559 - distributed.worker - INFO -          Listening to:  tcp://172.20.213.48:37354
2023-01-28 18:35:55,559 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-0
2023-01-28 18:35:55,559 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-2
2023-01-28 18:35:55,559 - distributed.worker - INFO -          dashboard at:        172.20.213.48:34146
2023-01-28 18:35:55,559 - distributed.worker - INFO -          dashboard at:        172.20.213.48:38813
2023-01-28 18:35:55,559 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.1:33063
2023-01-28 18:35:55,559 - distributed.worker - INFO - -------------------------------------------------
2023-01-28 18:35:55,559 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.1:33063
2023-01-28 18:35:55,559 - distributed.worker - INFO - -------------------------------------------------
2023-01-28 18:35:55,559 - distributed.worker - INFO -               Threads:                          1
2023-01-28 18:35:55,559 - distributed.worker - INFO -               Threads:                          1
2023-01-28 18:35:55,559 - distributed.worker - INFO -                Memory:                   1.86 GiB
2023-01-28 18:35:55,559 - distributed.worker - INFO -                Memory:                   1.86 GiB
2023-01-28 18:35:55,559 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gipm2gea
2023-01-28 18:35:55,559 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ctsfio1w
2023-01-28 18:35:55,560 - distributed.worker - INFO - -------------------------------------------------
2023-01-28 18:35:55,560 - distributed.worker - INFO - -------------------------------------------------
2023-01-28 18:35:55,560 - distributed.worker - INFO -       Start worker at:  tcp://172.20.213.48:40138
2023-01-28 18:35:55,561 - distributed.worker - INFO -          Listening to:  tcp://172.20.213.48:40138
2023-01-28 18:35:55,561 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-1
2023-01-28 18:35:55,561 - distributed.worker - INFO -          dashboard at:        172.20.213.48:42548
2023-01-28 18:35:55,561 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.1:33063
2023-01-28 18:35:55,561 - distributed.worker - INFO - -------------------------------------------------
2023-01-28 18:35:55,561 - distributed.worker - INFO -               Threads:                          1
2023-01-28 18:35:55,562 - distributed.worker - INFO -                Memory:                   1.86 GiB
2023-01-28 18:35:55,562 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-74dxn9yx
2023-01-28 18:35:55,562 - distributed.worker - INFO - -------------------------------------------------
2023-01-28 18:35:55,570 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.1:33063
2023-01-28 18:35:55,571 - distributed.worker - INFO - -------------------------------------------------
2023-01-28 18:35:55,571 - distributed.core - INFO - Starting established connection to tcp://172.20.207.1:33063
2023-01-28 18:35:55,571 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.1:33063
2023-01-28 18:35:55,571 - distributed.worker - INFO - -------------------------------------------------
2023-01-28 18:35:55,572 - distributed.core - INFO - Starting established connection to tcp://172.20.207.1:33063
2023-01-28 18:35:55,573 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.1:33063
2023-01-28 18:35:55,573 - distributed.worker - INFO - -------------------------------------------------
2023-01-28 18:35:55,573 - distributed.core - INFO - Starting established connection to tcp://172.20.207.1:33063
2023-01-28 18:35:55,577 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.1:33063
2023-01-28 18:35:55,578 - distributed.worker - INFO - -------------------------------------------------
2023-01-28 18:35:55,578 - distributed.core - INFO - Starting established connection to tcp://172.20.207.1:33063
slurmstepd: error: *** JOB 7690881 ON node1748 CANCELLED AT 2023-01-28T18:37:55 ***
