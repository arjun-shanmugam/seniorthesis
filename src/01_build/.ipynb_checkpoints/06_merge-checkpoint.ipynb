{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06_merge.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/geopandas/_compat.py:123: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_139962/4243570430.py:2: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from dask.distributed import Client\n",
    "import dask.dataframe as dd\n",
    "from dask_jobqueue import SLURMCluster\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as cx\n",
    "from build_utilities import generate_variable_names\n",
    "import dask_geopandas\n",
    "INPUT_DATA_EVICTIONS = \"../../data/02_intermediate/evictions.csv\"\n",
    "INPUT_DATA_TRACTS = \"../../data/02_intermediate/tracts.csv\"\n",
    "INPUT_DATA_TAX_PARCELS = \"../../data/02_intermediate/tax_parcels.gpkg\"\n",
    "INPUT_DATA_ZESTIMATES = \"../../data/02_intermediate/zestimates.csv\"\n",
    "INPUT_DATA_CRIME = \"../../data/01_raw/crime_incidents\"\n",
    "OUTPUT_DATA_UNRESTRICTED = \"../../data/03_cleaned/unrestricted.csv\"\n",
    "OUTPUT_DATA_ZILLOW = \"../../data/03_cleaned/zestimates_analysis.csv\"\n",
    "OUTPUT_DATA_CRIME = \"../../data/03_cleaned/crime_analysis.csv\"\n",
    "VERBOSE = True\n",
    "N_PARTITIONS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Evictions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load evictions data.\n",
    "with open(INPUT_DATA_EVICTIONS, 'r') as file:\n",
    "    all_column_names = set(file.readline().replace(\"\\\"\", \"\").replace(\"\\n\", \"\").split(\",\"))\n",
    "to_drop = {'Accuracy Score', 'Accuracy Type', 'Number', 'Street', 'Unit Type', 'Unit Number',\n",
    "           'State', 'Zip', 'Country', 'Source', 'Census Year', 'State FIPS', 'County FIPS',\n",
    "           'Place Name', 'Place FIPS', 'Census Tract Code', 'Census Block Code', 'Census Block Group',\n",
    "           'Metro/Micro Statistical Area Code', 'Metro/Micro Statistical Area Type',\n",
    "           'Combined Statistical Area Code', 'Metropolitan Division Area Code', 'court_location',\n",
    "           'defendant', 'defendant_atty', 'defendant_atty_address_apt',\n",
    "           'defendant_atty_address_city', 'defendant_atty_address_name', 'defendant_atty_address_state',\n",
    "           'defendant_atty_address_street', 'defendant_atty_address_zip', 'docket_history', 'execution', 'judgment_for',\n",
    "           'judgment_total', 'latest_docket_date', 'plaintiff', 'plaintiff_atty', 'plaintiff_atty_address_apt',\n",
    "           'plaintiff_atty_address_city', 'plaintiff_atty_address_name', 'plaintiff_atty_address_state',\n",
    "           'plaintiff_atty_address_street', 'plaintiff_atty_address_zip', 'Metropolitan Division Area Name',\n",
    "           'property_address_city', 'property_address_state', 'property_address_street',\n",
    "           'property_address_zip'}\n",
    "df = pd.read_csv(INPUT_DATA_EVICTIONS, usecols=set(all_column_names) - set(to_drop))\n",
    "original_N = len(df)\n",
    "if VERBOSE:\n",
    "    print(f\"Beginning with {original_N} observations.\")\n",
    "\n",
    "# Drop cases missing file_date.\n",
    "mask = df['file_date'].notna()\n",
    "if VERBOSE:\n",
    "    print(\n",
    "        f\"Dropping {(~mask).sum()} observations where file_date is missing.\")\n",
    "df = df.loc[mask, :]\n",
    "\n",
    "# Add file month and year to dataset.\n",
    "df.loc[:, 'file_month'] = pd.to_datetime(df['file_date']).dt.strftime('%Y-%m')\n",
    "df.loc[:, 'file_year'] = pd.to_datetime(df['file_date']).dt.year\n",
    "\n",
    "# Clean the values in the judgment_for_pdu variable.\n",
    "judgment_for_pdu_replacement_dict = {\"unknown\": \"Unknown\",\n",
    "                                     \"plaintiff\": \"Plaintiff\",\n",
    "                                     \"defendant\": \"Defendant\"}\n",
    "df.loc[:, \"judgment_for_pdu\"] = (df.loc[:, \"judgment_for_pdu\"]\n",
    "                                           .replace(judgment_for_pdu_replacement_dict))\n",
    "\n",
    "# Replace missing values in money judgment column with zeroes.\n",
    "df.loc[:, 'judgment'] = df['judgment'].fillna(0)\n",
    "\n",
    "# Rename duration to case_duration.\n",
    "df = df.rename(columns={'duration': 'case_duration'})\n",
    "\n",
    "# Drop malformed addresses.\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {df['property_address_full'].str.contains('span, span span').sum()} observations which \"\n",
    "          f\"have malformed addresses.\")\n",
    "df = df.loc[~df['property_address_full'].str.contains(\"span, span span\"), :]\n",
    "\n",
    "# Drop addresses without latitude and longitude.\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {df[['longitude', 'latitude']].isna().any(axis=1).sum()} evictions missing latitude \"\n",
    "          f\"or longitude.\")\n",
    "df = df.dropna(subset=['longitude', 'latitude'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Merging Evictions With Census Tract Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Merge with census tract characteristics.\n",
    "df = df.rename(columns={'Full FIPS (tract)': 'tract_geoid'})\n",
    "df = df.merge(pd.read_csv(INPUT_DATA_TRACTS, dtype={'tract_geoid': float}),\n",
    "                                  on='tract_geoid',\n",
    "                                  how='left',\n",
    "                                  validate='m:1')\n",
    "if VERBOSE:\n",
    "    print(f\"Successfully merged {df['med_hhinc2016'].notna().sum()} observations \"\n",
    "          f\"({df['med_hhinc2016'].notna().sum() / original_N:.2f} percent of observations) with census \"\n",
    "          f\"tracts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Merging Evictions With Zestimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(INPUT_DATA_ZESTIMATES).merge(df,\n",
    "                                                     on='case_number',\n",
    "                                                     how='right',\n",
    "                                                     validate='1:1')\n",
    "if VERBOSE:\n",
    "    successfully_matched_observations = (~df['2022-12'].isna()).sum()\n",
    "    print(\n",
    "        f\"Successfully matched {successfully_matched_observations} evictions \"\n",
    "        f\"({100 * (successfully_matched_observations / len(df)) :.2f} percent of observations) to \"\n",
    "        f\"Zestimates.\")\n",
    "\n",
    "# Rename columns containing Zestimates.\n",
    "years = [str(year) for year in range(2013, 2023)]\n",
    "months = [\"0\" + str(month) for month in range(1, 10)] + [str(month) for month in range(10, 13)]\n",
    "value_vars = [\"2012-12\"] + [str(year) + \"-\" + str(month) for year in years for month in months]\n",
    "value_vars_zestimates, _, _ = generate_variable_names('zestimate')\n",
    "for value_var, value_var_zestimates in zip(value_vars, value_vars_zestimates):\n",
    "    df = df.rename(columns={value_var: value_var_zestimates})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Merging Evictions with Tax Parcels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Request computing resources.\n",
    "cluster = SLURMCluster(queue='batch',\n",
    "                       cores=32,\n",
    "                       memory='230 GB',\n",
    "                       walltime='00:30:00',\n",
    "                      scheduler_options={'dashboard_address': '8787'} )\n",
    "cluster.scale(jobs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = gpd.GeoDataFrame(df,\n",
    "                          geometry=gpd.points_from_xy(df['longitude'], df['latitude']))\n",
    "\n",
    "df = df.set_crs(\"EPSG:4326\", allow_override=True).to_crs('EPSG:26986')\n",
    "ddf = dask_geopandas.from_geopandas(df, npartitions=N_PARTITIONS).repartition(partition_size='50 MB')\n",
    "\n",
    "tax_parcels_dgdf = dask_geopandas.read_file(INPUT_DATA_TAX_PARCELS, npartitions=N_PARTITIONS, layer='layer').repartition(partition_size='50 MB')\n",
    "\n",
    "ddf = dask_geopandas.sjoin(tax_parcels_dgdf, ddf, how='inner', predicate='contains')\n",
    "ddf = ddf.loc[ddf['LOC_ID'] != \"F_819960_2934955\", :]  # Drop the eviction which erroneously merges to two parcels. \n",
    "ddf = ddf.drop(columns=['index_right']).compute()\n",
    "\n",
    "# Append the evictions which could not be merged to tax parcels back into the Dask DataFrame.\n",
    "evictions_without_parcels = df.loc[~(df['case_number'].isin(ddf['case_number'])), :]\n",
    "ddf = pd.concat([ddf, evictions_without_parcels], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5a. Merge Evictions With Own-Parcel Crime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert eviction data to Dask-GeoDataFrame.\n",
    "ddf = dask_geopandas.from_geopandas(ddf, npartitions=N_PARTITIONS)\n",
    "ddf = ddf.repartition(partition_size='25 MB')  # Reduce partition size in preparation for spatial join.\n",
    "\n",
    "# Read crime data as Dask DataFrame; clean it.\n",
    "crime_dgdf = (dd.read_csv(INPUT_DATA_CRIME + \"/*.csv\", dtype={'REPORTING_AREA': 'object', 'SHOOTING': 'object'})\n",
    "                .dropna(subset=['Long', 'Lat', 'OCCURRED_ON_DATE'])\n",
    "                .rename(columns={'OCCURRED_ON_DATE': 'month_of_crime_incident'})\n",
    "                .drop(columns=['OFFENSE_CODE', 'OFFENSE_CODE_GROUP', 'OFFENSE_DESCRIPTION', 'DISTRICT', 'REPORTING_AREA', 'SHOOTING', 'YEAR', 'MONTH',\n",
    "                               'DAY_OF_WEEK', 'HOUR', 'UCR_PART', 'STREET', 'Location']))\n",
    "crime_dgdf['month_of_crime_incident'] = dd.to_datetime(crime_dgdf['month_of_crime_incident'].str[:10]).dt.to_period(\"M\").astype(str)\n",
    "crime_dgdf = crime_dgdf.compute()\n",
    "\n",
    "# Convert crime data to GeoDataFrame.\n",
    "crime_dgdf = (gpd.GeoDataFrame(crime_dgdf, geometry=gpd.points_from_xy(crime_dgdf['Long'], crime_dgdf['Lat']))\n",
    "                               .set_crs(\"EPSG:4326\", allow_override=True)\n",
    "                               .to_crs(\"EPSG:26986\"))\n",
    "\n",
    "# Convert crime data to Dask-GeoDataFrame.\n",
    "crime_dgdf = dask_geopandas.from_geopandas(crime_dgdf, npartitions=N_PARTITIONS).repartition(partition_size='25 MB')\n",
    "crime_dgdf = crime_dgdf.dissolve(by='INCIDENT_NUMBER').reset_index()\n",
    "\n",
    "# Join Dask-GeoDataFrames containing eviction data and crime data.\n",
    "ddf = dask_geopandas.sjoin(crime_dgdf,\n",
    "                            ddf,\n",
    "                            how='inner',\n",
    "                            predicate='within').compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b. Aggregating Own-Parcel Crime Data to Case-Month Level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set aggregation function for each column.\n",
    "# TODO: Abstract this into a function.\n",
    "# We will aggregate by the 'first' value for all columns except incident number, which we will count.\n",
    "aggregate_by_first = ddf.columns.tolist()\n",
    "aggregate_by_first.remove('INCIDENT_NUMBER')\n",
    "aggregate_by_first.remove('case_number')\n",
    "aggregate_by_first.remove('month_of_crime_incident')\n",
    "agg_dict = {'INCIDENT_NUMBER': 'count'}\n",
    "for column in aggregate_by_first:\n",
    "    agg_dict[column] = 'first'\n",
    "ddf = ddf.groupby(['case_number', 'month_of_crime_incident'])\n",
    "ddf = ddf.aggregate(agg_dict)\n",
    "ddf = ddf.reset_index()\n",
    "\n",
    "# Pivot from wide to long.\n",
    "ddf = ddf.rename(columns={'INCIDENT_NUMBER': 'crime_incidents'})\n",
    "ddf = pd.pivot(ddf, index=['case_number'], columns=['month_of_crime_incident'], values='crime_incidents').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, ddf contains wide format crime incident data for the 792 properties which experienced crime.\n",
    "# Merge with the evictions that experienced no crime, stored in df.\n",
    "ddf = ddf.set_index('case_number')\n",
    "df = df.set_index('case_number')\n",
    "ddf = pd.concat([df, ddf], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns containing own-parcel crime incident counts.\n",
    "years = [str(year) for year in range(2015, 2023)]\n",
    "months = [\"0\" + str(month) for month in range(1, 10)] + [str(month) for month in range(10, 13)]\n",
    "value_vars = [str(year) + \"-\" + str(month) for year in years for month in months]\n",
    "value_vars = value_vars[5:]\n",
    "value_vars.append('2023-01')\n",
    "for value_var in value_vars:\n",
    "    ddf = ddf.rename(columns={value_var: value_var + \"_crimes_own_parcel\"})\n",
    "value_vars_crime_own_parcel = [value_var + \"_crimes_own_parcel\" for value_var in value_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['2015-06_crimes_own_parcel', '2015-07_crimes_own_parcel',\\n       '2015-08_crimes_own_parcel', '2015-09_crimes_own_parcel',\\n       '2015-10_crimes_own_parcel', '2015-11_crimes_own_parcel',\\n       '2015-12_crimes_own_parcel', '2016-01_crimes_own_parcel',\\n       '2016-02_crimes_own_parcel', '2016-03_crimes_own_parcel',\\n       '2016-04_crimes_own_parcel', '2016-05_crimes_own_parcel',\\n       '2016-06_crimes_own_parcel', '2016-07_crimes_own_parcel',\\n       '2016-08_crimes_own_parcel', '2016-09_crimes_own_parcel',\\n       '2016-10_crimes_own_parcel', '2016-11_crimes_own_parcel',\\n       '2016-12_crimes_own_parcel', '2017-01_crimes_own_parcel',\\n       '2017-02_crimes_own_parcel', '2017-03_crimes_own_parcel',\\n       '2017-04_crimes_own_parcel', '2017-05_crimes_own_parcel',\\n       '2017-06_crimes_own_parcel', '2017-07_crimes_own_parcel',\\n       '2017-08_crimes_own_parcel', '2017-09_crimes_own_parcel',\\n       '2017-10_crimes_own_parcel', '2017-11_crimes_own_parcel',\\n       '2017-12_crimes_own_parcel', '2018-01_crimes_own_parcel',\\n       '2018-02_crimes_own_parcel', '2018-03_crimes_own_parcel',\\n       '2018-04_crimes_own_parcel', '2018-05_crimes_own_parcel',\\n       '2018-06_crimes_own_parcel', '2018-07_crimes_own_parcel',\\n       '2018-08_crimes_own_parcel', '2018-09_crimes_own_parcel',\\n       '2018-10_crimes_own_parcel', '2018-11_crimes_own_parcel',\\n       '2018-12_crimes_own_parcel', '2019-01_crimes_own_parcel',\\n       '2019-02_crimes_own_parcel', '2019-03_crimes_own_parcel',\\n       '2019-04_crimes_own_parcel', '2019-05_crimes_own_parcel',\\n       '2019-06_crimes_own_parcel', '2019-07_crimes_own_parcel',\\n       '2019-08_crimes_own_parcel', '2019-09_crimes_own_parcel',\\n       '2019-10_crimes_own_parcel', '2019-11_crimes_own_parcel',\\n       '2019-12_crimes_own_parcel', '2020-01_crimes_own_parcel',\\n       '2020-02_crimes_own_parcel', '2020-03_crimes_own_parcel',\\n       '2020-04_crimes_own_parcel', '2020-05_crimes_own_parcel',\\n       '2020-06_crimes_own_parcel', '2020-07_crimes_own_parcel',\\n       '2020-08_crimes_own_parcel', '2020-09_crimes_own_parcel',\\n       '2020-10_crimes_own_parcel', '2020-11_crimes_own_parcel',\\n       '2020-12_crimes_own_parcel', '2021-01_crimes_own_parcel',\\n       '2021-02_crimes_own_parcel', '2021-03_crimes_own_parcel',\\n       '2021-04_crimes_own_parcel', '2021-05_crimes_own_parcel',\\n       '2021-06_crimes_own_parcel', '2021-07_crimes_own_parcel',\\n       '2021-08_crimes_own_parcel', '2021-09_crimes_own_parcel',\\n       '2021-10_crimes_own_parcel', '2021-11_crimes_own_parcel',\\n       '2021-12_crimes_own_parcel', '2022-01_crimes_own_parcel',\\n       '2022-02_crimes_own_parcel', '2022-03_crimes_own_parcel',\\n       '2022-04_crimes_own_parcel', '2022-05_crimes_own_parcel',\\n       '2022-06_crimes_own_parcel', '2022-07_crimes_own_parcel',\\n       '2022-08_crimes_own_parcel', '2022-09_crimes_own_parcel',\\n       '2022-10_crimes_own_parcel', '2022-11_crimes_own_parcel',\\n       '2022-12_crimes_own_parcel', '2023-01_crimes_own_parcel'],\\n      dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Replace missing crime data with zero for evictions that were not matched to crimes.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m unrestricted_df\u001b[38;5;241m.\u001b[39mloc[:, value_vars_crime] \u001b[38;5;241m=\u001b[39m \u001b[43munrestricted_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalue_vars_crime\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/geopandas/geodataframe.py:1415\u001b[0m, in \u001b[0;36mGeoDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1409\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1411\u001b[0m \u001b[38;5;124;03m    If the result is a column containing only 'geometry', return a\u001b[39;00m\n\u001b[1;32m   1412\u001b[0m \u001b[38;5;124;03m    GeoSeries. If it's a DataFrame with any columns of GeometryDtype,\u001b[39;00m\n\u001b[1;32m   1413\u001b[0m \u001b[38;5;124;03m    return a GeoDataFrame.\u001b[39;00m\n\u001b[1;32m   1414\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1415\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1416\u001b[0m     geo_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_geometry_column_name\n\u001b[1;32m   1417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, Series) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result\u001b[38;5;241m.\u001b[39mdtype, GeometryDtype):\n",
      "File \u001b[0;32m/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/pandas/core/frame.py:3511\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3510\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3511\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3513\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:5782\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5780\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5782\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5784\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   5785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5786\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:5842\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5840\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   5841\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 5842\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5844\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   5845\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['2015-06_crimes_own_parcel', '2015-07_crimes_own_parcel',\\n       '2015-08_crimes_own_parcel', '2015-09_crimes_own_parcel',\\n       '2015-10_crimes_own_parcel', '2015-11_crimes_own_parcel',\\n       '2015-12_crimes_own_parcel', '2016-01_crimes_own_parcel',\\n       '2016-02_crimes_own_parcel', '2016-03_crimes_own_parcel',\\n       '2016-04_crimes_own_parcel', '2016-05_crimes_own_parcel',\\n       '2016-06_crimes_own_parcel', '2016-07_crimes_own_parcel',\\n       '2016-08_crimes_own_parcel', '2016-09_crimes_own_parcel',\\n       '2016-10_crimes_own_parcel', '2016-11_crimes_own_parcel',\\n       '2016-12_crimes_own_parcel', '2017-01_crimes_own_parcel',\\n       '2017-02_crimes_own_parcel', '2017-03_crimes_own_parcel',\\n       '2017-04_crimes_own_parcel', '2017-05_crimes_own_parcel',\\n       '2017-06_crimes_own_parcel', '2017-07_crimes_own_parcel',\\n       '2017-08_crimes_own_parcel', '2017-09_crimes_own_parcel',\\n       '2017-10_crimes_own_parcel', '2017-11_crimes_own_parcel',\\n       '2017-12_crimes_own_parcel', '2018-01_crimes_own_parcel',\\n       '2018-02_crimes_own_parcel', '2018-03_crimes_own_parcel',\\n       '2018-04_crimes_own_parcel', '2018-05_crimes_own_parcel',\\n       '2018-06_crimes_own_parcel', '2018-07_crimes_own_parcel',\\n       '2018-08_crimes_own_parcel', '2018-09_crimes_own_parcel',\\n       '2018-10_crimes_own_parcel', '2018-11_crimes_own_parcel',\\n       '2018-12_crimes_own_parcel', '2019-01_crimes_own_parcel',\\n       '2019-02_crimes_own_parcel', '2019-03_crimes_own_parcel',\\n       '2019-04_crimes_own_parcel', '2019-05_crimes_own_parcel',\\n       '2019-06_crimes_own_parcel', '2019-07_crimes_own_parcel',\\n       '2019-08_crimes_own_parcel', '2019-09_crimes_own_parcel',\\n       '2019-10_crimes_own_parcel', '2019-11_crimes_own_parcel',\\n       '2019-12_crimes_own_parcel', '2020-01_crimes_own_parcel',\\n       '2020-02_crimes_own_parcel', '2020-03_crimes_own_parcel',\\n       '2020-04_crimes_own_parcel', '2020-05_crimes_own_parcel',\\n       '2020-06_crimes_own_parcel', '2020-07_crimes_own_parcel',\\n       '2020-08_crimes_own_parcel', '2020-09_crimes_own_parcel',\\n       '2020-10_crimes_own_parcel', '2020-11_crimes_own_parcel',\\n       '2020-12_crimes_own_parcel', '2021-01_crimes_own_parcel',\\n       '2021-02_crimes_own_parcel', '2021-03_crimes_own_parcel',\\n       '2021-04_crimes_own_parcel', '2021-05_crimes_own_parcel',\\n       '2021-06_crimes_own_parcel', '2021-07_crimes_own_parcel',\\n       '2021-08_crimes_own_parcel', '2021-09_crimes_own_parcel',\\n       '2021-10_crimes_own_parcel', '2021-11_crimes_own_parcel',\\n       '2021-12_crimes_own_parcel', '2022-01_crimes_own_parcel',\\n       '2022-02_crimes_own_parcel', '2022-03_crimes_own_parcel',\\n       '2022-04_crimes_own_parcel', '2022-05_crimes_own_parcel',\\n       '2022-06_crimes_own_parcel', '2022-07_crimes_own_parcel',\\n       '2022-08_crimes_own_parcel', '2022-09_crimes_own_parcel',\\n       '2022-10_crimes_own_parcel', '2022-11_crimes_own_parcel',\\n       '2022-12_crimes_own_parcel', '2023-01_crimes_own_parcel'],\\n      dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# Replace missing crime data with zero for evictions that were not matched to crimes.\n",
    "ddf.loc[:, value_vars_crime] = ddf[value_vars_crime].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'value_vars_crime_own_parcel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Separately store own-parcel crime counts.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m to_concat \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m crimes_own_parcel_data \u001b[38;5;241m=\u001b[39m ddf[\u001b[43mvalue_vars_crime_own_parcel\u001b[49m]\n\u001b[1;32m      4\u001b[0m to_concat\u001b[38;5;241m.\u001b[39mappend(crimes_own_parcel_data)\n\u001b[1;32m      5\u001b[0m ddf \u001b[38;5;241m=\u001b[39m ddf\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39mvalue_vars_crime_own_parcel)  \u001b[38;5;66;03m# Drop from ddf so that we are not spatially joining unnecessary data.\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'value_vars_crime_own_parcel' is not defined"
     ]
    }
   ],
   "source": [
    "# Separately store own-parcel crime counts.\n",
    "to_concat = []\n",
    "crimes_own_parcel_data = ddf[value_vars_crime_own_parcel]\n",
    "to_concat.append(crimes_own_parcel_data)\n",
    "ddf = ddf.drop(columns=value_vars_crime_own_parcel)  # Drop from ddf so that we are not spatially joining unnecessary data.\n",
    "print(ddf.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6a. Merge Evictions with Crimes Within Varying Distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    "ddf : GeoDataFrame = \n",
    "for each radius in [60, 90, 140, 200]:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop existing geometry column and create a new one using the latitude and longitude of the eviction coordinates.\n",
    "ddf = ddf.drop(columns='geometry')\n",
    "ddf = gpd.GeoDataFrame(ddf,\n",
    "                          geometry=gpd.points_from_xy(ddf['longitude'], ddf['latitude']))\n",
    "\n",
    "ddf = ddf.set_crs(\"EPSG:4326\", allow_override=True).to_crs('EPSG:26986')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ddf.geometry = ddf.geometry.buffer(500)  # Add 500m buffer around every eviction. \n",
    "ddf = dask_geopandas.from_geopandas(ddf, npartitions=N_PARTITIONS).repartition(partition_size='50 MB')\n",
    "# ddf is now a dask-geodataframe where each eviction's geometry is a 500\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Producing the Unrestricted Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrestricted_df = pd.DataFrame(unrestricted_df.drop(columns='geometry'))\n",
    "unrestricted_df.to_csv(OUTPUT_DATA_UNRESTRICTED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Producing the Samples Used in Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 16883 cases resolved through mediation.\n"
     ]
    }
   ],
   "source": [
    "# Drop cases resolved via mediation.\n",
    "mediated_mask = unrestricted_df['disposition_found'] == \"Mediated\"\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {mediated_mask.sum()} cases resolved through mediation.\")\n",
    "unrestricted_df = unrestricted_df.loc[~mediated_mask, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Droppping 7455 cases resolved through voluntary dismissal.\n"
     ]
    }
   ],
   "source": [
    "# Drop cases resolved via volntary dismissal (dropped by plaintiff). \n",
    "voluntary_dismissal_mask = unrestricted_df['disposition'].str.contains(\"R 41(a)(1) Voluntary Dismissal on\", na=False, regex=False)\n",
    "if VERBOSE:\n",
    "    print(f\"Droppping {voluntary_dismissal_mask.sum()} cases resolved through voluntary dismissal.\")\n",
    "unrestricted_df = unrestricted_df.loc[~voluntary_dismissal_mask, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 245 cases where disposition_found is \"Other\"\n"
     ]
    }
   ],
   "source": [
    "# Drop cases where disposition found is other.\n",
    "disposition_found_other_mask = unrestricted_df['disposition_found'] == \"Other\"\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {disposition_found_other_mask.sum()} cases where disposition_found is \\\"Other\\\"\")\n",
    "unrestricted_df = unrestricted_df.loc[~disposition_found_other_mask, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 2 observations where disposition_found is \"Defaulted\" but judgment_for_pdu is \"Defendant\".\n",
      "Dropping 150 observations where disposition_found is \"Dismissed\" but judgment_for_pdu is \"Plaintiff\".\n"
     ]
    }
   ],
   "source": [
    "# Drop rows which contain consistent values of disposition_found and judgment_for_pdu.\n",
    "\n",
    "# First, we drop cases where disposition_found is \"Defaulted\" but judgment_for_pdu is \"Defendant\"\n",
    "inconsistent_mask_1 = ((unrestricted_df['disposition_found'] == \"Defaulted\") & (unrestricted_df['judgment_for_pdu'] == \"Defendant\"))\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {inconsistent_mask_1.sum()} observations where disposition_found is \\\"Defaulted\\\" but judgment_for_pdu is \\\"Defendant\\\".\")\n",
    "unrestricted_df = unrestricted_df.loc[~inconsistent_mask_1, :]\n",
    "          \n",
    "# Next, we drop cases where disposition_found is \"Dismissed\" yet judgment_for_pdu is \"Plaintiff\"\n",
    "inconsistent_mask_2 = ((unrestricted_df['disposition_found'] == \"Dismissed\") & (unrestricted_df['judgment_for_pdu'] == \"Plaintiff\"))\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {inconsistent_mask_2.sum()} observations where disposition_found is \\\"Dismissed\\\" but judgment_for_pdu is \\\"Plaintiff\\\".\")\n",
    "unrestricted_df = unrestricted_df.loc[~inconsistent_mask_2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a variable indicating judgment in favor of defendant.\n",
    "unrestricted_df.loc[:, 'judgment_for_defendant'] = 0\n",
    "defendant_won_mask = ((unrestricted_df['disposition_found'] == \"Dismissed\") |\n",
    "                      (unrestricted_df['judgment_for_pdu'] == \"Defendant\"))\n",
    "unrestricted_df.loc[defendant_won_mask, 'judgment_for_defendant'] = 1\n",
    "\n",
    "# Generate a variable indicating judgement in favor of plaintiff.\n",
    "unrestricted_df.loc[:, 'judgment_for_plaintiff'] = 1 - unrestricted_df['judgment_for_defendant']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8a. Producing the Zillow Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limiting sample to 3298 evictions for which we observe Zestimates at every month from 2012-12 to 2022-12.\n"
     ]
    }
   ],
   "source": [
    "# Drop rows where we are missing any Zestimates.\n",
    "zestimates_df = unrestricted_df.copy()\n",
    "has_all_zestimates_mask = zestimates_df[value_vars_zestimates].notna().all(axis=1)\n",
    "if VERBOSE:\n",
    "    print(f\"Limiting sample to {has_all_zestimates_mask.sum()} evictions for which we observe Zestimates at every month from 2012-12 to 2022-12.\")\n",
    "zestimates_df = zestimates_df.loc[has_all_zestimates_mask, :]\n",
    "zestimates_df.to_csv(OUTPUT_DATA_ZILLOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8b. Producing the Crime Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limiting sample to 2318 observations which are in Boston.\n"
     ]
    }
   ],
   "source": [
    "crime_df = unrestricted_df.copy()\n",
    "# Restrict to evictions which took place in Boston.\n",
    "boston_mask = ((crime_df['County'] == \"Suffolk County\") & (~crime_df['City'].isin([\"Chelsea\", \"Revere\", \"Winthrop\"])))\n",
    "if VERBOSE:\n",
    "    print(f\"Limiting sample to {boston_mask.sum()} observations which are in Boston.\")\n",
    "crime_df = crime_df.loc[boston_mask, :]\n",
    "crime_df.to_csv(OUTPUT_DATA_CRIME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
