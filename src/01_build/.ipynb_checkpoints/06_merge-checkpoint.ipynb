{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06_merge.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from dask.distributed import Client\n",
    "import dask.dataframe as dd\n",
    "from dask_jobqueue import SLURMCluster\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as cx\n",
    "from build_utilities import generate_variable_names, aggregate_crime_to_case_month\n",
    "import dask_geopandas\n",
    "INPUT_DATA_EVICTIONS = \"../../data/02_intermediate/evictions.csv\"\n",
    "INPUT_DATA_TRACTS = \"../../data/02_intermediate/tracts.csv\"\n",
    "INPUT_DATA_TAX_PARCELS = \"../../data/02_intermediate/tax_parcels.gpkg\"\n",
    "INPUT_DATA_ZESTIMATES = \"../../data/02_intermediate/zestimates.csv\"\n",
    "INPUT_DATA_CRIME = \"../../data/01_raw/crime_incidents\"\n",
    "OUTPUT_DATA_UNRESTRICTED = \"../../data/03_cleaned/unrestricted.csv\"\n",
    "OUTPUT_DATA_ZILLOW = \"../../data/03_cleaned/zestimates_analysis.csv\"\n",
    "OUTPUT_DATA_CRIME = \"../../data/03_cleaned/crime_analysis.csv\"\n",
    "VERBOSE = True\n",
    "N_PARTITIONS = 1\n",
    "value_vars_to_concat = []  # A list of DataFrames, where each DataFrame contains the panel data for a single outcome variable and has case_number as its index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Evictions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning with 40759 observations.\n",
      "Dropping 0 observations where file_date is missing.\n",
      "Dropping 24 observations which have malformed addresses.\n",
      "Dropping 1 evictions missing latitude or longitude.\n"
     ]
    }
   ],
   "source": [
    "# Load evictions data.\n",
    "with open(INPUT_DATA_EVICTIONS, 'r') as file:\n",
    "    all_column_names = set(file.readline().replace(\"\\\"\", \"\").replace(\"\\n\", \"\").split(\",\"))\n",
    "to_drop = {'Accuracy Score', 'Accuracy Type', 'Number', 'Street', 'Unit Type', 'Unit Number',\n",
    "           'State', 'Zip', 'Country', 'Source', 'Census Year', 'State FIPS', 'County FIPS',\n",
    "           'Place Name', 'Place FIPS', 'Census Tract Code', 'Census Block Code', 'Census Block Group',\n",
    "           'Metro/Micro Statistical Area Code', 'Metro/Micro Statistical Area Type',\n",
    "           'Combined Statistical Area Code', 'Metropolitan Division Area Code', 'court_location',\n",
    "           'defendant', 'defendant_atty', 'defendant_atty_address_apt',\n",
    "           'defendant_atty_address_city', 'defendant_atty_address_name', 'defendant_atty_address_state',\n",
    "           'defendant_atty_address_street', 'defendant_atty_address_zip', 'docket_history', 'execution', 'judgment_for',\n",
    "           'judgment_total', 'latest_docket_date', 'plaintiff', 'plaintiff_atty', 'plaintiff_atty_address_apt',\n",
    "           'plaintiff_atty_address_city', 'plaintiff_atty_address_name', 'plaintiff_atty_address_state',\n",
    "           'plaintiff_atty_address_street', 'plaintiff_atty_address_zip', 'Metropolitan Division Area Name',\n",
    "           'property_address_city', 'property_address_state', 'property_address_street',\n",
    "           'property_address_zip'}\n",
    "evictions_df = pd.read_csv(INPUT_DATA_EVICTIONS, usecols=set(all_column_names) - set(to_drop))\n",
    "original_N = len(evictions_df)\n",
    "if VERBOSE:\n",
    "    print(f\"Beginning with {original_N} observations.\")\n",
    "\n",
    "# Drop cases missing file_date.\n",
    "mask = evictions_df['file_date'].notna()\n",
    "if VERBOSE:\n",
    "    print(\n",
    "        f\"Dropping {(~mask).sum()} observations where file_date is missing.\")\n",
    "evictions_df = evictions_df.loc[mask, :]\n",
    "\n",
    "# Add file month and year to dataset.\n",
    "evictions_df.loc[:, 'file_month'] = pd.to_datetime(evictions_df['file_date']).dt.strftime('%Y-%m')\n",
    "evictions_df.loc[:, 'file_year'] = pd.to_datetime(evictions_df['file_date']).dt.year\n",
    "\n",
    "# Clean the values in the judgment_for_pdu variable.\n",
    "judgment_for_pdu_replacement_dict = {\"unknown\": \"Unknown\",\n",
    "                                     \"plaintiff\": \"Plaintiff\",\n",
    "                                     \"defendant\": \"Defendant\"}\n",
    "evictions_df.loc[:, \"judgment_for_pdu\"] = (evictions_df.loc[:, \"judgment_for_pdu\"]\n",
    "                                           .replace(judgment_for_pdu_replacement_dict))\n",
    "\n",
    "# Replace missing values in money judgment column with zeroes.\n",
    "evictions_df.loc[:, 'judgment'] = evictions_df['judgment'].fillna(0)\n",
    "\n",
    "# Rename duration to case_duration.\n",
    "evictions_df = evictions_df.rename(columns={'duration': 'case_duration'})\n",
    "\n",
    "# Drop malformed addresses.\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {evictions_df['property_address_full'].str.contains('span, span span').sum()} observations which \"\n",
    "          f\"have malformed addresses.\")\n",
    "evictions_df = evictions_df.loc[~evictions_df['property_address_full'].str.contains(\"span, span span\"), :]\n",
    "\n",
    "# Drop addresses without latitude and longitude.\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {evictions_df[['longitude', 'latitude']].isna().any(axis=1).sum()} evictions missing latitude \"\n",
    "          f\"or longitude.\")\n",
    "evictions_df = evictions_df.dropna(subset=['longitude', 'latitude'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Merging Evictions With Census Tract Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully merged 40732 observations with census tracts.\n"
     ]
    }
   ],
   "source": [
    "# Merge with census tract characteristics.\n",
    "evictions_df = evictions_df.rename(columns={'Full FIPS (tract)': 'tract_geoid'})\n",
    "evictions_tracts_df = evictions_df.merge(pd.read_csv(INPUT_DATA_TRACTS, dtype={'tract_geoid': float}),\n",
    "                                  on='tract_geoid',\n",
    "                                  how='left',\n",
    "                                  validate='m:1').set_index('case_number')\n",
    "if VERBOSE:\n",
    "    print(f\"Successfully merged {evictions_tracts_df['med_hhinc2016'].notna().sum()} observations with census tracts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Merging Evictions With Zestimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully matched 11496 evictions (28.22 percent of observations) to Zestimates.\n"
     ]
    }
   ],
   "source": [
    "evictions_tracts_zestimates_df = pd.read_csv(INPUT_DATA_ZESTIMATES, index_col='case_number').merge(evictions_tracts_df,\n",
    "                                                     right_index=True,\n",
    "                                                     left_index=True,\n",
    "                                                     how='right',\n",
    "                                                     validate='1:1')\n",
    "if VERBOSE:\n",
    "    successfully_matched_observations = (~evictions_tracts_zestimates_df['2022-12'].isna()).sum()\n",
    "    print(\n",
    "        f\"Successfully matched {successfully_matched_observations} evictions \"\n",
    "        f\"({100 * (successfully_matched_observations / len(evictions_tracts_zestimates_df)) :.2f} percent of observations) to \"\n",
    "        f\"Zestimates.\")\n",
    "\n",
    "# Rename columns containing Zestimates.\n",
    "years = [str(year) for year in range(2013, 2023)]\n",
    "months = [\"0\" + str(month) for month in range(1, 10)] + [str(month) for month in range(10, 13)]\n",
    "value_vars = [\"2012-12\"] + [str(year) + \"-\" + str(month) for year in years for month in months]\n",
    "value_vars_zestimates, _, _ = generate_variable_names('zestimate')\n",
    "for value_var, value_var_zestimates in zip(value_vars, value_vars_zestimates):\n",
    "    evictions_tracts_zestimates_df = evictions_tracts_zestimates_df.rename(columns={value_var: value_var_zestimates})\n",
    "value_vars_to_concat.append(evictions_tracts_zestimates_df[value_vars_zestimates])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Merge Evictions with Crimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Request computing resources.\n",
    "cluster = SLURMCluster(queue='batch',\n",
    "                       cores=32,\n",
    "                       memory='230 GB',\n",
    "                       walltime='01:00:00',\n",
    "                      scheduler_options={'dashboard_address': '8787'} )\n",
    "cluster.scale(jobs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GeoSeries containing eviction Points as its geometry, case_number as its index, and no other columns.\n",
    "evictions_gdf = gpd.GeoDataFrame(evictions_df, geometry=gpd.points_from_xy(evictions_df['longitude'], evictions_df['latitude']))[['geometry', 'case_number']]\n",
    "evictions_gdf = evictions_gdf.set_crs(\"EPSG:4326\", allow_override=True).to_crs('EPSG:26986')\n",
    "\n",
    "# Read crime data as Dask DataFrame, then compute back to DataFrame.\n",
    "crime_df = (dd.read_csv(INPUT_DATA_CRIME + \"/*.csv\", dtype={'REPORTING_AREA': 'object', 'SHOOTING': 'object'})\n",
    "                .dropna(subset=['Long', 'Lat', 'OCCURRED_ON_DATE'])  # Drop crimes missing latitude, longitude, or date, as they cannot be merged with panel.\n",
    "                .rename(columns={'OCCURRED_ON_DATE': 'month_of_crime_incident'})\n",
    "                .drop(columns=['OFFENSE_CODE', 'OFFENSE_CODE_GROUP', 'OFFENSE_DESCRIPTION', 'DISTRICT', 'REPORTING_AREA', 'SHOOTING', 'YEAR', 'MONTH',\n",
    "                               'DAY_OF_WEEK', 'HOUR', 'UCR_PART', 'STREET', 'Location'])  # Drop unneeded columns\n",
    "                .compute())\n",
    "                # Must call compute here and then briefly convert back to in-memory dataset because dask_geopandas.points_from_xy not working.\n",
    "# Keep track of the month of crime incident in YYYY-MM format.\n",
    "crime_df.loc[:, 'month_of_crime_incident'] = pd.to_datetime(crime_df['month_of_crime_incident'].str[:10]).dt.to_period(\"M\").astype(str)\n",
    "# Convert DataFrame to GeoDataFrame.\n",
    "crime_gdf = gpd.GeoDataFrame(crime_df, geometry=gpd.points_from_xy(crime_df['Long'], crime_df['Lat'])).drop(columns=['Long', 'Lat'])\n",
    "crime_gdf = crime_gdf.set_crs(\"EPSG:4326\", allow_override=True).to_crs(\"EPSG:26986\")  # Convert to the correct CRS.\n",
    "# Convert GeoDataFrame to Dask GeoDataFrame.\n",
    "crime_dgdf = dask_geopandas.from_geopandas(crime_gdf, npartitions=N_PARTITIONS).repartition(partition_size='25 MB')\n",
    "\n",
    "columns_for_each_outcome = []\n",
    "for radius in [60, 90, 140, 200]:\n",
    "    # Create a new GeoDataFrame with geometry equal to circles around each eviction with the current radius.\n",
    "    current_evictions_gdf = evictions_gdf.copy()\n",
    "    current_evictions_gdf.geometry = current_evictions_gdf.geometry.buffer(radius)\n",
    "    current_evictions_dgdf = dask_geopandas.from_geopandas(current_evictions_gdf, npartitions=N_PARTITIONS).repartition(partition_size='25 MB')\n",
    "    \n",
    "    # Merge evictions with crimes that fall into radius.\n",
    "    current_evictions_crime_dgdf = dask_geopandas.sjoin(crime_dgdf, current_evictions_dgdf, how='inner', predicate='within')\n",
    "    current_evictions_crime_dgdf = current_evictions_crime_dgdf.drop(columns=['geometry','index_right'])\n",
    "    \n",
    "    # Aggregate crimes to case-month level.\n",
    "    current_panel_long = (current_evictions_crime_dgdf\n",
    "                          .groupby(['case_number', 'month_of_crime_incident'])\n",
    "                          .aggregate({'INCIDENT_NUMBER': 'count'})\n",
    "                          .reset_index()\n",
    "                          .rename(columns={'INCIDENT_NUMBER': 'crime_incidents'})\n",
    "                          .compute())\n",
    "    current_panel_wide = pd.pivot(current_panel_long, index=['case_number'], columns=['month_of_crime_incident'],\n",
    "                                  values='crime_incidents').reset_index().set_index('case_number')\n",
    "    current_panel_wide.columns = [f'{column}_any_crime_{radius}m' for column in current_panel_wide.columns]\n",
    "    columns_for_each_outcome.append(current_panel_wide.columns)\n",
    "    value_vars_to_concat.append(current_panel_wide)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Producing the Unrestricted Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrestricted_df = pd.concat([evictions_tracts_df] + value_vars_to_concat, axis=1)\n",
    "# For evictions not matched to any crimes, fill NA values with 0.\n",
    "for group_of_columns in columns_for_each_outcome:\n",
    "    unrestricted_df[group_of_columns] = unrestricted_df[group_of_columns].fillna(0)\n",
    "unrestricted_df.to_csv(OUTPUT_DATA_UNRESTRICTED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Producing the Samples Used in Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 16883 cases resolved through mediation.\n"
     ]
    }
   ],
   "source": [
    "# Drop cases resolved via mediation.\n",
    "mediated_mask = unrestricted_df['disposition_found'] == \"Mediated\"\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {mediated_mask.sum()} cases resolved through mediation.\")\n",
    "unrestricted_df = unrestricted_df.loc[~mediated_mask, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Droppping 7455 cases resolved through voluntary dismissal.\n"
     ]
    }
   ],
   "source": [
    "# Drop cases resolved via volntary dismissal (dropped by plaintiff). \n",
    "voluntary_dismissal_mask = unrestricted_df['disposition'].str.contains(\"R 41(a)(1) Voluntary Dismissal on\", na=False, regex=False)\n",
    "if VERBOSE:\n",
    "    print(f\"Droppping {voluntary_dismissal_mask.sum()} cases resolved through voluntary dismissal.\")\n",
    "unrestricted_df = unrestricted_df.loc[~voluntary_dismissal_mask, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 245 cases where disposition_found is \"Other\"\n"
     ]
    }
   ],
   "source": [
    "# Drop cases where disposition found is other.\n",
    "disposition_found_other_mask = unrestricted_df['disposition_found'] == \"Other\"\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {disposition_found_other_mask.sum()} cases where disposition_found is \\\"Other\\\"\")\n",
    "unrestricted_df = unrestricted_df.loc[~disposition_found_other_mask, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 2 observations where disposition_found is \"Defaulted\" but judgment_for_pdu is \"Defendant\".\n",
      "Dropping 150 observations where disposition_found is \"Dismissed\" but judgment_for_pdu is \"Plaintiff\".\n"
     ]
    }
   ],
   "source": [
    "# Drop rows which contain consistent values of disposition_found and judgment_for_pdu.\n",
    "\n",
    "# First, we drop cases where disposition_found is \"Defaulted\" but judgment_for_pdu is \"Defendant\"\n",
    "inconsistent_mask_1 = ((unrestricted_df['disposition_found'] == \"Defaulted\") & (unrestricted_df['judgment_for_pdu'] == \"Defendant\"))\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {inconsistent_mask_1.sum()} observations where disposition_found is \\\"Defaulted\\\" but judgment_for_pdu is \\\"Defendant\\\".\")\n",
    "unrestricted_df = unrestricted_df.loc[~inconsistent_mask_1, :]\n",
    "          \n",
    "# Next, we drop cases where disposition_found is \"Dismissed\" yet judgment_for_pdu is \"Plaintiff\"\n",
    "inconsistent_mask_2 = ((unrestricted_df['disposition_found'] == \"Dismissed\") & (unrestricted_df['judgment_for_pdu'] == \"Plaintiff\"))\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {inconsistent_mask_2.sum()} observations where disposition_found is \\\"Dismissed\\\" but judgment_for_pdu is \\\"Plaintiff\\\".\")\n",
    "unrestricted_df = unrestricted_df.loc[~inconsistent_mask_2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a variable indicating judgment in favor of defendant.\n",
    "unrestricted_df.loc[:, 'judgment_for_defendant'] = 0\n",
    "defendant_won_mask = ((unrestricted_df['disposition_found'] == \"Dismissed\") |\n",
    "                      (unrestricted_df['judgment_for_pdu'] == \"Defendant\"))\n",
    "unrestricted_df.loc[defendant_won_mask, 'judgment_for_defendant'] = 1\n",
    "\n",
    "# Generate a variable indicating judgement in favor of plaintiff.\n",
    "unrestricted_df.loc[:, 'judgment_for_plaintiff'] = 1 - unrestricted_df['judgment_for_defendant']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6a. Producing the Zillow Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limiting sample to 3298 evictions for which we observe Zestimates at every month from 2012-12 to 2022-12.\n"
     ]
    }
   ],
   "source": [
    "# Drop rows where we are missing any Zestimates.\n",
    "zestimates_df = unrestricted_df.copy()\n",
    "has_all_zestimates_mask = zestimates_df[value_vars_zestimates].notna().all(axis=1)\n",
    "if VERBOSE:\n",
    "    print(f\"Limiting sample to {has_all_zestimates_mask.sum()} evictions for which we observe Zestimates at every month from 2012-12 to 2022-12.\")\n",
    "zestimates_df = zestimates_df.loc[has_all_zestimates_mask, :]\n",
    "zestimates_df.to_csv(OUTPUT_DATA_ZILLOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6b. Producing the Crime Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limiting sample to 2318 observations which are in Boston.\n"
     ]
    }
   ],
   "source": [
    "crime_df = unrestricted_df.copy()\n",
    "# Restrict to evictions which took place in Boston.\n",
    "boston_mask = ((crime_df['County'] == \"Suffolk County\") & (~crime_df['City'].isin([\"Chelsea\", \"Revere\", \"Winthrop\"])))\n",
    "if VERBOSE:\n",
    "    print(f\"Limiting sample to {boston_mask.sum()} observations which are in Boston.\")\n",
    "crime_df = crime_df.loc[boston_mask, :]\n",
    "crime_df.to_csv(OUTPUT_DATA_CRIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
