{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06_merge_monthly.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from dask.distributed import Client\n",
    "import dask.dataframe as dd\n",
    "from dask_jobqueue import SLURMCluster\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import contextily as cx\n",
    "from build_utilities import generate_variable_names, aggregate_crime_to_case_month\n",
    "import dask_geopandas\n",
    "INPUT_DATA_EVICTIONS = \"../../data/02_intermediate/evictions.csv\"\n",
    "INPUT_DATA_TRACTS = \"../../data/02_intermediate/tracts.csv\"\n",
    "INPUT_DATA_TAX_PARCELS = \"../../data/02_intermediate/tax_parcels.gpkg\"\n",
    "INPUT_DATA_ZESTIMATES = \"../../data/02_intermediate/zestimates.csv\"\n",
    "INPUT_DATA_CRIME = \"../../data/01_raw/crime_incidents\"\n",
    "OUTPUT_DATA_UNRESTRICTED = \"../../data/03_cleaned/unrestricted_monthly.parquet\"\n",
    "OUTPUT_DATA_ZILLOW = \"../../data/03_cleaned/zestimates_analysis.csv\"\n",
    "OUTPUT_DATA_CRIME = \"../../data/03_cleaned/crime_analysis_monthly.parquet\"\n",
    "OUTPUT_TABLES = \"../../output/summary_statistics/tables/\"\n",
    "VERBOSE = True\n",
    "N_PARTITIONS = 1\n",
    "value_vars_to_concat = []  # A list of DataFrames, where each DataFrame contains the panel data for a single outcome variable and has case_number as its index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Evictions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restricting to 6856 observations which are from Boston.\n",
      "Beginning with 6856 observations.\n",
      "Dropping 784 observations where latest_docket_date is missing.\n",
      "Dropping 1 observations which have malformed addresses.\n",
      "Dropping 0 rows missing property_address_full\n",
      "Dropping 2719 cases which concluded after pandemic began\n",
      "Dropping 1651 cases resolved through mediation.\n",
      "Dropping 12 cases where disposition_found is \"Other\"\n",
      "Dropping 0 observations where disposition_found is \"Defaulted\" but judgment_for_pdu is \"Defendant\".\n",
      "Dropping 0 observations where disposition_found is \"Dismissed\" but judgment_for_pdu is \"Plaintiff\".\n"
     ]
    }
   ],
   "source": [
    "# Load evictions data.\n",
    "with open(INPUT_DATA_EVICTIONS, 'r') as file:\n",
    "    all_column_names = set(file.readline().replace(\"\\\"\", \"\").replace(\"\\n\", \"\").split(\",\"))\n",
    "to_drop = {'Accuracy Score', 'Accuracy Type', 'Number', 'Street', 'Unit Type', 'Unit Number',\n",
    "           'State', 'Zip', 'Country', 'Source', 'Census Year', 'State FIPS', 'County FIPS',\n",
    "           'Place Name', 'Place FIPS', 'Census Tract Code', 'Census Block Code', 'Census Block Group',\n",
    "           'Metro/Micro Statistical Area Code', 'Metro/Micro Statistical Area Type',\n",
    "           'Combined Statistical Area Code', 'Metropolitan Division Area Code', 'court_location',\n",
    "           'defendant', 'defendant_atty', 'defendant_atty_address_apt',\n",
    "           'defendant_atty_address_city', 'defendant_atty_address_name', 'defendant_atty_address_state',\n",
    "           'defendant_atty_address_street', 'defendant_atty_address_zip', 'docket_history', 'judgment_for',\n",
    "           'judgment_total', 'plaintiff', 'plaintiff_atty', 'plaintiff_atty_address_apt',\n",
    "           'plaintiff_atty_address_city', 'plaintiff_atty_address_name', 'plaintiff_atty_address_state',\n",
    "           'plaintiff_atty_address_street', 'plaintiff_atty_address_zip', 'Metropolitan Division Area Name',\n",
    "           'property_address_city', 'property_address_state', 'property_address_street',\n",
    "           'property_address_zip'}\n",
    "evictions_df = pd.read_csv(INPUT_DATA_EVICTIONS, usecols=set(all_column_names) - set(to_drop))\n",
    "sample_restriction_table_index = []\n",
    "sample_restriction_table_values = []\n",
    "\n",
    "# Restrict to cases in Boston.\n",
    "boston_mask = ((evictions_df['County'] == \"Suffolk County\") & (~evictions_df['City'].isin([\"Chelsea\", \"Revere\", \"Winthrop\"])))\n",
    "if VERBOSE:\n",
    "    print(f\"Restricting to {boston_mask.sum()} observations which are from Boston.\")\n",
    "evictions_df = evictions_df.loc[boston_mask, :]\n",
    "original_N = len(evictions_df)\n",
    "if VERBOSE:\n",
    "    print(f\"Beginning with {original_N} observations.\")\n",
    "sample_restriction_table_index.append(\"Case Filed in Boston\")\n",
    "sample_restriction_table_values.append(original_N)\n",
    "\n",
    "# Drop cases missing latest_docket_date.\n",
    "mask = evictions_df['latest_docket_date'].notna()\n",
    "if VERBOSE:\n",
    "    print(\n",
    "        f\"Dropping {evictions_df['latest_docket_date'].isna().sum()} observations where latest_docket_date is missing.\")\n",
    "evictions_df = evictions_df.loc[mask, :]\n",
    "sample_restriction_table_index.append(\"Non-missing latest docket date\")\n",
    "sample_restriction_table_values.append(len(evictions_df))\n",
    "\n",
    "# Drop malformed addresses.\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {evictions_df['property_address_full'].str.contains('span, span span').sum()} observations which \"\n",
    "          f\"have malformed addresses.\")\n",
    "evictions_df = evictions_df.loc[~evictions_df['property_address_full'].str.contains(\"span, span span\"), :]\n",
    "\n",
    "# Drop missing addresses.\n",
    "no_address_info_mask = (evictions_df['property_address_full'].isna())\n",
    "if VERBOSE:\n",
    "    print(\n",
    "        f\"Dropping {no_address_info_mask.sum()} rows missing property_address_full\")\n",
    "evictions_df = evictions_df.loc[~no_address_info_mask, :]\n",
    "sample_restriction_table_index.append(\"Non-missing property address\")\n",
    "sample_restriction_table_values.append(len(evictions_df))\n",
    "\n",
    "# Add file month and year to dataset.\n",
    "evictions_df.loc[:, 'file_month'] = pd.to_datetime(evictions_df['file_date']).dt.strftime('%Y-%m')\n",
    "evictions_df.loc[:, 'file_year'] = pd.to_datetime(evictions_df['file_date']).dt.year\n",
    "\n",
    "# Add latest docket month and year to dataset.\n",
    "evictions_df.loc[:, 'latest_docket_month'] = pd.to_datetime(evictions_df['latest_docket_date']).dt.strftime('%Y-%m')\n",
    "evictions_df.loc[:, 'latest_docket_year'] = pd.to_datetime(evictions_df['latest_docket_date']).dt.year\n",
    "\n",
    "# Drop cases concluded in April 2020 or later.\n",
    "pre_pandemic_months = ['2019-04',\n",
    " '2019-05',\n",
    " '2019-06',\n",
    " '2019-07',\n",
    " '2019-08',\n",
    " '2019-09',\n",
    " '2019-10',\n",
    " '2019-11',\n",
    " '2019-12',\n",
    " '2020-01',\n",
    " '2020-02',\n",
    " '2020-03']\n",
    "pre_pandemic_mask = evictions_df['latest_docket_month'].isin(pre_pandemic_months)\n",
    "evictions_df = evictions_df.loc[pre_pandemic_mask, :]\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {(~pre_pandemic_mask).sum()} cases which concluded after pandemic began\")\n",
    "sample_restriction_table_index.append(\"Case concluded before April 2020\")\n",
    "sample_restriction_table_values.append(len(evictions_df))\n",
    "\n",
    "\n",
    "# Drop cases resolved via mediation.\n",
    "mediated_mask = evictions_df['disposition_found'] == \"Mediated\"\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {mediated_mask.sum()} cases resolved through mediation.\")\n",
    "evictions_df = evictions_df.loc[~mediated_mask, :]\n",
    "sample_restriction_table_index.append(\"Case not resolved through mediation\")\n",
    "sample_restriction_table_values.append(len(evictions_df))\n",
    "\n",
    "# Drop cases where disposition found is other.\n",
    "disposition_found_other_mask = evictions_df['disposition_found'] == \"Other\"\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {disposition_found_other_mask.sum()} cases where disposition_found is \\\"Other\\\"\")\n",
    "evictions_df = evictions_df.loc[~disposition_found_other_mask, :]\n",
    "# Drop rows which contain inconsistent values of disposition_found and judgment_for_pdu.\n",
    "# First, we drop cases where disposition_found is \"Defaulted\" but judgment_for_pdu is \"Defendant\"\n",
    "inconsistent_mask_1 = ((evictions_df['disposition_found'] == \"Defaulted\") & (evictions_df['judgment_for_pdu'] == \"Defendant\"))\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {inconsistent_mask_1.sum()} observations where disposition_found is \\\"Defaulted\\\" but judgment_for_pdu is \\\"Defendant\\\".\")\n",
    "evictions_df = evictions_df.loc[~inconsistent_mask_1, :]\n",
    "          \n",
    "# Next, we drop cases where disposition_found is \"Dismissed\" yet judgment_for_pdu is \"Plaintiff\"\n",
    "inconsistent_mask_2 = ((evictions_df['disposition_found'] == \"Dismissed\") & (evictions_df['judgment_for_pdu'] == \"Plaintiff\"))\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {inconsistent_mask_2.sum()} observations where disposition_found is \\\"Dismissed\\\" but judgment_for_pdu is \\\"Plaintiff\\\".\")\n",
    "evictions_df = evictions_df.loc[~inconsistent_mask_2, :]\n",
    "sample_restriction_table_index.append(\"Succesfully scraped judgment\")\n",
    "sample_restriction_table_values.append(len(evictions_df))\n",
    "\n",
    "# Clean the values in the judgment_for_pdu variable.\n",
    "judgment_for_pdu_replacement_dict = {\"unknown\": \"Unknown\",\n",
    "                                     \"plaintiff\": \"Plaintiff\",\n",
    "                                     \"defendant\": \"Defendant\"}\n",
    "evictions_df.loc[:, \"judgment_for_pdu\"] = (evictions_df.loc[:, \"judgment_for_pdu\"]\n",
    "                                           .replace(judgment_for_pdu_replacement_dict))\n",
    "\n",
    "# Replace missing values in money judgment column with zeroes.\n",
    "evictions_df.loc[:, 'judgment'] = evictions_df['judgment'].fillna(0)\n",
    "\n",
    "# Rename duration to case_duration.\n",
    "evictions_df = evictions_df.rename(columns={'duration': 'case_duration'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Observations</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Restriction</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Case Filed in Boston</th>\n",
       "      <td>6856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non-missing latest docket date</th>\n",
       "      <td>6072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non-missing property address</th>\n",
       "      <td>6071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Case concluded before April 2020</th>\n",
       "      <td>3352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Case not resolved through mediation</th>\n",
       "      <td>1701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Succesfully scraped judgment</th>\n",
       "      <td>1689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Observations\n",
       "Restriction                                      \n",
       "Case Filed in Boston                         6856\n",
       "Non-missing latest docket date               6072\n",
       "Non-missing property address                 6071\n",
       "Case concluded before April 2020             3352\n",
       "Case not resolved through mediation          1701\n",
       "Succesfully scraped judgment                 1689"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_restriction_table = pd.DataFrame()\n",
    "sample_restriction_table[\"Restriction\"] = sample_restriction_table_index\n",
    "sample_restriction_table[\"Observations\"] = sample_restriction_table_values                         \n",
    "sample_restriction_table = sample_restriction_table.set_index(\"Restriction\")\n",
    "\n",
    "# Export to LaTeX.\n",
    "filename = os.path.join(OUTPUT_TABLES, \"sample_restriction.tex\")\n",
    "sample_restriction_table.style.format(formatter=\"{:,.0f}\").to_latex(filename, hrules=True)\n",
    "sample_restriction_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Merging Evictions With Census Tract Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully merged 1689 observations with census tracts.\n"
     ]
    }
   ],
   "source": [
    "# Merge with census tract characteristics.\n",
    "evictions_df = evictions_df.rename(columns={'Full FIPS (tract)': 'tract_geoid'})\n",
    "evictions_tracts_df = evictions_df.merge(pd.read_csv(INPUT_DATA_TRACTS, dtype={'tract_geoid': float}),\n",
    "                                  on='tract_geoid',\n",
    "                                  how='left',\n",
    "                                  validate='m:1').set_index('case_number')\n",
    "if VERBOSE:\n",
    "    print(f\"Successfully merged {evictions_tracts_df['med_hhinc2016'].notna().sum()} observations with census tracts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Merge Evictions with Crimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Request computing resources.\n",
    "cluster = SLURMCluster(queue='batch',\n",
    "                       cores=32,\n",
    "                       memory='230 GB',\n",
    "                       walltime='00:20:00',\n",
    "                      scheduler_options={'dashboard_address': '8787'} )\n",
    "cluster.scale(jobs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GeoSeries containing eviction Points as its geometry, case_number as its index, and no other columns.\n",
    "evictions_gdf = gpd.GeoDataFrame(evictions_df, geometry=gpd.points_from_xy(evictions_df['Longitude'], evictions_df['Latitude']))[['geometry', 'case_number']]\n",
    "evictions_gdf = evictions_gdf.set_crs(\"EPSG:4326\", allow_override=True).to_crs('EPSG:26986')\n",
    "\n",
    "# Read crime data as Dask DataFrame, then compute back to DataFrame.\n",
    "crime_df = (dd.read_csv(INPUT_DATA_CRIME + \"/*.csv\", dtype={'REPORTING_AREA': 'object', 'SHOOTING': 'object'})\n",
    "                .dropna(subset=['Long', 'Lat', 'OCCURRED_ON_DATE'])  # Drop crimes missing latitude, longitude, or date, as they cannot be merged with panel.\n",
    "                .rename(columns={'OCCURRED_ON_DATE': 'month_of_crime_incident'})\n",
    "                .drop(columns=['OFFENSE_CODE_GROUP', 'OFFENSE_DESCRIPTION', 'DISTRICT', 'REPORTING_AREA', 'SHOOTING', 'YEAR', 'MONTH',\n",
    "                               'DAY_OF_WEEK', 'HOUR', 'UCR_PART', 'STREET', 'Location'])  # Drop unneeded columns\n",
    "                .compute())\n",
    "                # Must call compute here and then briefly convert back to in-memory dataset because dask_geopandas.points_from_xy not working.\n",
    "crime_df.loc[:, 'INCIDENT_NUMBER'] = 1  # Replace column with 1s so we can count crimes using sum function.\n",
    "# Keep track of the month of crime incident in YYYY-MM format.\n",
    "crime_df.loc[:, 'month_of_crime_incident'] = pd.to_datetime(crime_df['month_of_crime_incident'].str[:10]).dt.to_period(\"M\").astype(str)\n",
    "# Convert DataFrame to GeoDataFrame.\n",
    "crime_gdf = gpd.GeoDataFrame(crime_df, geometry=gpd.points_from_xy(crime_df['Long'], crime_df['Lat']))\n",
    "crime_gdf = crime_gdf.set_crs(\"EPSG:4326\", allow_override=True).to_crs(\"EPSG:26986\")  # Convert to the correct CRS.\n",
    "# Convert GeoDataFrame to Dask GeoDataFrame.\n",
    "crime_dgdf = dask_geopandas.from_geopandas(crime_gdf, npartitions=N_PARTITIONS).repartition(partition_size='5 MB')\n",
    "\n",
    "\n",
    "# Create lists containing each group of offense codes.\n",
    "# Group 1\n",
    "vandalism = [1402, 1415]\n",
    "# Group 2\n",
    "violence_offense_codes = [111, 801, 802,803, 423, 413, 401, 402, 403, 404, 411, 412, 413, 421, 422, 423, 424, 431, 432 ,433]\n",
    "# Group 3\n",
    "substance_abuse_response_codes = [3023, 3021, 1875, 3022, 1832, 1831, 1830]\n",
    "# Group 4\n",
    "firearm_codes = [3119, 1501, 1503, 1502, 1504, 1510, 3017, 3016, 1501, 3119, 3203, 3204, 1503,\n",
    "                 1502]\n",
    "# Group 5\n",
    "rape_codes = [242, 254, 271, 244, 251, 241, 243,261, 252,253, 222, 230, 236, 211, 232, 233, 224,\n",
    "              213, 223, 231, 237, 212, 234, 235, 242]\n",
    "\n",
    "# Group 6\n",
    "burglary_codes = [2616, 541, 540, 562, 561, 542, 521, 520, 522, 560]\n",
    "larceny_codes = [616, 618, 617, 614, 619, 611, 616, 626, 636, 618, 628, 638, 617, 627,\n",
    "                 637, 614, 624, 634, 619, 629,639,621,611,631,612,622,632,613,623,633,\n",
    "                 615,625,635,649, 612, 613, 615]\n",
    "\n",
    "\n",
    "columns_for_each_outcome = []\n",
    "for offense_group_num, offense_group in enumerate(['all',\n",
    "                                                   vandalism,\n",
    "                                                   violence_offense_codes,\n",
    "                                                   substance_abuse_response_codes,\n",
    "                                                   firearm_codes,\n",
    "                                                   rape_codes,\n",
    "                                                   burglary_codes+larceny_codes]):\n",
    "    for radius in [500]:\n",
    "        # Create a new GeoDataFrame with geometry equal to circles around each eviction with the current radius.\n",
    "        current_evictions_gdf = evictions_gdf.copy()\n",
    "        current_evictions_gdf.geometry = current_evictions_gdf.geometry.buffer(radius)\n",
    "        current_evictions_dgdf = dask_geopandas.from_geopandas(current_evictions_gdf, npartitions=N_PARTITIONS).repartition(partition_size='5 MB')\n",
    "\n",
    "        # Merge evictions with crimes that fall into radius.\n",
    "        current_evictions_crime_dgdf = dask_geopandas.sjoin(crime_dgdf, current_evictions_dgdf, how='inner', predicate='within')\n",
    "        current_evictions_crime_dgdf = current_evictions_crime_dgdf.drop(columns=['geometry','index_right'])\n",
    "        \n",
    "        \n",
    "        if offense_group != 'all':  # If we are summing a specific subcategory of crimes\n",
    "            mask = current_evictions_crime_dgdf['OFFENSE_CODE'].isin(offense_group)\n",
    "            # Multiply mask by 'INCIDENT_NUMBER' to zero out crimes in different offense groups.\n",
    "            current_evictions_crime_dgdf['INCIDENT_NUMBER'] = current_evictions_crime_dgdf['INCIDENT_NUMBER'] * mask\n",
    "\n",
    "            \n",
    "        # Aggregate crimes to case-month level.\n",
    "        current_panel_long = (current_evictions_crime_dgdf\n",
    "                              .groupby(['case_number', 'month_of_crime_incident'])\n",
    "                              .aggregate({'INCIDENT_NUMBER': 'sum'})\n",
    "                              .reset_index()\n",
    "                              .rename(columns={'INCIDENT_NUMBER': 'crime_incidents'})\n",
    "                              .compute())\n",
    "\n",
    "        current_panel_wide = pd.pivot(current_panel_long, index=['case_number'], columns=['month_of_crime_incident'],\n",
    "                                      values='crime_incidents').reset_index().set_index('case_number')\n",
    "        current_panel_wide.columns = [f'{column}_group_{offense_group_num}_crimes_{radius}m' for column in current_panel_wide.columns]\n",
    "        columns_for_each_outcome.append(current_panel_wide.columns)\n",
    "        value_vars_to_concat.append(dd.from_pandas(current_panel_wide, npartitions=N_PARTITIONS))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Producing the Unrestricted Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrestricted_df = dd.multi.concat([dd.from_pandas(evictions_tracts_df, npartitions=N_PARTITIONS)] + value_vars_to_concat, axis=1).compute()\n",
    "# For evictions not matched to any crimes, fill NA values with 0.\n",
    "for group_of_columns in columns_for_each_outcome:\n",
    "    unrestricted_df[group_of_columns] = unrestricted_df[group_of_columns].fillna(0)\n",
    "unrestricted_df.to_parquet(OUTPUT_DATA_UNRESTRICTED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Producing the Samples Used in Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop cases resolved via volntary dismissal (dropped by plaintiff). \n",
    "# voluntary_dismissal_mask = unrestricted_df['disposition'].str.contains(\"R 41(a)(1) Voluntary Dismissal on\", na=False, regex=False)\n",
    "# if VERBOSE:\n",
    "#     print(f\"Droppping {voluntary_dismissal_mask.sum()} cases resolved through voluntary dismissal.\")\n",
    "# unrestricted_df = unrestricted_df.loc[~voluntary_dismissal_mask, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a variable indicating judgment in favor of defendant.\n",
    "unrestricted_df.loc[:, 'judgment_for_defendant'] = 0\n",
    "defendant_won_mask = ((unrestricted_df['disposition_found'] == \"Dismissed\") |\n",
    "                      (unrestricted_df['judgment_for_pdu'] == \"Defendant\") |\n",
    "                      (unrestricted_df['disposition'].str.contains('R 41(a)(1) Voluntary Dismissal', regex=False)))\n",
    "unrestricted_df.loc[defendant_won_mask, 'judgment_for_defendant'] = 1\n",
    "\n",
    "# Generate a variable indicating judgement in favor of plaintiff.\n",
    "unrestricted_df.loc[:, 'judgment_for_plaintiff'] = 1 - unrestricted_df['judgment_for_defendant']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6b. Producing the Crime Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_df = unrestricted_df.copy()\n",
    "crime_df.to_parquet(OUTPUT_DATA_CRIME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
