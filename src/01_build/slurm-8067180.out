## SLURM PROLOG ###############################################################
##    Job ID : 8067180
##  Job Name : dask-worker
##  Nodelist : node1745
##      CPUs : 1
##  Mem/Node : 191488 MB
## Directory : /gpfs/home/ashanmu1/seniorthesis/src/01_build
##   Job Started : Fri Feb  3 22:18:52 EST 2023
###############################################################################
2023-02-03 22:18:53,348 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.213.45:36941'
2023-02-03 22:18:53,350 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.213.45:42444'
2023-02-03 22:18:53,353 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.213.45:40458'
2023-02-03 22:18:53,353 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.213.45:46841'
2023-02-03 22:18:53,354 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.213.45:38002'
2023-02-03 22:18:53,360 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.213.45:36631'
2023-02-03 22:18:53,361 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.213.45:38855'
2023-02-03 22:18:53,363 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.20.213.45:41859'
2023-02-03 22:18:54,111 - distributed.worker - INFO -       Start worker at:  tcp://172.20.213.45:46806
2023-02-03 22:18:54,111 - distributed.worker - INFO -          Listening to:  tcp://172.20.213.45:46806
2023-02-03 22:18:54,111 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-1
2023-02-03 22:18:54,111 - distributed.worker - INFO -          dashboard at:        172.20.213.45:41626
2023-02-03 22:18:54,111 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.1:36032
2023-02-03 22:18:54,111 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 22:18:54,111 - distributed.worker - INFO -               Threads:                          4
2023-02-03 22:18:54,111 - distributed.worker - INFO -                Memory:                  23.28 GiB
2023-02-03 22:18:54,111 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bkygnisr
2023-02-03 22:18:54,111 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 22:18:54,112 - distributed.worker - INFO -       Start worker at:  tcp://172.20.213.45:46420
2023-02-03 22:18:54,112 - distributed.worker - INFO -          Listening to:  tcp://172.20.213.45:46420
2023-02-03 22:18:54,113 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-3
2023-02-03 22:18:54,113 - distributed.worker - INFO -          dashboard at:        172.20.213.45:34930
2023-02-03 22:18:54,113 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.1:36032
2023-02-03 22:18:54,113 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 22:18:54,113 - distributed.worker - INFO -               Threads:                          4
2023-02-03 22:18:54,113 - distributed.worker - INFO -                Memory:                  23.28 GiB
2023-02-03 22:18:54,113 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-i1s9pm4j
2023-02-03 22:18:54,113 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 22:18:54,116 - distributed.worker - INFO -       Start worker at:  tcp://172.20.213.45:33687
2023-02-03 22:18:54,116 - distributed.worker - INFO -          Listening to:  tcp://172.20.213.45:33687
2023-02-03 22:18:54,117 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-6
2023-02-03 22:18:54,117 - distributed.worker - INFO -          dashboard at:        172.20.213.45:46019
2023-02-03 22:18:54,117 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.1:36032
2023-02-03 22:18:54,117 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 22:18:54,117 - distributed.worker - INFO -               Threads:                          4
2023-02-03 22:18:54,117 - distributed.worker - INFO -                Memory:                  23.28 GiB
2023-02-03 22:18:54,118 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fkakcll2
2023-02-03 22:18:54,118 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 22:18:54,120 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.1:36032
2023-02-03 22:18:54,120 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 22:18:54,120 - distributed.core - INFO - Starting established connection to tcp://172.20.207.1:36032
2023-02-03 22:18:54,121 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.1:36032
2023-02-03 22:18:54,121 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 22:18:54,121 - distributed.core - INFO - Starting established connection to tcp://172.20.207.1:36032
2023-02-03 22:18:54,125 - distributed.worker - INFO -       Start worker at:  tcp://172.20.213.45:36872
2023-02-03 22:18:54,126 - distributed.worker - INFO -          Listening to:  tcp://172.20.213.45:36872
2023-02-03 22:18:54,126 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-0
2023-02-03 22:18:54,126 - distributed.worker - INFO -          dashboard at:        172.20.213.45:42641
2023-02-03 22:18:54,126 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.1:36032
2023-02-03 22:18:54,126 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 22:18:54,126 - distributed.worker - INFO -               Threads:                          4
2023-02-03 22:18:54,126 - distributed.worker - INFO -                Memory:                  23.28 GiB
2023-02-03 22:18:54,126 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_znt16yt
2023-02-03 22:18:54,126 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 22:18:54,126 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.1:36032
2023-02-03 22:18:54,126 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 22:18:54,127 - distributed.core - INFO - Starting established connection to tcp://172.20.207.1:36032
2023-02-03 22:18:54,131 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.1:36032
2023-02-03 22:18:54,132 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 22:18:54,132 - distributed.core - INFO - Starting established connection to tcp://172.20.207.1:36032
2023-02-03 22:18:54,134 - distributed.worker - INFO -       Start worker at:  tcp://172.20.213.45:38851
2023-02-03 22:18:54,134 - distributed.worker - INFO -          Listening to:  tcp://172.20.213.45:38851
2023-02-03 22:18:54,134 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-2
2023-02-03 22:18:54,134 - distributed.worker - INFO -          dashboard at:        172.20.213.45:38704
2023-02-03 22:18:54,134 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.1:36032
2023-02-03 22:18:54,134 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 22:18:54,134 - distributed.worker - INFO -               Threads:                          4
2023-02-03 22:18:54,135 - distributed.worker - INFO -                Memory:                  23.28 GiB
2023-02-03 22:18:54,135 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-vjz_6m__
2023-02-03 22:18:54,134 - distributed.worker - INFO -       Start worker at:  tcp://172.20.213.45:35333
2023-02-03 22:18:54,135 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 22:18:54,135 - distributed.worker - INFO -          Listening to:  tcp://172.20.213.45:35333
2023-02-03 22:18:54,135 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-4
2023-02-03 22:18:54,135 - distributed.worker - INFO -          dashboard at:        172.20.213.45:43906
2023-02-03 22:18:54,135 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.1:36032
2023-02-03 22:18:54,135 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 22:18:54,135 - distributed.worker - INFO -               Threads:                          4
2023-02-03 22:18:54,135 - distributed.worker - INFO -                Memory:                  23.28 GiB
2023-02-03 22:18:54,135 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-tggf0oaa
2023-02-03 22:18:54,135 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 22:18:54,137 - distributed.worker - INFO -       Start worker at:  tcp://172.20.213.45:44241
2023-02-03 22:18:54,137 - distributed.worker - INFO -          Listening to:  tcp://172.20.213.45:44241
2023-02-03 22:18:54,137 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-7
2023-02-03 22:18:54,137 - distributed.worker - INFO -          dashboard at:        172.20.213.45:41349
2023-02-03 22:18:54,137 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.1:36032
2023-02-03 22:18:54,137 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 22:18:54,137 - distributed.worker - INFO -               Threads:                          4
2023-02-03 22:18:54,137 - distributed.worker - INFO -                Memory:                  23.28 GiB
2023-02-03 22:18:54,137 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bj24qnc0
2023-02-03 22:18:54,137 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 22:18:54,139 - distributed.worker - INFO -       Start worker at:  tcp://172.20.213.45:36247
2023-02-03 22:18:54,139 - distributed.worker - INFO -          Listening to:  tcp://172.20.213.45:36247
2023-02-03 22:18:54,139 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-5
2023-02-03 22:18:54,139 - distributed.worker - INFO -          dashboard at:        172.20.213.45:41403
2023-02-03 22:18:54,139 - distributed.worker - INFO - Waiting to connect to:   tcp://172.20.207.1:36032
2023-02-03 22:18:54,139 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 22:18:54,139 - distributed.worker - INFO -               Threads:                          4
2023-02-03 22:18:54,140 - distributed.worker - INFO -                Memory:                  23.28 GiB
2023-02-03 22:18:54,140 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hni3ek1e
2023-02-03 22:18:54,140 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 22:18:54,142 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.1:36032
2023-02-03 22:18:54,142 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 22:18:54,142 - distributed.core - INFO - Starting established connection to tcp://172.20.207.1:36032
2023-02-03 22:18:54,143 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.1:36032
2023-02-03 22:18:54,143 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 22:18:54,144 - distributed.core - INFO - Starting established connection to tcp://172.20.207.1:36032
2023-02-03 22:18:54,148 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.1:36032
2023-02-03 22:18:54,148 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 22:18:54,148 - distributed.core - INFO - Starting established connection to tcp://172.20.207.1:36032
2023-02-03 22:18:54,149 - distributed.worker - INFO -         Registered to:   tcp://172.20.207.1:36032
2023-02-03 22:18:54,149 - distributed.worker - INFO - -------------------------------------------------
2023-02-03 22:18:54,149 - distributed.core - INFO - Starting established connection to tcp://172.20.207.1:36032
/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/geopandas/_compat.py:123: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.
  warnings.warn(
/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/distributed/protocol/pickle.py:71: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:

import os
os.environ['USE_PYGEOS'] = '0'
import geopandas

In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).
  return pickle.loads(x, buffers=buffers)
/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/geopandas/_compat.py:123: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.
  warnings.warn(
/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/distributed/protocol/pickle.py:71: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:

import os
os.environ['USE_PYGEOS'] = '0'
import geopandas

In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).
  return pickle.loads(x, buffers=buffers)
/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/geopandas/_compat.py:123: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.
  warnings.warn(
/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/distributed/protocol/pickle.py:71: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:

import os
os.environ['USE_PYGEOS'] = '0'
import geopandas

In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).
  return pickle.loads(x, buffers=buffers)
/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/geopandas/_compat.py:123: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.
  warnings.warn(
/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/distributed/protocol/pickle.py:71: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:

import os
os.environ['USE_PYGEOS'] = '0'
import geopandas

In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).
  return pickle.loads(x, buffers=buffers)
/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/geopandas/_compat.py:123: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.
  warnings.warn(
/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/distributed/protocol/pickle.py:71: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:

import os
os.environ['USE_PYGEOS'] = '0'
import geopandas

In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).
  return pickle.loads(x, buffers=buffers)
/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/geopandas/_compat.py:123: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.
  warnings.warn(
/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/distributed/protocol/pickle.py:71: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:

import os
os.environ['USE_PYGEOS'] = '0'
import geopandas

In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).
  return pickle.loads(x, buffers=buffers)
/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/geopandas/_compat.py:123: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.
  warnings.warn(
/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/distributed/protocol/pickle.py:71: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:

import os
os.environ['USE_PYGEOS'] = '0'
import geopandas

In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).
  return pickle.loads(x, buffers=buffers)
/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/geopandas/_compat.py:123: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.
  warnings.warn(
/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/distributed/protocol/pickle.py:71: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:

import os
os.environ['USE_PYGEOS'] = '0'
import geopandas

In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).
  return pickle.loads(x, buffers=buffers)
2023-02-03 22:18:58,300 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-03 22:18:59,091 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-03 22:18:59,368 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-03 22:18:59,396 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-03 22:18:59,618 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-03 22:18:59,936 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-03 22:19:00,180 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-03 22:19:00,480 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-03 22:19:03,709 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-03 22:19:05,837 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-03 22:19:06,186 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-03 22:19:06,723 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-03 22:19:07,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-03 22:19:08,136 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-03 22:19:08,657 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-03 22:19:09,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-03 22:19:10,017 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-03 22:19:10,039 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-03 22:19:14,685 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-03 22:19:15,134 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-03 22:19:15,524 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-03 22:19:16,151 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-03 22:19:18,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-03 22:19:19,386 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-03 22:19:20,147 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-03 22:19:21,882 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-03 22:19:22,065 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-03 22:19:22,101 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-03 22:19:23,375 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-03 22:19:23,489 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-03 22:19:23,507 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-03 22:19:24,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-03 22:19:24,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-03 22:19:25,276 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-03 22:19:26,643 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-03 22:19:37,052 - distributed.utils_perf - INFO - full garbage collection released 22.32 MiB from 21 reference cycles (threshold: 9.54 MiB)
/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/dask/dataframe/io/csv.py:194: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.
  df = reader(bio, **kwargs)
/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/dask/dataframe/io/csv.py:194: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.
  df = reader(bio, **kwargs)
/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/dask/dataframe/io/csv.py:194: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.
  df = reader(bio, **kwargs)
/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/dask/dataframe/io/csv.py:194: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.
  df = reader(bio, **kwargs)
/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/dask/dataframe/io/csv.py:194: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.
  df = reader(bio, **kwargs)
/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/dask/dataframe/io/csv.py:194: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.
  df = reader(bio, **kwargs)
/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/dask/dataframe/io/csv.py:194: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.
  df = reader(bio, **kwargs)
/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/dask/dataframe/io/csv.py:194: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.
  df = reader(bio, **kwargs)
slurmstepd: error: *** JOB 8067180 ON node1745 CANCELLED AT 2023-02-03T22:35:02 ***
2023-02-03 22:35:02,194 - distributed.worker - INFO - Stopping worker at tcp://172.20.213.45:46420. Reason: scheduler-close
2023-02-03 22:35:02,194 - distributed.worker - INFO - Stopping worker at tcp://172.20.213.45:38851. Reason: scheduler-close
2023-02-03 22:35:02,194 - distributed.worker - INFO - Stopping worker at tcp://172.20.213.45:36872. Reason: scheduler-close
2023-02-03 22:35:02,194 - distributed.worker - INFO - Stopping worker at tcp://172.20.213.45:44241. Reason: scheduler-close
2023-02-03 22:35:02,194 - distributed.worker - INFO - Stopping worker at tcp://172.20.213.45:33687. Reason: scheduler-close
2023-02-03 22:35:02,194 - distributed.worker - INFO - Stopping worker at tcp://172.20.213.45:46806. Reason: scheduler-close
2023-02-03 22:35:02,194 - distributed.worker - INFO - Stopping worker at tcp://172.20.213.45:36247. Reason: scheduler-close
2023-02-03 22:35:02,194 - distributed.worker - INFO - Stopping worker at tcp://172.20.213.45:35333. Reason: scheduler-close
2023-02-03 22:35:02,194 - distributed.core - INFO - Received 'close-stream' from tcp://172.20.207.1:36032; closing.
2023-02-03 22:35:02,194 - distributed.core - INFO - Received 'close-stream' from tcp://172.20.207.1:36032; closing.
2023-02-03 22:35:02,194 - distributed.core - INFO - Received 'close-stream' from tcp://172.20.207.1:36032; closing.
2023-02-03 22:35:02,194 - distributed.core - INFO - Received 'close-stream' from tcp://172.20.207.1:36032; closing.
2023-02-03 22:35:02,194 - distributed.core - INFO - Received 'close-stream' from tcp://172.20.207.1:36032; closing.
2023-02-03 22:35:02,194 - distributed.core - INFO - Received 'close-stream' from tcp://172.20.207.1:36032; closing.
2023-02-03 22:35:02,194 - distributed.core - INFO - Received 'close-stream' from tcp://172.20.207.1:36032; closing.
2023-02-03 22:35:02,194 - distributed.core - INFO - Received 'close-stream' from tcp://172.20.207.1:36032; closing.
2023-02-03 22:35:02,194 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://172.20.213.45:47694 remote=tcp://172.20.207.1:36032>
Traceback (most recent call last):
  File "/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
  File "/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-02-03 22:35:02,195 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://172.20.213.45:47692 remote=tcp://172.20.207.1:36032>
Traceback (most recent call last):
  File "/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
  File "/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-02-03 22:35:02,195 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://172.20.213.45:47688 remote=tcp://172.20.207.1:36032>
Traceback (most recent call last):
  File "/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
  File "/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-02-03 22:35:02,195 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://172.20.213.45:47690 remote=tcp://172.20.207.1:36032>
Traceback (most recent call last):
  File "/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
  File "/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-02-03 22:35:02,195 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://172.20.213.45:47698 remote=tcp://172.20.207.1:36032>
Traceback (most recent call last):
  File "/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
  File "/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-02-03 22:35:02,195 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://172.20.213.45:47700 remote=tcp://172.20.207.1:36032>
Traceback (most recent call last):
  File "/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
  File "/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-02-03 22:35:02,195 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://172.20.213.45:47696 remote=tcp://172.20.207.1:36032>
Traceback (most recent call last):
  File "/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
  File "/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-02-03 22:35:02,195 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://172.20.213.45:47686 remote=tcp://172.20.207.1:36032>
Traceback (most recent call last):
  File "/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
  File "/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-02-03 22:35:02,198 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://172.20.213.45:40458'. Reason: scheduler-close
2023-02-03 22:35:02,199 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://172.20.213.45:46841'. Reason: scheduler-close
2023-02-03 22:35:02,199 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://172.20.213.45:41859'. Reason: scheduler-close
2023-02-03 22:35:02,200 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://172.20.213.45:36941'. Reason: scheduler-close
2023-02-03 22:35:02,200 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://172.20.213.45:38855'. Reason: scheduler-close
2023-02-03 22:35:02,200 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://172.20.213.45:36631'. Reason: scheduler-close
2023-02-03 22:35:02,201 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://172.20.213.45:38002'. Reason: scheduler-close
2023-02-03 22:35:02,201 - distributed.nanny - INFO - Worker closed
2023-02-03 22:35:02,201 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://172.20.213.45:42444'. Reason: scheduler-close
2023-02-03 22:35:02,202 - distributed.nanny - INFO - Worker closed
2023-02-03 22:35:02,202 - distributed.nanny - INFO - Worker closed
2023-02-03 22:35:02,203 - distributed.nanny - INFO - Worker closed
2023-02-03 22:35:02,203 - distributed.nanny - INFO - Worker closed
2023-02-03 22:35:02,204 - distributed.nanny - INFO - Worker closed
2023-02-03 22:35:02,205 - distributed.nanny - INFO - Worker closed
2023-02-03 22:35:02,205 - distributed.nanny - INFO - Worker closed
