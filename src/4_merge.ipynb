{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T01:04:43.357654Z",
     "start_time": "2023-08-16T01:04:41.150513Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/geopandas/_compat.py:123: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_9710/1660558052.py:2: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from dask.distributed import Client\n",
    "import dask.dataframe as dd\n",
    "from dask_jobqueue import SLURMCluster\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import constants\n",
    "import numpy as np\n",
    "from panel_utilities import get_value_variable_names\n",
    "import dask_geopandas\n",
    "INPUT_DATA_EVICTIONS = \"../data/02_intermediate/evictions.csv\"\n",
    "INPUT_DATA_TRACTS = \"../data/02_intermediate/tracts.csv\"\n",
    "INPUT_DATA_TAX_PARCELS = \"../data/02_intermediate/tax_parcels.gpkg\"\n",
    "INPUT_DATA_ZESTIMATES = \"../data/02_intermediate/zestimates.csv\"\n",
    "INPUT_DATA_CRIME = \"../data/01_raw/crime_incidents\"\n",
    "INPUT_DATA_NEIGHBORHOODS = \"../data/01_raw/Boston_Neighborhoods/Boston_Neighborhoods.shp\"\n",
    "OUTPUT_DATA_ZILLOW = \"../data/03_cleaned/zestimates_analysis.csv\"\n",
    "OUTPUT_DATA_CRIME = \"../data/03_cleaned/crime_analysis_monthly.csv\"\n",
    "OUTPUT_DATA_NEIGHBORHOOD_CRIME_COUNTS = \"../data/03_cleaned/neighborhood_crime_counts.csv\"\n",
    "OUTPUT_TABLES = \"../output/final_paper/tables/\"\n",
    "VERBOSE = True\n",
    "N_PARTITIONS = 1\n",
    "value_vars_to_concat = []  # A list of DataFrames, where each DataFrame contains the panel data for a single outcome variable and has case_number as its index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Evictions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T01:04:43.687083Z",
     "start_time": "2023-08-16T01:04:43.364965Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restricting to 7842 observations which are from Boston.\n",
      "Beginning with 7842 observations.\n",
      "Dropping 266 cases for which the initiating action could not be scraped\n",
      "Dropping 21 cases which began due to foreclosure\n",
      "Dropping 861 observations where latest_docket_date is missing.\n",
      "Dropping 1 observations which have malformed addresses.\n",
      "Dropping 0 rows missing property_address_full\n",
      "Dropping 156 cases where disposition_found is \"Other\" or missing.\n",
      "Dropping 2783 cases resolved through mediation.\n",
      "Dropping 2247 cases which concluded after pandemic began\n",
      "Dropping 0 observations where disposition_found is \"Defaulted\" but judgment_for_pdu is \"Defendant\".\n",
      "Dropping 0 observations where disposition_found is \"Dismissed\" but judgment_for_pdu is \"Plaintiff\".\n"
     ]
    }
   ],
   "source": [
    "# Load evictions data.\n",
    "with open(INPUT_DATA_EVICTIONS, 'r') as file:\n",
    "    all_column_names = set(file.readline().replace(\"\\\"\", \"\").replace(\"\\n\", \"\").split(\",\"))\n",
    "to_drop = {'Accuracy Score', 'Accuracy Type', 'Number', 'Street', 'Unit Type', 'Unit Number',\n",
    "           'State', 'Zip', 'Country', 'Source', 'Census Year', 'State FIPS', 'County FIPS',\n",
    "           'Place Name', 'Place FIPS', 'Census Tract Code', 'Census Block Code', 'Census Block Group',\n",
    "           'Metro/Micro Statistical Area Code', 'Metro/Micro Statistical Area Type',\n",
    "           'Combined Statistical Area Code', 'Metropolitan Division Area Code', 'court_location',\n",
    "           'defendant', 'defendant_atty', 'defendant_atty_address_apt',\n",
    "           'defendant_atty_address_city', 'defendant_atty_address_name', 'defendant_atty_address_state',\n",
    "           'defendant_atty_address_street', 'defendant_atty_address_zip', 'docket_history', 'judgment_for',\n",
    "           'judgment_total', 'plaintiff', 'plaintiff_atty', 'plaintiff_atty_address_apt',\n",
    "           'plaintiff_atty_address_city', 'plaintiff_atty_address_name', 'plaintiff_atty_address_state',\n",
    "           'plaintiff_atty_address_street', 'plaintiff_atty_address_zip', 'Metropolitan Division Area Name',\n",
    "           'property_address_city', 'property_address_state', 'property_address_street',\n",
    "           'property_address_zip'}\n",
    "evictions_df = pd.read_csv(INPUT_DATA_EVICTIONS, usecols=set(all_column_names) - set(to_drop))\n",
    "sample_restriction_table_index = []\n",
    "sample_restriction_table_values = []\n",
    "\n",
    "# Restrict to cases in Boston.\n",
    "boston_mask = ((evictions_df['County'] == \"Suffolk County\") & (~evictions_df['City'].isin([\"Chelsea\", \"Revere\", \"Winthrop\"])))\n",
    "if VERBOSE:\n",
    "    print(f\"Restricting to {boston_mask.sum()} observations which are from Boston.\")\n",
    "evictions_df = evictions_df.loc[boston_mask, :]\n",
    "original_N = len(evictions_df)\n",
    "if VERBOSE:\n",
    "    print(f\"Beginning with {original_N} observations.\")\n",
    "sample_restriction_table_index.append(\"Case Filed in Boston\")\n",
    "sample_restriction_table_values.append(original_N)\n",
    "\n",
    "\n",
    "\n",
    "# Drop cases where initiating action is unknown\n",
    "mask = evictions_df['initiating_action'] == \"Summary Process - Residential (c239)\"\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {mask.sum()} cases for which the initiating action could not be scraped\")\n",
    "evictions_df = evictions_df.loc[~mask, :]\n",
    "sample_restriction_table_index.append(\"Non-missing case initiating action\")\n",
    "sample_restriction_table_values.append(len(evictions_df))\n",
    "\n",
    "# Drop cases initiated due to foreclosure\n",
    "mask = evictions_df['initiating_action'].str.contains(\"Foreclosure\")\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {mask.sum()} cases which began due to foreclosure\")\n",
    "evictions_df = evictions_df.loc[~mask, :]\n",
    "sample_restriction_table_index.append(\"Case initiated for reason other than foreclosure\")\n",
    "sample_restriction_table_values.append(len(evictions_df))\n",
    "\n",
    "\n",
    "\n",
    "# Drop cases missing latest_docket_date.\n",
    "mask = evictions_df['latest_docket_date'].notna()\n",
    "if VERBOSE:\n",
    "    print(\n",
    "        f\"Dropping {evictions_df['latest_docket_date'].isna().sum()} observations where latest_docket_date is missing.\")\n",
    "evictions_df = evictions_df.loc[mask, :]\n",
    "sample_restriction_table_index.append(\"Non-missing latest docket date\")\n",
    "sample_restriction_table_values.append(len(evictions_df))\n",
    "\n",
    "# Drop malformed addresses.\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {evictions_df['property_address_full'].str.contains('span, span span').sum()} observations which \"\n",
    "          f\"have malformed addresses.\")\n",
    "evictions_df = evictions_df.loc[~evictions_df['property_address_full'].str.contains(\"span, span span\"), :]\n",
    "\n",
    "# Drop missing addresses.\n",
    "no_address_info_mask = (evictions_df['property_address_full'].isna())\n",
    "if VERBOSE:\n",
    "    print(\n",
    "        f\"Dropping {no_address_info_mask.sum()} rows missing property_address_full\")\n",
    "evictions_df = evictions_df.loc[~no_address_info_mask, :]\n",
    "sample_restriction_table_index.append(\"Non-missing property address\")\n",
    "sample_restriction_table_values.append(len(evictions_df))\n",
    "\n",
    "# Add file month and year to dataset.\n",
    "evictions_df.loc[:, 'file_month'] = pd.to_datetime(evictions_df['file_date']).dt.strftime('%Y-%m')\n",
    "evictions_df.loc[:, 'file_year'] = pd.to_datetime(evictions_df['file_date']).dt.year\n",
    "\n",
    "# Add latest docket month and year to dataset.\n",
    "evictions_df.loc[:, 'latest_docket_month'] = pd.to_datetime(evictions_df['latest_docket_date']).dt.strftime('%Y-%m')\n",
    "evictions_df.loc[:, 'latest_docket_year'] = pd.to_datetime(evictions_df['latest_docket_date']).dt.year\n",
    "\n",
    "# Drop cases where disposition found is other.\n",
    "disposition_found_other_mask = (evictions_df['disposition_found'] == \"Other\") | (evictions_df['disposition_found'].isna())\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {disposition_found_other_mask.sum()} cases where disposition_found is \\\"Other\\\" or missing.\")\n",
    "evictions_df = evictions_df.loc[~disposition_found_other_mask, :]\n",
    "sample_restriction_table_index.append(\"Cases for which disposition could be scraped\")\n",
    "sample_restriction_table_values.append(len(evictions_df))\n",
    "\n",
    "# Drop cases resolved via mediation.\n",
    "mediated_mask = evictions_df['disposition_found'] == \"Mediated\"\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {mediated_mask.sum()} cases resolved through mediation.\")\n",
    "evictions_df = evictions_df.loc[~mediated_mask, :]\n",
    "sample_restriction_table_index.append(\"Case not resolved through mediation\")\n",
    "sample_restriction_table_values.append(len(evictions_df))\n",
    "\n",
    "# Drop cases concluded in April 2020 or later.\n",
    "pre_pandemic_months = ['2019-04',\n",
    " '2019-05',\n",
    " '2019-06',\n",
    " '2019-07',\n",
    " '2019-08',\n",
    " '2019-09',\n",
    " '2019-10',\n",
    " '2019-11',\n",
    " '2019-12',\n",
    " '2020-01',\n",
    " '2020-02',\n",
    " '2020-03']\n",
    "pre_pandemic_mask = evictions_df['latest_docket_month'].isin(pre_pandemic_months)\n",
    "evictions_df = evictions_df.loc[pre_pandemic_mask, :]\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {(~pre_pandemic_mask).sum()} cases which concluded after pandemic began\")\n",
    "sample_restriction_table_index.append(\"Case concluded before April 2020\")\n",
    "sample_restriction_table_values.append(len(evictions_df))\n",
    "\n",
    "# Drop rows which contain inconsistent values of disposition_found and judgment_for_pdu.\n",
    "# First, we drop cases where disposition_found is \"Defaulted\" but judgment_for_pdu is \"Defendant\"\n",
    "inconsistent_mask_1 = ((evictions_df['disposition_found'] == \"Defaulted\") & (evictions_df['judgment_for_pdu'] == \"Defendant\"))\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {inconsistent_mask_1.sum()} observations where disposition_found is \\\"Defaulted\\\" but judgment_for_pdu is \\\"Defendant\\\".\")\n",
    "evictions_df = evictions_df.loc[~inconsistent_mask_1, :]\n",
    "          \n",
    "# Next, we drop cases where disposition_found is \"Dismissed\" yet judgment_for_pdu is \"Plaintiff\"\n",
    "inconsistent_mask_2 = ((evictions_df['disposition_found'] == \"Dismissed\") & (evictions_df['judgment_for_pdu'] == \"Plaintiff\"))\n",
    "if VERBOSE:\n",
    "    print(f\"Dropping {inconsistent_mask_2.sum()} observations where disposition_found is \\\"Dismissed\\\" but judgment_for_pdu is \\\"Plaintiff\\\".\")\n",
    "evictions_df = evictions_df.loc[~inconsistent_mask_2, :]\n",
    "sample_restriction_table_index.append(\"Succesfully scraped judgment\")\n",
    "sample_restriction_table_values.append(len(evictions_df))\n",
    "\n",
    "# Clean the values in the judgment_for_pdu variable.\n",
    "judgment_for_pdu_replacement_dict = {\"unknown\": \"Unknown\",\n",
    "                                     \"plaintiff\": \"Plaintiff\",\n",
    "                                     \"defendant\": \"Defendant\"}\n",
    "evictions_df.loc[:, \"judgment_for_pdu\"] = (evictions_df.loc[:, \"judgment_for_pdu\"]\n",
    "                                           .replace(judgment_for_pdu_replacement_dict))\n",
    "\n",
    "# Replace missing values in money judgment column with zeroes.\n",
    "evictions_df.loc[:, 'judgment'] = evictions_df['judgment'].fillna(0)\n",
    "\n",
    "# Rename duration to case_duration.\n",
    "evictions_df = evictions_df.rename(columns={'duration': 'case_duration'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T01:04:43.914515Z",
     "start_time": "2023-08-16T01:04:43.688615Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Observations</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Restriction</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Case Filed in Boston</th>\n",
       "      <td>7842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non-missing case initiating action</th>\n",
       "      <td>7576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Case initiated for reason other than foreclosure</th>\n",
       "      <td>7555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non-missing latest docket date</th>\n",
       "      <td>6694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non-missing property address</th>\n",
       "      <td>6693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cases for which disposition could be scraped</th>\n",
       "      <td>6537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Case not resolved through mediation</th>\n",
       "      <td>3754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Case concluded before April 2020</th>\n",
       "      <td>1507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Succesfully scraped judgment</th>\n",
       "      <td>1507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Observations\n",
       "Restriction                                                   \n",
       "Case Filed in Boston                                      7842\n",
       "Non-missing case initiating action                        7576\n",
       "Case initiated for reason other than foreclosure          7555\n",
       "Non-missing latest docket date                            6694\n",
       "Non-missing property address                              6693\n",
       "Cases for which disposition could be scraped              6537\n",
       "Case not resolved through mediation                       3754\n",
       "Case concluded before April 2020                          1507\n",
       "Succesfully scraped judgment                              1507"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build sample restriction table\n",
    "sample_restriction_table = pd.DataFrame()\n",
    "sample_restriction_table[\"Restriction\"] = sample_restriction_table_index\n",
    "sample_restriction_table[\"Observations\"] = sample_restriction_table_values                         \n",
    "sample_restriction_table = sample_restriction_table.set_index(\"Restriction\")\n",
    "\n",
    "# Export to LaTeX.\n",
    "filename = os.path.join(OUTPUT_TABLES, \"sample_restriction.tex\")\n",
    "sample_restriction_table.style.format(formatter=\"{:,.0f}\").to_latex(filename, hrules=True)\n",
    "sample_restriction_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Merging Evictions With Census Tract Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully merged 1507 observations with census tracts.\n"
     ]
    }
   ],
   "source": [
    "# Merge with census tract characteristics.\n",
    "evictions_df = evictions_df.rename(columns={'Full FIPS (tract)': 'tract_geoid'})\n",
    "evictions_tracts_df = evictions_df.merge(pd.read_csv(INPUT_DATA_TRACTS, dtype={'tract_geoid': float}),\n",
    "                                  on='tract_geoid',\n",
    "                                  how='left',\n",
    "                                  validate='m:1').set_index('case_number')\n",
    "if VERBOSE:\n",
    "    print(f\"Successfully merged {evictions_tracts_df['med_hhinc2016'].notna().sum()} observations with census tracts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Merging Evictions with Boston Neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GeoSeries containing eviction Points as its geometry, case_number as its index, and no other columns.\n",
    "evictions_gdf = gpd.GeoDataFrame(evictions_df, geometry=gpd.points_from_xy(evictions_df['Longitude'], evictions_df['Latitude']))[['geometry', 'case_number']]\n",
    "evictions_gdf = evictions_gdf.set_crs(\"EPSG:4326\", allow_override=True).to_crs('EPSG:26986')\n",
    "\n",
    "# Read shapefile containing Boston neighborhood boundaries\n",
    "boston_neighborhoods_gdf = (gpd.read_file(INPUT_DATA_NEIGHBORHOODS)[['geometry', 'Name']]\n",
    "                            .to_crs('EPSG:26986')\n",
    "                            .rename(columns={'Name': 'neighborhood'}))\n",
    "\n",
    "# Spatial join\n",
    "evictions_neighborhoods_df = (gpd.sjoin(evictions_gdf, boston_neighborhoods_gdf, how='inner', predicate='within')\n",
    "                              .drop(columns=['index_right', 'geometry'])\n",
    "                              .set_index('case_number'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Merge Evictions with Crimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 42029 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Request computing resources.\n",
    "cluster = SLURMCluster(queue='batch',\n",
    "                       cores=32,\n",
    "                       memory='230 GB',\n",
    "                       walltime='00:20:00',\n",
    "                      scheduler_options={'dashboard_address': '8787'} )\n",
    "cluster.scale(jobs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read crime data as Dask DataFrame, then compute back to DataFrame.\n",
    "crime_df = (dd.read_csv(INPUT_DATA_CRIME + \"/*.csv\", dtype={'REPORTING_AREA': 'object', 'SHOOTING': 'object'})\n",
    "                .dropna(subset=['Long', 'Lat', 'OCCURRED_ON_DATE'])  # Drop crimes missing latitude, longitude, or date, as they cannot be merged with panel.\n",
    "                .rename(columns={'OCCURRED_ON_DATE': 'month_of_crime_incident'})\n",
    "                .drop(columns=['OFFENSE_CODE_GROUP', 'OFFENSE_DESCRIPTION', 'DISTRICT', 'REPORTING_AREA', 'YEAR', 'MONTH',\n",
    "                               'DAY_OF_WEEK', 'HOUR', 'UCR_PART', 'STREET', 'Location'])  # Drop unneeded columns\n",
    "                .compute())\n",
    "                # Must call compute here and then briefly convert back to in-memory dataset because dask_geopandas.points_from_xy not working.\n",
    "crime_df.loc[:, 'INCIDENT_NUMBER'] = 1  # Replace column with 1s so we can count crimes using sum function.\n",
    "# Keep track of the month of crime incident in YYYY-MM format.\n",
    "crime_df.loc[:, 'month_of_crime_incident'] = pd.to_datetime(crime_df['month_of_crime_incident'].str[:10]).dt.to_period(\"M\").astype(str)\n",
    "# Convert DataFrame to GeoDataFrame.\n",
    "crime_gdf = gpd.GeoDataFrame(crime_df, geometry=gpd.points_from_xy(crime_df['Long'], crime_df['Lat']))\n",
    "crime_gdf = crime_gdf.set_crs(\"EPSG:4326\", allow_override=True).to_crs(\"EPSG:26986\")  # Convert to the correct CRS.\n",
    "# Convert GeoDataFrame to Dask GeoDataFrame.\n",
    "crime_dgdf = dask_geopandas.from_geopandas(crime_gdf, npartitions=N_PARTITIONS).repartition(partition_size='5 MB')\n",
    "\n",
    "\"\"\"\n",
    "# Merge crimes with Boston neighborhoods here\n",
    "(dask_geopandas\n",
    ".sjoin(crime_dgdf.loc[crime_dgdf['month_of_crime_incident'].isin(pre_pandemic_months), :],\n",
    "       dask_geopandas.from_geopandas(boston_neighborhoods_gdf, npartitions=N_PARTITIONS).repartition(partition_size='5 MB'),\n",
    "       how='inner',\n",
    "       predicate='within')\n",
    ".drop(columns='geometry')\n",
    ".groupby('neighborhood')\n",
    ".agg({'INCIDENT_NUMBER': 'count'})\n",
    ".rename(columns={'INCIDENT_NUMBER': 'total_crimes'})\n",
    ".compute()\n",
    ".to_csv(OUTPUT_DATA_NEIGHBORHOOD_CRIME_COUNTS))\n",
    "\"\"\"\n",
    "\n",
    "# Create lists containing each group of offense codes.\n",
    "# Group 1\n",
    "wealthy_area_crimes = [613, 3801, 3410]\n",
    "\n",
    "# Group 2\n",
    "assault = [801, 802, 803, 423, 413, 401, 402, 403, 404, 411, 412, 413, 421, 422,  # Assault\n",
    "                         423,424, 431, 432, 433, 3301,  # Assault\n",
    "                         2647]\n",
    "\n",
    "\n",
    "# Group 3\n",
    "investigate = [3114, 3115]\n",
    "           \n",
    "\"\"\"# Group 4\n",
    "drugs = [1870, 2609, 1874, 1842, 1841, 1849, 1848, 1858, 1855, 1864, 1863, 1866,  # Drugs \n",
    "                         1868, 1843, 3023, 3021, 1875, 3022, 1847, 1840, 1873, 1843, 1844, 1845,  # Drugs \n",
    "                         1846, 1870, 1874, 1842, 1841, 1849, 1847, 1848, 1850, 1825, 1815, 1832,  # Drugs\n",
    "                         1831, 2609, 1810, 1830, 1805, 1806, 1807, 1825, 1815, 1832, 1831, 2609, 1810, 1830, 9067, 7974]  # Drugs\"\"\"\n",
    "\n",
    "vandalism = [1402, 1415]\n",
    "\n",
    "auto_theft = [701,\n",
    "702,\n",
    "704,\n",
    "711,\n",
    "712,\n",
    "713,\n",
    "714,\n",
    "715,\n",
    "804,\n",
    "900,\n",
    "706, \n",
    "724,\n",
    "727,\n",
    "706,\n",
    "723,\n",
    "724,\n",
    "727,\n",
    "735,\n",
    "770,\n",
    "780,\n",
    "790]\n",
    "\n",
    "\n",
    "columns_for_each_outcome = []\n",
    "offense_groups = ['all', wealthy_area_crimes, assault, investigate, vandalism, auto_theft]\n",
    "radii = [250, 300, 350, \"250_to_300\", \"250_to_350\", \"250_to_400\"]\n",
    "for offense_group_num, offense_group in enumerate(offense_groups):\n",
    "    for radius in radii:\n",
    "        \n",
    "        if isinstance(radius, int):\n",
    "            # Create a new GeoDataFrame with geometry equal to circles around each eviction with the current radius.\n",
    "            current_evictions_gdf = evictions_gdf.copy()\n",
    "            current_evictions_gdf.geometry = current_evictions_gdf.geometry.buffer(radius)\n",
    "            current_evictions_dgdf = dask_geopandas.from_geopandas(current_evictions_gdf, npartitions=N_PARTITIONS).repartition(partition_size='5 MB')\n",
    "        else: \n",
    "            # We need to create a \"donut\" around the property\n",
    "            inner_radius = evictions_gdf.geometry.buffer(int(radius[:3]))\n",
    "            outer_radius = evictions_gdf.geometry.buffer(int(radius[-3:]))\n",
    "            donut = outer_radius.difference(inner_radius)\n",
    "            current_evictions_gdf = evictions_gdf.copy()\n",
    "            current_evictions_gdf.geometry = donut\n",
    "            current_evictions_dgdf = dask_geopandas.from_geopandas(current_evictions_gdf, npartitions=N_PARTITIONS).repartition(partition_size='5 MB')\n",
    "\n",
    "\n",
    "    \n",
    "        # Merge evictions with crimes that fall into radius.\n",
    "        current_evictions_crime_dgdf = dask_geopandas.sjoin(crime_dgdf, current_evictions_dgdf, how='inner', predicate='within')\n",
    "        current_evictions_crime_dgdf = current_evictions_crime_dgdf.drop(columns=['geometry','index_right'])\n",
    "        \n",
    "        \n",
    "        if offense_group != 'all':  # If we are summing a specific subcategory of crimes\n",
    "            mask = current_evictions_crime_dgdf['OFFENSE_CODE'].isin(offense_group)\n",
    "            # Multiply mask by 'INCIDENT_NUMBER' to zero out crimes in different offense groups.\n",
    "            current_evictions_crime_dgdf['INCIDENT_NUMBER'] = current_evictions_crime_dgdf['INCIDENT_NUMBER'] * mask\n",
    "\n",
    "                \n",
    "            \n",
    "        # Aggregate crimes to case-month level.\n",
    "        current_panel_long = (current_evictions_crime_dgdf\n",
    "                              .groupby(['case_number', 'month_of_crime_incident'])\n",
    "                              .aggregate({'INCIDENT_NUMBER': 'sum'})\n",
    "                              .reset_index()\n",
    "                              .rename(columns={'INCIDENT_NUMBER': 'crime_incidents'})\n",
    "                              .compute())\n",
    "\n",
    "        current_panel_wide = pd.pivot(current_panel_long, index=['case_number'], columns=['month_of_crime_incident'],\n",
    "                                      values='crime_incidents').reset_index().set_index('case_number')\n",
    "        current_panel_wide.columns = [f'{column}_group_{offense_group_num}_crimes_{radius}m' for column in current_panel_wide.columns]\n",
    "        columns_for_each_outcome.append(current_panel_wide.columns)\n",
    "        value_vars_to_concat.append(dd.from_pandas(current_panel_wide, npartitions=N_PARTITIONS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Producing the Analysis Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_df = dd.multi.concat([dd.from_pandas(evictions_tracts_df, npartitions=N_PARTITIONS), dd.from_pandas(evictions_neighborhoods_df, npartitions=N_PARTITIONS)] + value_vars_to_concat, axis=1).compute()\n",
    "# For evictions not matched to any crimes, fill NA values with 0.\n",
    "for group_of_columns in columns_for_each_outcome:\n",
    "    crime_df[group_of_columns] = crime_df[group_of_columns].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9710/1181585433.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  crime_df.loc[:, 'judgment_for_defendant'] = 0\n",
      "/tmp/ipykernel_9710/1181585433.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  crime_df.loc[:, 'judgment_for_plaintiff'] = 1 - crime_df['judgment_for_defendant']\n"
     ]
    }
   ],
   "source": [
    "# Generate a variable indicating judgment in favor of defendant.\n",
    "crime_df.loc[:, 'judgment_for_defendant'] = 0\n",
    "defendant_won_mask = ((crime_df['disposition_found'] == \"Dismissed\") |\n",
    "                      (crime_df['judgment_for_pdu'] == \"Defendant\") |\n",
    "                      (crime_df['disposition'].str.contains('R 41(a)(1) Voluntary Dismissal', regex=False)))\n",
    "crime_df.loc[defendant_won_mask, 'judgment_for_defendant'] = 1\n",
    "\n",
    "# Generate a variable indicating judgement in favor of plaintiff.\n",
    "crime_df.loc[:, 'judgment_for_plaintiff'] = 1 - crime_df['judgment_for_defendant']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create variables containing pre-treatment outcomes\n",
    "outcomes = [f\"group_{group_num}_crimes_{radius}m\" for group_num in range(len(offense_groups)) for radius in radii]\n",
    "\n",
    "for outcome in outcomes:\n",
    "    # Save value variable names and dictionary mapping months to integers\n",
    "    triplet = get_value_variable_names(crime_df, outcome)\n",
    "    weekly_value_vars_crime, month_to_int_dictionary, _ = triplet\n",
    "    \n",
    "    # Create variable equal to sum of crimes in 2017\n",
    "    columns_from_2017 = [column for column in crime_df.columns if column.startswith('2017') and column.endswith(outcome)]\n",
    "    crime_df.loc[:, f'total_twenty_seventeen_{outcome}'] = crime_df[columns_from_2017].sum(axis=1)\n",
    "    \n",
    "    # Create variable equal to sum of crimes in 2019 \n",
    "    columns_from_2019 = [column for column in crime_df.columns if column.startswith('2019') and column.endswith(outcome)]\n",
    "    crime_df.loc[:, f'total_twenty_nineteen_{outcome}'] = crime_df[columns_from_2019].sum(axis=1)\n",
    "    \n",
    "    # Create variable equal to change in the total number of crimes between 2019 and 2017\n",
    "    crime_df.loc[:, f'pre_treatment_change_in_{outcome}'] = crime_df[f'total_twenty_nineteen_{outcome}'] - crime_df[f'total_twenty_seventeen_{outcome}']\n",
    "    \n",
    "    # Create variable equal to change in the total number of crimes in the two years prior to treatment\n",
    "    crime_df_copy = crime_df.copy().reset_index()\n",
    "    crime_df_copy = pd.melt(crime_df_copy,\n",
    "                            id_vars=['case_number',\n",
    "                                     'latest_docket_month'],\n",
    "                            value_vars=weekly_value_vars_crime,\n",
    "                            var_name='month')\n",
    "    crime_df_copy.loc[:, 'month'] = crime_df_copy['month'].str[:7]\n",
    "    crime_df_copy.loc[:, ['latest_docket_month', 'month']] = crime_df_copy[['latest_docket_month', 'month']].replace(month_to_int_dictionary)\n",
    "    crime_df_copy.loc[:, 'treatment_relative_month'] = crime_df_copy['month'] - crime_df_copy['latest_docket_month']\n",
    "    crime_df_copy = crime_df_copy.loc[crime_df_copy['treatment_relative_month'].isin([-12, 0]), ['case_number', 'treatment_relative_month', 'value']]\n",
    "    crime_df_copy = crime_df_copy.pivot(index='case_number', columns='treatment_relative_month', values='value')\n",
    "    relative_pre_treatment_change_in_outcome = (crime_df_copy[0] - crime_df_copy[-12]).rename(f'relative_pre_treatment_change_in_{outcome}')\n",
    "    crime_df = pd.concat([crime_df, relative_pre_treatment_change_in_outcome], axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_cause_mask = crime_df['initiating_action'].isin([\"SP Summons and Complaint - Cause\",\n",
    "                                                     \"Summary Process - Residential-Cause other than Non payment of rent.\",\n",
    "                                                     \"SP Transfer - Cause\"])\n",
    "crime_df.loc[:, 'for_cause'] = np.where(for_cause_mask, 1, 0)\n",
    "\n",
    "no_cause_mask = crime_df['initiating_action'].isin([\"SP Summons and Complaint - No Cause\",\n",
    "                                                     \"SP Transfer- No Cause\"])\n",
    "crime_df.loc[:, 'no_cause'] = np.where(no_cause_mask, 1, 0)\n",
    "\n",
    "non_payment_of_rent_mask = crime_df['initiating_action'].isin([\"SP Summons and Complaint - Non-payment of Rent\",\n",
    "                                                               \"SP Transfer - Non-payment of Rent\"])\n",
    "crime_df.loc[:, 'non_payment'] = np.where(non_payment_of_rent_mask, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create case resolution variables\n",
    "panel_E_columns = ['dismissed', 'defaulted', 'heard']\n",
    "origin_columns = ['disposition_found', 'disposition_found',\n",
    "                  'disposition_found']\n",
    "target_values = [\"Dismissed\", \"Defaulted\", \"Heard\"]\n",
    "\n",
    "for dummy_column, origin_column, target_value in zip(panel_E_columns, origin_columns, target_values):\n",
    "    crime_df.loc[:, dummy_column] = np.where(crime_df[origin_column] == target_value, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_df.to_csv(OUTPUT_DATA_CRIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "def run_event_study(df: pd.DataFrame, treatment_date_variable: str):\n",
    "    # Reshape to long\n",
    "    triplet = get_value_variable_names(df, f\"group_0_crimes_{100}m\")\n",
    "    weekly_value_vars_crime, month_to_int_dictionary, int_to_month_dictionary = triplet\n",
    "    df = pd.melt(df,\n",
    "                 id_vars=['case_number',\n",
    "                          'judgment_for_plaintiff',\n",
    "                          treatment_date_variable],\n",
    "                 value_vars=weekly_value_vars_crime,\n",
    "                 var_name='month')\n",
    "    df.loc[:, 'month'] = df['month'].str[:7]  # Drop \"_group_0_crimes_500m\" from the end of each month\n",
    "\n",
    "    # Replace months with integers\n",
    "    df.loc[:, [treatment_date_variable, 'month']] = df[[treatment_date_variable, 'month']].replace(month_to_int_dictionary)\n",
    "\n",
    "    # Calculate crime levels during each month relative to treatment, separately for treatment and control gropu\n",
    "    df.loc[:, 'treatment_relative_month'] = df['month'] - df[treatment_date_variable]\n",
    "\n",
    "    # Create column containing calendar month so that we can add dummies to model\n",
    "    df.loc[:, 'calendar_month'] = df['month'].replace(int_to_month_dictionary).str[-2:]\n",
    "\n",
    "    y = df['value']\n",
    "    x_variables = []\n",
    "    x_variables.append(df['judgment_for_plaintiff'])\n",
    "    month_dummies = pd.get_dummies(df['treatment_relative_month'], prefix='month', drop_first=True)\n",
    "    x_variables.append(month_dummies)\n",
    "    month_times_treatment_indicator_dummies = (month_dummies\n",
    "                                               .mul(df['judgment_for_plaintiff'], axis=0))\n",
    "    month_times_treatment_indicator_dummies.columns = [col + \"_X_treatment_indicator\" for col in\n",
    "                                                       month_times_treatment_indicator_dummies.columns]\n",
    "    x_variables.append(month_times_treatment_indicator_dummies)\n",
    "    X = pd.concat(x_variables, axis=1)\n",
    "\n",
    "    omitted_period = df['treatment_relative_month'].min()\n",
    "    omitted_period_control_mean = df.loc[(df['treatment_relative_month'] == omitted_period) &\n",
    "                                         (df['judgment_for_plaintiff'] == 0), 'value'].mean()\n",
    "\n",
    "    return sm.OLS(y, X).fit(), omitted_period_control_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot concatenate unaligned mixed dimensional NDFrame objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result, omitted_period_control_mean \u001b[38;5;241m=\u001b[39m \u001b[43mrun_event_study\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcrime_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlatest_docket_month\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 33\u001b[0m, in \u001b[0;36mrun_event_study\u001b[0;34m(df, treatment_date_variable)\u001b[0m\n\u001b[1;32m     30\u001b[0m month_times_treatment_indicator_dummies\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_X_treatment_indicator\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m\n\u001b[1;32m     31\u001b[0m                                                    month_times_treatment_indicator_dummies\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[1;32m     32\u001b[0m x_variables\u001b[38;5;241m.\u001b[39mappend(month_times_treatment_indicator_dummies)\n\u001b[0;32m---> 33\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_variables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m omitted_period \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtreatment_relative_month\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmin()\n\u001b[1;32m     36\u001b[0m omitted_period_control_mean \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtreatment_relative_month\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m omitted_period) \u001b[38;5;241m&\u001b[39m\n\u001b[1;32m     37\u001b[0m                                      (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjudgment_for_plaintiff\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/pandas/core/reshape/concat.py:347\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcat\u001b[39m(\n\u001b[1;32m    145\u001b[0m     objs: Iterable[NDFrame] \u001b[38;5;241m|\u001b[39m Mapping[Hashable, NDFrame],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m     copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    155\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m    156\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m    Concatenate pandas objects along a particular axis with optional set logic\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    along the other axes.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03m    ValueError: Indexes have overlapping values: ['a']\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m/gpfs/home/ashanmu1/seniorthesis/venv/lib/python3.10/site-packages/pandas/core/reshape/concat.py:501\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m ndim \u001b[38;5;241m!=\u001b[39m max_ndim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    502\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot concatenate unaligned mixed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdimensional NDFrame objects\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m     )\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    507\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot concatenate unaligned mixed dimensional NDFrame objects"
     ]
    }
   ],
   "source": [
    "result, omitted_period_control_mean = run_event_study(crime_df.reset_index(), 'latest_docket_month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "month = []\n",
    "treatment = []\n",
    "control = []\n",
    "for i in range(-12, 36):\n",
    "    month.append(i)\n",
    "    control.append(result.params[f'month_{i}'] )\n",
    "    treatment.append((result.params['judgment_for_plaintiff'] +\n",
    "                       result.params[f'month_{i}_X_treatment_indicator'] +\n",
    "                       result.params[f'month_{i}'] ))\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(month,\n",
    "    control,\n",
    "    color='grey',\n",
    "    linestyle='--',\n",
    "    marker='o',\n",
    "    label=\"Properties where Tenant Won Eviction Case\")\n",
    "ax.plot(month,\n",
    "        treatment,\n",
    "        color='black',\n",
    "        linestyle='--',\n",
    "        marker='o',\n",
    "        label=\"Properties where Plaintiff Won Eviction Case\")\n",
    "label = 'latest_docket_month'.replace(\" \", \"\\n\")\n",
    "\n",
    "ax.set_xlabel(f\"Month Relative to latest_docket_month\")\n",
    "ax.set_ylabel(f\"Crime Incidents within {constants.Analysis.MAIN_RESULTS_RADIUS} Meters\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
